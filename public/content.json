[{"title":"Python中的元组和Packing/Unpacking","date":"2017-03-30T03:02:31.000Z","path":"2017/03/30/packing-and-unpacking-tuples/","text":"什么是元组元组（Tuple）是与列表类似的数据结构，只可被创建，不可被修改，用一对圆括号()包起来，如：&gt;&gt;&gt; x = ('a','b','c') #具有三个元素的元组&gt;&gt;&gt; y = ('a',) #只有一个元素的元组，注意，必须有一个逗号来标识&gt;&gt;&gt; z = () #空元组 元组的操作跟列表非常类似，如+，*，切片等，在元组中同样适用：&gt;&gt;&gt; a = (1,2,3)&gt;&gt;&gt; a[:2](1, 2)&gt;&gt;&gt; a * 1(1, 2, 3)&gt;&gt;&gt; a + (4,5)(1, 2, 3, 4, 5) 元组的Packing/UnpackingPython中允许元组出现在赋值运算符的左侧，这样元组中的每个变量就可以被赋值为右侧对应位置的值，如：&gt;&gt;&gt; (a,b,c,d) = (1,2,3,4)&gt;&gt;&gt; a1&gt;&gt;&gt; c3 上面的写法还可以简化为:&gt;&gt;&gt; a,b,c,d = 1,2,3,4 这个用法还可以非常方便的完成交换两个变量的值：&gt;&gt;&gt; var1, var2 = var2, var1 在Python3中，还提供了一个扩展的unpacking特性——使用*来标注元素来吸收与其他元素不匹配的任何数量的元素，举例如下：&gt;&gt;&gt; x = (1,2,3,4)&gt;&gt;&gt; a, b, *c = x&gt;&gt;&gt; a, b, c(1, 2, [3, 4])&gt;&gt;&gt; a, *b, c = x&gt;&gt;&gt; a, b, c(1, [2, 3], 4)&gt;&gt;&gt; *a, b, c = x&gt;&gt;&gt; a, b, c([1, 2], 3, 4)&gt;&gt;&gt; a, b, c, d, *e = x&gt;&gt;&gt; a, b, c, d, e(1, 2, 3, 4, []) 被标星的元素接收多余的元素作为一个列表，如果没有多余的元素，则会接收一个空列表。 Python中的Packing/Unpacking应用Python中，我们可以使用*（对元组来说）和**（对字典来说）的Packing和Unpacking函数的参数。 * for tuples从下面这个例子说起，我们有一个函数fun()接收四个函数，并打印出来：def fun(a, b, c, d): print(a, b, c, d) 假设我们有一个list：&gt;&gt;&gt; my_list = [1, 2, 3, 4] 调用函数fun()：&gt;&gt;&gt; my_list = [1,2,3,4]&gt;&gt;&gt; fun(my_list)Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;TypeError: fun() missing 3 required positional arguments: 'b', 'c', and 'd'&gt;&gt;&gt; 会得到报错信息，函数认为我们的my_list是单独的一个参数，而函数还需要额外的三个参数。 这时候，我们可以使用*来解包（Unpacking）列表，使之作为四个参数：&gt;&gt;&gt; fun(*my_list)1 2 3 4 这里还有另一个例子，使用内置的range()函数，来演示解包列表的操作： 注，这里使用的是Python3.x，range(3,7)不会直接打印区间的所有值 &gt;&gt;&gt; list(range(3,7))[3, 4, 5, 6]&gt;&gt;&gt; args = [3,7]&gt;&gt;&gt; list(range(args))Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;TypeError: 'list' object cannot be interpreted as an integer&gt;&gt;&gt; list(range(*args))[3, 4, 5, 6] 当我们不知道究竟要传递多少参数给函数时，我们可以使用打包（Packing）把所有的参数打包到一个元组中，用下面的例子来演示打包操作。 我们有函数iSum()来做求和操作def iSum(*args): sum=0 print(args) #把args打印出来，看是否被打包为一个元组 for i in range(0, len(args)): sum = sum + args[i] return sum 测试时，传参个数不等，得到的输出如下：&gt;&gt;&gt; print(iSum(1,2,3,4,5))(1, 2, 3, 4, 5)15&gt;&gt;&gt; print(iSum(10,20,30))(10, 20, 30)60 可以看到入参确实被打包成了一个元组，然后循环遍历元组求和。 如果打包后我们想修改参数，由于元组不可修改，所以需要先转换成列表。下面展示一个打包和解包混合使用的例子。我们有函数func1()用于打印入参，func2()用户修改入参的值：&gt;&gt;&gt; def func1(a,b,c):... print(a,b,c)... &gt;&gt;&gt; def func2(*args):... args = list(args)... args[0] = 'elbarco.cn'... args[1] = 'awesome'... func1(*args)... 调用func2()，传递的三个参数，首先打包为一个元组，然后将元组转换为列表，并修改前两个元素的值，再解包为三个参数，打印出结果，如下所示：&gt;&gt;&gt; func2('Hello','nice','visitors')elbarco.cn awesome visitors ** for dictionaries对于字典（Dictionary），Packing/Unpacking操作使用**。 还是使用上面的func1()，如果要打印字典的值，则需要使用**来解包：&gt;&gt;&gt; dict = &#123;&apos;a&apos;:1,&apos;b&apos;:3,&apos;c&apos;:5&#125;&gt;&gt;&gt; func1(**dict)1 3 5 下面来一个打包的例子：&gt;&gt;&gt; def func3(**elbarco):... print(type(elbarco))... for key in elbarco:... print(&quot;%s = %s&quot; % (key, elbarco[key]))... 传几个参数，用func3()将入参打包为字典，然后在函数中把key和value输出出来，结果如下所示：&gt;&gt;&gt; func3(name='elbarco', location='Beijing', language='Java/Python')&lt;class 'dict'&gt;name = elbarcolocation = Beijinglanguage = Java/Python 以上。 参考1.The Quick Python Book 2nd Edition.Chaptor 5.7.32.Packing and Unpacking Arguments in Python3.Packing and Unpacking Arguments in Python4.Python’s range() Function Explained","tags":[{"name":"Python","slug":"Python","permalink":"http://elbarco.cn/tags/Python/"},{"name":"Tuple","slug":"Tuple","permalink":"http://elbarco.cn/tags/Tuple/"}]},{"title":"从Python中内嵌列表复制说起","date":"2017-03-29T02:00:31.000Z","path":"2017/03/29/nested-lists-and-deep-copies/","text":"写在前面在学习Python3时，看到了列表的拷贝，于是把这个小课题整理在这里，以作记录。 内嵌列表和列表拷贝中的问题Python中的列表是可以嵌入列表的，这个特性的应用场景之一便是用于表示二维矩阵。如下所示：&gt;&gt;&gt; m = [[0, 1, 2], [10, 11, 12], [20, 21, 22]]&gt;&gt;&gt; m[0][0, 1, 2]&gt;&gt;&gt; m[0][1]1&gt;&gt;&gt; m[2][20, 21, 22]&gt;&gt;&gt; m[2][2]22 当然，这一特性可以用于按照我们自己的方式扩展到更多维的矩阵。 大部分情况下，内嵌列表如果只是这样使用，我们需要关心的也就到此为止了。但是因为有变量引用对象，而对象本身又是可被修改的情况，比如列表中内嵌列表，而内嵌列表本身是可被修改的，我们还会遇到下面提到的问题，我们通过例子来演示。 创建一个含有内嵌列表的列表original：&gt;&gt;&gt; nested = [0]&gt;&gt;&gt; original = [nested, 1]&gt;&gt;&gt; original[[0], 1] 列表original的第一个元素指向了列表nested，如图所示： 列表nested的修改可以通过直接修改其本身，也可以通过修改列表original来实现，即：&gt;&gt;&gt; nested[0] = 'zero'&gt;&gt;&gt; original[['zero'], 1]&gt;&gt;&gt; original[0][0] = 0&gt;&gt;&gt; nested[0]&gt;&gt;&gt; original[[0], 1] 如果我们将nested赋值为其他列表，则nested和original之间的连接就会断掉，即：&gt;&gt;&gt; nested = [2]&gt;&gt;&gt; original[[0], 1] 如图所示： 除了上面提到的直接赋值的方式，列表的拷贝我们还可以使用—— 全切片(full slice) &gt;&gt;&gt; x = [0, 1, 2]&gt;&gt;&gt; y = x[:]&gt;&gt;&gt; y[0, 1, 2] +或*运算符 &gt;&gt;&gt; x = [0, 1, 2]&gt;&gt;&gt; y = x + []&gt;&gt;&gt; y[0, 1, 2]&gt;&gt;&gt; z = x * 1&gt;&gt;&gt; z[0, 1, 2] 但是无论上面哪种复制方式，只要列表中存在嵌入列表，就会存在这种问题，我们把这种复制称之为“浅拷贝”，即shallow copy，与之相对的，是“深拷贝”，即deep copy。 对列表的深拷贝对于含有内嵌列表的列表来讲，如果我们需要把内嵌列表也一并拷贝，则需要使用copy模块的deepcopy功能。 &gt;&gt;&gt; original = [[0], 1]&gt;&gt;&gt; shallow = original[:]&gt;&gt;&gt; import copy&gt;&gt;&gt; deep = copy.deepcopy(original) 复制后两个列表的构成其实如下图所示： 在得到列表shallow和列表deep后，我们去尝试修改列表中的值和内嵌列表的值，并看看效果如何：&gt;&gt;&gt; shallow[1]=2 #更改列表中非内嵌列表的值，原列表值不变&gt;&gt;&gt; shallow[[0], 2]&gt;&gt;&gt; original[[0], 1]&gt;&gt;&gt; shallow[0][0]='zero' #更改内嵌列表的值，原列表内嵌列表值改变&gt;&gt;&gt; shallow[['zero'], 2]&gt;&gt;&gt; original[['zero'], 1]&gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; deep[0][0]=5 #对于deep copy的列表，即使修改内嵌列表的值也不会影响原列表&gt;&gt;&gt; deep[[5], 1]&gt;&gt;&gt; original[['zero'], 1]&gt;&gt;&gt; 此外，对于Python来讲，任何列表中嵌入的对象是可修改的，如字典，都会存在这样的问题。 总结和引申思考首先，明确一点，deep copy和shallow copy并不是Python中特有的概念，而是一个与复制对象时对象的成员是否被复制有关的通用的概念。 根据维基百科中的Object copying中的描述，我们总结如下： 我们有变量A和变量B指向不同的内存地址，当B被赋值为A时，两个变量指向了同样的内存地址，之后无论是修改A还是B的内容，都会在另一个变量中立即体现出来，因为两者共享内容。 我们有变量A和B指向了不同的内容地址，当B被赋值为A时，指向A内存地址的内容被复制到B的内存中，之后无论是修改A还是B的内容，A和B都是保持独立的，因为两者不共享内存，即不共享内容。 其他语言，如Java，可以参见StackOverFlow上的这一个讨论：How do I copy an object in Java，后面有机会再详细的梳理一下。 参考1.What’s the difference between a deep copy and a shallow copy2.Object copying3.Shallow and deep copy4.The Quick Python Book, 2nd Edition. Chapter 5.6","tags":[{"name":"Python","slug":"Python","permalink":"http://elbarco.cn/tags/Python/"},{"name":"Deep Copy","slug":"Deep-Copy","permalink":"http://elbarco.cn/tags/Deep-Copy/"},{"name":"Shallow Copy","slug":"Shallow-Copy","permalink":"http://elbarco.cn/tags/Shallow-Copy/"}]},{"title":"并发编程模型之Actor模型","date":"2017-01-21T09:30:30.000Z","path":"2017/01/21/introduction-to-actor-model/","text":"上一篇文章《几个概念：并发、并行、进程、线程和协程》中，对并发和并行的概念做了一个简单的解释，而本文中则从两种并发编程模型讲起，简单的介绍一下Actor模型。 两种并发编程模型并发编程中有两类常见的模型：共享内存和消息传递。 共享内存模型 在并发编程的共享内存模型中，各组件通过读写内存中的共享对象进行交互。 共享内存模型的其他示例： A、B两个在同一个电脑中的处理器（或者同一个处理器的两个核）共享同一物理内存 A、B两个运行在同一电脑上的程序，共享同一文件系统，其中文件可以读写 A、B是同一Java程序中的两个线程，共享相同的Java对象。 消息传递模型 在消息传递模型中，并发模块通过通信信道将消息发送到彼此进行交互。发出的消息会在队列中等待处理。 消息传递模型的示例还有： A和B是通网络中的两台计算机，通过网络通讯 A是一个浏览器，B是一个web服务器，A打开连接请求网页，B发送页面数据给A Actor模型认识Actor模型上面我们认识了两种并发模型，actor模型就属于消息传递模型。actor模型的基本思想是使用actor作为并发基元，可以根据接收的消息做出不同的响应（或动作、行为）： 将有限数量的消息传递给其他的actor 产生有限数量的新的actor 当下一个传入的消息被处理时，改变自己的内部行为 actor模型使用异步消息传递进行通信。特别要指出，actor之间不适用任何中间实体，比如通道，相反的，每个actor拥有可以被寻址的信箱。不要把地址和身份信息弄混淆，每个actor可以有零个、一个或多个地址。当一个actor发送信息时，它必须知道接收方的地址。此外，actor可以给自己发信息，这样他们就可以自己接受信息并且稍后进行处理。注意，这里提到的邮箱并不是概念的一部分，而是一个特性的实现， actor模型可以用于并发系统的建模，正是因为每个actor与其他actor完全独立，没有共享状态，则就没有了竞争状态（race condition），actor之间的通讯和交互完全通过异步消息。 支持actor模型的编程语言有Elixir、Erlang、Scala等，Java语言层面并不支持，但是可以引入Akka，一个用Scala编写的库，用于简化编写容错的、高可伸缩性的Java和Scala的actor模型应用。 写在后面本文重点关注并发编程的两种模型及对Actor模型做一个简单的介绍，Akka的学习会放到后面，由于对Scala不了解，网上看到的例子没有办法贴到这里与大家一起分析。 参考资料[1].MIT.6.005.Reading 17:Concurrency[2].Wikipedia.Actor Model[3].Ruben Vermeersch.Concurrency in Erlang and Scala[4].Marko Dvečko.Introduction to Concurrent Programming","tags":[{"name":"Concurrency","slug":"Concurrency","permalink":"http://elbarco.cn/tags/Concurrency/"},{"name":"Actor Model","slug":"Actor-Model","permalink":"http://elbarco.cn/tags/Actor-Model/"}]},{"title":"几个概念：并发、并行、进程、线程和协程","date":"2017-01-20T02:11:19.000Z","path":"2017/01/20/general-concepts-concurrency-parallelism-process-thread-coroutine/","text":"“有了协程框架再也不用关注线程池调度问题”，在阿里双十一电子书《不一样的技术创新》中看到这样一句话。看到这句话的时候，内心活动是这样的——当我们还在玩线程池的时候，阿里的爸爸们已经在研究调度问题并有解决方案了。所以保持对这句话的质疑和思考，有了今天的整理和学习，为进一步学习协程框架，如Akka起一个头。 并发和并行在多线程编程中，并发（Concurrency）和并行（Parallelism）这两个概念时常会被提到，但是这两个概念却不是一个意思。 并发（Concurrency）并发指的是应用程序同时处理多个任务，多个任务都能同时取得进展。 比如吃饭的时候，电话来了，停下手和嘴去接电话，打完电话继续吃饭，这叫并发。 并行（Parallelism）并行指的是应用可以将任务拆成更小的子任务，而这些子任务可以同时平行着被处理。 比如吃饭的同时，我还能打电话，这就是并行。 对比由上面可以看出，并发依赖于应用如何处理多任务。应用同时只能处理一个任务，处理完在进行下一个任务，这叫顺序地（Sequentially）执行；如果应用同时能处理多个任务，这就叫并发地。 另一方面，并行，则依赖于应用如何处理每个独立的任务。应用可以顺序的将任务从头至尾的执行完，也可以将任务分解成子任务，而子任务可以平行执行。 并发与顺序相对，而并行是并发的子集。 这里还有一个更形象的例子： 进程、线程和协程进程（Process）是计算机中的程序关于某数据集合上的一次运行活动，是系统进行资源分配和调度的基本单位，是操作系统结构的基础。在当代面向线程设计的计算机结构中，进程是线程的容器。程序是指令、数据及其组织形式的描述，进程是程序的实体。 线程（Thread）线程，有时被称为轻量级进程(Lightweight Process，LWP），是程序执行流的最小单元。一个标准的线程由线程ID，当前指令指针(PC），寄存器集合和堆栈组成。 协程（Coroutine, Fiber）协程，又称为微线程，在Lua、Python、Go中有所体现。这里参考廖雪峰的文章，举个例子：如果有两个子程序A和B：def A(): print '1' print '2' print '3'def B(): print 'x' print 'y' print 'z' 对于这两个子程序，一次调用，一次返回，返回结果很有可能是：123xyz 而协程看上去虽然也是子程序，但是在执行过程中，在子程序内部可以中断，然后转而执行别的子程序，在适当的时候在返回来接着执行。比如上面的两个子程序假设由协程执行，那么再执行A的过程中，可以随时终端，去执行B，B也有可能在执行过程中中断再去执行A，结果有可能是：12xy3z 有些类似多线程，但是协程的特点实在一个线程中执行。其优势就是具有极高的执行效率，因为执行过程中不需要县城切换，减少了CPU切换线程的开销。另一方面，避免了多线程的锁机制，不会存在同时的写冲突。 Java与协程Java语言本身不支持协程，通过第三方的库、协程框架可以实现，如注明的akka，kilim等。","tags":[{"name":"Concurrency","slug":"Concurrency","permalink":"http://elbarco.cn/tags/Concurrency/"},{"name":"Parallelism","slug":"Parallelism","permalink":"http://elbarco.cn/tags/Parallelism/"},{"name":"Coroutine","slug":"Coroutine","permalink":"http://elbarco.cn/tags/Coroutine/"}]},{"title":"Galera Cluster for MySQL介绍","date":"2016-12-28T06:27:53.000Z","path":"2016/12/28/introduction-to-galera-cluster-for-mysql/","text":"关于数据库复制数据库的复制，一般指的是在数据库集群中，数据在一个数据库服务节点拷贝到另一个数据库节点。常见的RDBMS的复制方式有两种—— Master/Slave Replication Multi-master Replication 对于主从方式的复制方式，master节点上的写操作会通过数据库日志（如MySQ了的binary log）记录，并通过网络传递给slave节点，然后由slave节点根据master节点传递的日志执行这些变更。而对于多主的复制方式，每个节点都可执行写操作，然后将写操作同步到其他节点。 无论是哪种方式，根据事务在集群中传递的方式，我们又将复制分为两类—— Synchronous Replication - 同步复制，所有的节点在一个单一事务中完成同步，即，在一个事务提交时，所有节点有相同的值。 Asynchronous Replication - 异步复制，主节点的写操作，异步的更新到其他节点中，即，当主节点事务提交时，在很短的时间内，有些节点的值与直接点不一致。 目前，我们的MySQL集群部署方式是双主，但是同一时刻所有的读写压力只在启动一台上，并没有真正意义上实现资源的合理利用，即，仅保证了高可用，但是没有保证负载均衡。 为了实现真正的数据库集群的负载均衡及高可用，我们找到了一个不错的MySQL集群的解决方案，即Galera Cluster for MySQL。它将多个数据库节点组织成一个cluster，并提供以下特性： 同步复制，主备无延迟 支持多主同时读写，保证数据一致性 集群中各个节点保存全量数据 节点添加或删除，集群具备自动监测和配置 行级锁并行复制 不需要写binlog Galera Cluster for MySQL架构使用了Galera之后，客户端和Galera节点之间交互的时序图如下所示： 当客户端执行COMMIT命令，但实际提交未发生前，所有的数据库同一事务中的变更和变更行的主键会被收集到一个write-set中，紧接着，数据库节点就会将write-set发送到所有的其他节点。 之后，write-set会使用主键执行一次验证，这个操作在集群的每个节点上都会进行，验证操作决定了是否可以应用write-set。如果验证未通过，则节点丢掉write-set并且集群回滚；如果验证通过，则事务提交，并且write-set会被应用到集群的其他节点。 上面这中复制方式又称为“基于认证的复制”（Certification Based Replication）。 那么Galera Cluster内部又是如何工作的呢？ 如上图所示，Galera Cluster有四个组件组成： DBMS - Galera Cluster支持MySQL、MariaDB和Percona XtraDB wsrep API Galera Replication Plugin Group Communication plugins 这里就不一一展开具体解释了，详细的可以参见Replication API 部分关键字解释Primary Componet除了单一节点故障之外，集群可能会由于网络原因分裂成几个组件，在这种情况下，为了避免冲突，只有一个组件可以继续修改数据库状态，而这个组件，就称为Primary Component。 Primary Component其实是一个集群，当发生集群分裂的时候，Galera Cluster会执行一个特殊的权重算法，来选举一个组件作为Primary Component，如下图所示： 如果集群具有偶数个节点，则会存在脑裂的风险。如果由于网络导致集群被分裂成恰好数量相等的两个cluster，则每个cluster都有可能保持自己的权重，并且两个都会变成non-primary状态。 所以为了能够实现自动故障转移，需要至少三个节点—— 单交换器的集群应该至少具备3个节点 跨交换机的集群应该至少具备3个交换机 跨网络的集群应该至少具备3个网络 跨数据中心的集群应该至少具备3个数据中心 Replication Configuration wsrep_cluster_name - 集群名称，所有集群中的节点，名称必须一致。 wsrep_cluster_address - 定义集群中的节点IP地址，多个地址使用逗号分割。 wsrep_node_name - 节点名称。 wsrep_node_address - 每个节点自己的IP地址。 wsrep_provider - 定义Galera Replication Plugin的路径，安装之后不确定在哪里的情况下，可以通过find / -name libgalera_smm.so 来查找。 wsrep_provider_options - 定义节点传递给wsrep provider的一些可选配置，如：gcache.size，表示节点缓存write-sets集合的磁盘空间，默认值是128M；gcache.page_size表示页存储中单页大小，整体页面存储的上限是磁盘的大小，默认值是128M。 wsrep_method - 定义了节点在单个状态快照传输（State Snapshot Transfer，指完整的数据从一个集群节点——又称为donor——拷贝到一个新加入的节点——又称为joiner——的过程）中使用的方法或者脚本，支持的方法有mysqldump和rsync两种，在大数据集的场景中，后者比前者更快。 上面是我们/etc/my.cnf.d/wsrep.cnf文件中几个配置项的解释，更多的详细内容，请参见http://galeracluster.com/documentation-webpages/reference.html。","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://elbarco.cn/tags/MySQL/"},{"name":"Galera Cluster","slug":"Galera-Cluster","permalink":"http://elbarco.cn/tags/Galera-Cluster/"},{"name":"DB","slug":"DB","permalink":"http://elbarco.cn/tags/DB/"}]},{"title":"简述StringBuilder和StringBuffer的区别","date":"2016-09-05T06:36:37.000Z","path":"2016/09/05/difference-between-stringbuilder-and-stringbuffer/","text":"从StringBuilder和StringBuffer的不同说起最近在搬砖的时候，发现在拼接字符串的时候，有人习惯使用StringBuffer，有人习惯使用StringBuilder，于是想到了之前在知乎上看到的这个讨论：国内Java面试总是问StringBuffer，StringBuilder区别是啥？档次为什么这么低，果然这在面试中只是一道预热筛选题嘛，不过一下子让我答，却并不能立刻回答上来区别，于是顺手Google了一下，在StringBuilder和StringBuffer的API(JDK1.7)里找到了答案。下面就做一下简述—— 首先，StringBuffer和StringBuilder都是可变字符串，但是前者是线程安全的，因为在调用StringBuffer的操作时是同步的，在源代码中看到的就是方法上加了synchronized关键字：...public synchronized StringBuffer append(String str) &#123; super.append(str); return this;&#125;... 而在StringBuilder的源码中，我们看到的是这样的：public StringBuilder append(String str) &#123; super.append(str); return this; &#125; 上面仅截取部分代码，更多的代码大家可自行查看。 在StringBuffer的API说明中，提到，在JDK5中，开始提供了功能相同，但是非线程安全、不使用synchronized、性能更好的类StringBuilder，在StringBuilderAPI说明中，有提到这么一句话： Instances of StringBuilder are not safe for use by multiple threads. If such synchronization is required then it is recommended that StringBuffer be used. 即只有在同步是必要的情况下，才建议使用StringBuffer。 再论拼接字符串的不同方法和效率至此，区别就简述完了。什么，这就完了？摔……按照面试套路，理论上应该是进入下一话题了，不过这里我们还是要继续，现在就抛出一个非常基础常见的套路问题—— 问：常见的拼接字符串的方法有哪些？ 答案是：String的concat方法、+操作符；StringBuffer和StringBuilder的append方法。 再问：上面几种方法效率如何？ 答案也很简单，当然是StringBuilder&gt;StringBuffer&gt;concat或+操作符。 回答完是什么之后，我们再问问为什么。首先，StringBuffer的每个append操作都是同步的，所以比StringBuilder要慢，那么为什么都比concat或者+效率搞呢？于是又Google一下，找到了这个讨论（Google大法好！Stackoverflow大法好！Orz..），里面提到，在JDK1.6之后，使用”+”操作符时，编译器会自动使用StringBuilder将两个字符append到一起，比如我们代码里是这样写的：String one = \"abc\";String two = \"xyz\";String three = one + two; 在编译的时候，String three会被编译成：String three = new StringBuilder().append(one).append(two).toString(); 乍一看，是效率了很多，但是如果在循环中这样干：String out = \"\";for( int i = 0; i &lt; 10000 ; i++ ) &#123; out = out + i;&#125;return out; 那么在编译时，可能得到的内容就是这样子的：String out = \"\";for( int i = 0; i &lt; 10000; i++ ) &#123; out = new StringBuilder().append(out).append(i).toString();&#125;return out; 此时，我们其实都知道应该这样写：StringBuilder out = new StringBuilder();for( int i = 0 ; i &lt; 10000; i++ ) &#123; out.append(i);&#125;return out.toString(); 这也反映了，编译器一定程度上可以帮助我们优化，但是写出高效的代码，还需要我们自己。 另一个角度较真儿的验证上面的代码是13年答主在JDK1.6中测试的结果，又有一位较真儿的朋友，在不同的JDK版本中进行了测试，全文见Java StringBuilder myth debunked，最终得到了下面的图表： 使用+操作符 使用StringBuilder 使用StringBuilder的基准 这位童鞋贴心的把测试用的代码托管在Github上，有兴趣的可以去看一下。最终这篇文章得出的结论就是——通过对字节码的分析，我们得到了答案，显而易见的是，使用StringBuilder是可以提高性能的。文章开篇还提到这么一句话—— Concatenating two Strings with the plus operator is the source of all evil — Anonymous Java dev 与大家共勉。","tags":[{"name":"Java","slug":"Java","permalink":"http://elbarco.cn/tags/Java/"}]},{"title":"使用Logrotate管理MongoDB日志-后记","date":"2016-07-01T07:23:11.000Z","path":"2016/07/01/使用Logrotate管理MongoDB日志-后记/","text":"发现问题昨天完成了Logrotate管理MongoDB日志的配置工作，手动执行验证通过，但是今天查看日志切换情况，却没有如期待的一般——在日志目录下仅有一个mongodb.log文件——日志没有切换？！ 分析确定执行情况为了确定配置的每天执行的MongoDB日至切换是否执行过，我们首先查看/var/log/cron，下面是截取了部分内容： ...Jul 1 03:01:02 localhost anacron[19152]: Will run job &apos;cron.daily&apos; in 49 min.Jul 1 03:01:02 localhost anacron[19152]: Jobs will be executed sequentially...Jul 1 03:50:02 localhost anacron[19152]: Job &apos;cron.daily&apos; started...Jul 1 03:50:02 localhost run-parts(/etc/cron.daily)[19251]: starting logrotateJul 1 03:50:02 localhost run-parts(/etc/cron.daily)[19267]: finished logrotate...Jul 1 03:53:49 localhost anacron[19152]: Job &apos;cron.daily&apos; terminatedJul 1 03:53:49 localhost anacron[19152]: Normal exit (1 job run) 可以看到，在7月1日凌晨3点50左右确实执行了每日的计划任务，并且cron.daily正常退出。但是Logrotate有没有出错还要继续分析。 查看/var/log/message，在同样的时间段，发现了这样一条错误信息：Jul 1 03:50:02 localhost logrotate: ALERT exited abnormally with [1] 而这段错误信息，正是Logrotate每日执行的计划任务脚本中执行异常退出的提示信息：[root@localhost ~]# cat /etc/cron.daily/logrotate #!/bin/sh/usr/sbin/logrotate /etc/logrotate.conf &gt;/dev/null 2&gt;&amp;1EXITVALUE=$?if [ $EXITVALUE != 0 ]; then /usr/bin/logger -t logrotate &quot;ALERT exited abnormally with [$EXITVALUE]&quot;fiexit 0 原因探究原因探究的过程非常简单——Google，所以略。 噗……友谊的小船说翻就翻！（╯－_－）╯╧╧ 回到正题。 引起该问题的原因与SELinux有关。使用getenforce查询SELinux状态：[root@localhost ~]# getenforceEnforcing 可以看到，我们当前的SELinux处于Enforcing模式下，此时，因为我们在之前MongoDB轮换配置文件中，使用了除了/var/log/之外的路径，那么： SELinux was restricting the access to logrotate on log files in directories which does not have the required SELinux file context type. “/var/log” directory has “var_log_t” file context, and logrotate was able to do the needful. 即，/var/log目录具有var_log_t文件上下文，如果要使用Logrotate，我们的日志目录也应该具备这个向下问。所以解决方案就是为配置文件中使用的日志目录设置文件上下文，可以通过下面两个命令做到：semanage fcontext -a -t var_log_t &lt;directory/logfile&gt;restorecon -v &lt;directory/logfile&gt; 第一个命令，用于设置上下文，第二个命令用于对于需要设置上下文的目录活文件，递归的设置。 解决过程检查安装情况执行man semanage或semanage -h检查是否安装semanage:[root@localhost ~]# man semanageNo manual entry for semanage[root@localhost ~]# semanage -h-bash: semanage: command not found 这里我们并没有找到这个命令，所以需要安装相关软件，如果已安装，则跳过这一步。 安装找到是什么软件提供了semanage命令：[root@localhost ~]# yum provides */semanageLoaded plugins: fastestmirror, refresh-packagekit, securityLoading mirror speeds from cached hostfile * base: mirrors.yun-idc.com * extras: mirrors.yun-idc.com * updates: mirrors.yun-idc.comlibsemanage-devel-2.0.43-5.1.el6.x86_64 : Header files and libraries used to build policy manipulation toolsRepo : baseMatched from:Filename : /usr/include/semanagelibsemanage-devel-2.0.43-5.1.el6.i686 : Header files and libraries used to build policy manipulation toolsRepo : baseMatched from:Filename : /usr/include/semanagepolicycoreutils-python-2.0.83-29.el6.x86_64 : SELinux policy core python utilitiesRepo : baseMatched from:Filename : /usr/sbin/semanagepolicycoreutils-python-2.0.83-29.el6.x86_64 : SELinux policy core python utilitiesRepo : installedMatched from:Filename : /usr/sbin/semanage 这里，我们手动安装一下policycoreutils-python即可：[root@localhost ~]# yum -y install policycoreutils-python 执行命令安装完毕，执行命令:[root@localhost ~]# semanage fcontext -a -t var_log_t &apos;/mongoData/mongodb_log/mongodb.log&apos;[root@localhost ~]# restorecon -Frvv /mongoData/mongodb_log/mongodb.log 设置完file context之后，记录会被持久化到/etc/selinux/targeted/contexts/files/file_contexts.local中，我们可以检查一下：[root@localhost ~]# cat /etc/selinux/targeted/contexts/files/file_contexts.local# This file is auto-generated by libsemanage# Do not edit directly./mongoData/mongodb_log/mongodb.log system_u:object_r:var_log_t:s0 此时，补锅工作结束。 更多详细内容，点击这里查看参考文章","tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"http://elbarco.cn/tags/MongoDB/"},{"name":"Logrotate","slug":"Logrotate","permalink":"http://elbarco.cn/tags/Logrotate/"}]},{"title":"使用Logrotate管理MongoDB日志","date":"2016-06-30T07:26:03.000Z","path":"2016/06/30/使用Logrotate管理MongoDB日志/","text":"痛点前段时间需要查询MongoDB日志，惊觉MongoDB的日志并没有配置自动切换轮转，这会导致在繁忙的业务下，日志增长量惊人。面对海量的MongoDB日志，开发和运维人员查看日志变的十分不方便，所以需要寻求使日志自动切换轮转的方式。 选型通过查看MongoDB官方文档，知悉MongoDB提供了几种轮转日志文件的策略，详见这里（据说新版本的MongoDB已经完成了自动的日志轮转功能？）。其中，可以使用MongoDB提供的logRotate命令或者通过向mongod进程发送SIGUSR1信号来实现。 然而看很多文章中均表示，MongoDB本身提供的logRotate机制存在很多问题，比如由于其不稳定性，会造成日志轮换中mongodb进程终止，不提供旧日志的压缩，即使轮转切换日志，还是占用了很多磁盘空间；日志文件重命名格式mongodb.log.2016-10-22T17-44-44不友好等等。所以我们在选择时就会变得很小心，尽量避免使用其内置logRotate。 被广泛认可的方案是通过Logrotate进行日志管理，其中可以执行脚本实现向mongod进程发送SIGUSR1信号。 Logrotate简介Logrotate可以帮助我们管理日志文件。比如周期性的读取日志、压缩日志、备份日志、创建新的日志文件等，基本上你希望做的，都能实现。通常来讲，常被用于来避免单个日志文件增长为难以处理的大小。也常被用于删除旧的日志文件来释放磁盘空间。 通常来讲，默认的Logrotate会作为/etc/cron.daily/中的一个计划任务每天执行一次。[root@localhost etc]# ls /etc/cron.daily/cups logrotate makewhatis.cron mlocate.cron 配置说明配置Logrotate通过编辑两处配置文件来完成： /etc/logrotate.conf /etc/logrotate.d/下面的不同服务特定的配置 logrotate.conf包含了通用的配置，下面是一个默认配置： # see &quot;man logrotate&quot; for details# rotate log files weeklyweekly# keep 4 weeks worth of backlogsrotate 4# create new (empty) log files after rotating old onescreate# use date as a suffix of the rotated filedateext# uncomment this if you want your log files compressed#compress# RPM packages drop log rotation information into this directoryinclude /etc/logrotate.d# no packages own wtmp and btmp -- we&apos;ll rotate them here/var/log/wtmp &#123; monthly create 0664 root utmp minsize 1M rotate 1&#125;/var/log/btmp &#123; missingok monthly create 0600 root utmp rotate 1&#125;# system-specific logs may be also be configured here. 上面的通用配置我们不用过多关心，因为我们具体服务的具体配置在目录/etc/logrotate.d/下。在这个目录里，许多应用在安装后已经设置了Logrotate，比如httpd，nginx等。下面，我们拿nginx的配置做一个简要的说明： [root@localhost ~]# cd /etc/logrotate.d/[root@localhost logrotate.d]# lltotal 44-rw-r--r--. 1 root root 185 Aug 2 2013 httpd-rw-r--r--. 1 root root 871 Jun 22 2015 mysqld-rw-r--r--. 1 root root 302 Apr 26 23:10 nginx-rw-r--r--. 1 root root 219 Nov 23 2013 sssd-rw-r--r--. 1 root root 210 Aug 15 2013 syslog-rw-r--r--. 1 root root 100 Feb 22 2013 yum[root@localhost logrotate.d]# cat nginx /var/log/nginx/*.log &#123; daily missingok rotate 52 compress delaycompress notifempty create 640 nginx adm sharedscripts postrotate [ -f /var/run/nginx.pid ] &amp;&amp; kill -USR1 `cat /var/run/nginx.pid` endscript&#125; 首先第一行，配置了要自动轮换的日志文件的路径/var/log/nginx/*.log，即针对在/var/log/nginx下的*.log文件进行轮换。 daily：每天轮换日志。可选选项有daily，weekly，monthly和yearly missingok：找不到*.log文件也是ok的，不要方…… rotate 52：保留52个日志文件，其他更老旧的日志文件删掉（在这里要配合daily使用，即保留52天的日志文件） compress：压缩日志文件（默认gzip格式） delaycompress：延迟压缩任务直到第二次轮换日志才进行。结果会导致你会有当前的日志文件，一个较旧的没有被压缩过的日志文件和一些压缩过的日志文件 compresscmd：设置使用什么命令来进行压缩，默认是gzip uncompresscmd：设置解压的命令，默认是gunzip。 notifempty：不轮转空文件 create 640 nginx adm：创建一个新的日志文件，并设置权限permissions/owner/group。本例中，使用用户ngxin和用户组adm创建了一个日志文件，文件权限为640.在很多系统中，owner和group一般都会是root。 sharedscripts：在所有的日志轮换完毕后执行postrotate脚本。如果该项没有设置，则会在每个匹配的文件轮换后执行postrotate脚本。 postrotate：轮换日志完成后运行的脚本。 更多的选项，参见这里。 使用Logrotate管理MongoDB日志经过上面对Logrotate的简单说明，这是我们就可以开始使用它来管理MongoDB日志了。 找到日志文件及PID记录文件首先，我们的MongoDB启动配置中，指定了logpath和pidfilepath： logpath=/mongoData/mongodb_log/mongodb.log pidfilepath=/mongoData/mongodb.pid mongod.pid和文件/mongoData/mongodb_data/mongod.lock中都存有mongod的PID，用这两个文件都可以获取PID，任选其一即可。 编写配置文件通过man logrotate查看详细参数，结合业务需求，编写的配置文件如下： /mongoData/mongodb_log/mongodb.log &#123; daily missingok rotate 30 copytruncate dateext compress notifempty create 644 root root sharedscripts postrotate /bin/kill -SIGUSR1 &apos;cat /mongoData/mongodb.pid 2&gt; /dev/null&apos; 2&gt; /dev/null || true endscript&#125; 这里做一下简单说明： copytruncate 这个命令很重要，意思是在创建副本后，将原文件清空，而不是将原文件重命名并创建新的日志文件。这样可以避免有些应用继续向原日志文件中输出，而不是新的日志文件。在没有配置这个命令之前，mongodb一直向轮换后的带时间戳的旧文件中输出日志。 dateext 用于切换日志文件时命名成为mongodb.log-YYYYMMDD格式。 create 644 root root 644权限，即-rw-r--r--与之前的日志文件保持一直的权限即可。 验证编写完配置文件之后，我们将文件拷贝到/etc/logrotate.d/下，执行命令logrotate -f -v /etc/logrotate.d/&lt;YOUR_CONFIG_FILE_NAME&gt;来验证日志是否被轮换了，示例执行结果如下： root@localhost mongodb_log]# logrotate -f -v /etc/logrotate.d/mongologrotatereading config file /etc/logrotate.d/mongologrotatereading config info for /mongoData/mongodb_log/mongodb.logHandling 1 logsrotating pattern: /mongoData/mongodb_log/mongodb.log forced from command line (30 rotations)empty log files are not rotated, old logs are removedconsidering log /mongoData/mongodb_log/mongodb.log log needs rotatingrotating log /mongoData/mongodb_log/mongodb.log, log-&gt;rotateCount is 30dateext suffix &apos;-20160630&apos;glob pattern &apos;-[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]&apos;glob finding old rotated logs failedcopying /mongoData/mongodb_log/mongodb.log to /mongoData/mongodb_log/mongodb.log-20160630set default create contexttruncating /mongoData/mongodb_log/mongodb.logrunning postrotate scriptcompressing log with: /bin/gzip[root@localhost mongodb_log]# lltotal 69604-rw-r--r--. 1 root root 37092 Jun 30 13:24 mongodb.log-rw-r--r--. 1 root root 1047190 Jun 30 13:24 mongodb.log-20160630.gz 特别指出由于我们的服务期开启了SELinux，并且是Enforcing模式下，会造成非/var/log/目录下的logrotate操作失败，所以需要执行下面的命令：[root@localhost ~]# semanage fcontext -a -t var_log_t &apos;/mongoData/mongodb_log/mongodb.log&apos;[root@localhost ~]# restorecon -Frvv /mongoData/mongodb_log/mongodb.log 上面的第一条命令用来定义mongodb.log这个文件的上下文，记录会被持久化到/etc/selinux/targeted/contexts/files/file_contexts.local里，我们可以验证一下。[root@localhost ~]# cat /etc/selinux/targeted/contexts/files/file_contexts.local# This file is auto-generated by libsemanage# Do not edit directly./mongoData/mongodb_log/mongodb.log system_u:object_r:var_log_t:s0 上面的第二条命令，可以递归的设置上下文，如果我们传入的是一个目录，则目录下的所有子目录及文件都会被递归的统一设置。 关于这个问题的说明，可以点击这里查看更多说明和解释。 结语至此，我们边完成了使用Logrotate来管理MongoDB日志了。可以看到，Logrotate十分强大，在使用时，可以通过man logrotate查看一下具体参数，知其然并知其所以然，让其更好地为我们所用。","tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"http://elbarco.cn/tags/MongoDB/"},{"name":"Logrotate","slug":"Logrotate","permalink":"http://elbarco.cn/tags/Logrotate/"}]},{"title":"MongoDB复制集Secondary节点持续Recovering状态解决办法","date":"2016-04-26T21:51:27.000Z","path":"2016/04/27/MongoDB复制集Secondary节点持续Recovering状态解决办法/","text":"前段时间发现MongoDB Replica Set中的某个Secondary节点一直持续Recovering状态，无法恢复，且上次操作时间（optimeDate）已经是N天前了，经过查看官方文档，得知出现这种情况的原因在于复制集中主节点（Primary）一直写入oplog，而从节点（Secondary）的复制过程远远落后，赶不上主节点的oplog写入，就像赌气的孩子跑步一样，赶不上前面的小伙伴，索性一赌气就不走了……当遇到这种情况的时候，是不可能指望从节点自己恢复的，需要我们手动重新同步（initial sync）。 官方给出了两种执行重新同步的方式—— 完全清空数据目录然后重启mongod服务 在其他成员的数据目录下拷贝最近的数据然后重启mongod服务 这里，偷懒不想打包scp数据，索性采用了第一种方式： 停止mongod服务：可在mongo shell中执行db.shutdownServer()来关闭mongod服务，也可以在shell中直接敲mongod --shutdown，或者简单粗暴直接kill -2 &lt;PID&gt;（这里不推荐-9，会造成下次启动不起来的情况，需要删除dbPath目录下的mongo.lock再尝试重新启动）。 对旧的dbPath的目录重命名，以做备份 启动mongod，指向新的空的dbPath目录 简单三步，MongoDB就会重新进行初始化同步，受限于数据量和网络环境等因素的影响，重新同步时间有长有短。重新同步完毕后，打开mongo shell查看复制集状态，一般情况下，这个从节点状态就会恢复正常了。然后要做的就是验证主从数据一致性，确保没问题之后，重命名过的dbPath目录可以删除了。 第二种方式，利用其它成员的最近数据进行启动的操作可见官方文档，这里就不赘述了。","tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"http://elbarco.cn/tags/MongoDB/"}]},{"title":"为CentOS6.5安装Kernel3.10","date":"2016-03-12T01:08:34.000Z","path":"2016/03/12/为CentOS6-5安装Kernel3-10/","text":"最近有想学习下Docker，在Linux下安装Docker对内核的要求至少是3.10以上，然而CentOS 6.5内核版本是2.6，所以首先要做的就是为CentOS 6.5安装3.10的Kernel。 我们并不需要自己编译安装，而是有小伙伴在在ELRepo上为我们准备好了一个package，我们只关心如何安装就好了。 启用ELReporpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org rpm -Uvh http://www.elrepo.org/elrepo-release-6-6.el6.elrepo.noarch.rpm 安装Kernelyum --enablerepo=elrepo-kernel install kernel-lt 配置grub需要编辑/etc/grub.conf来更改kernel顺序，将默认的1改为0.所以看起来应该是酱婶儿的： default=0timeout=5splashimage=(hd0,0)/boot/grub/splash.xpm.gzhiddenmenutitle CentOS (3.10.99-1.el6.elrepo.x86_64) root (hd0,0) kernel /boot/vmlinuz-3.10.99-1.el6.elrepo.x86_64 ro root=UUID=94e4e384-0ace-437f-bc96-057dd64f42ee rd_NO_LUKS rd_NO_LVM LANG=en_US.UTF-8 rd_NO_MD SYSFONT=latarcyrheb-sun16 crashkernel=auto KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM rhgb quiet initrd /boot/initramfs-3.10.99-1.el6.elrepo.x86_64.imgtitle CentOS (2.6.32-573.12.1.el6.x86_64) root (hd0,0) kernel /boot/vmlinuz-2.6.32-573.12.1.el6.x86_64 ro root=UUID=94e4e384-0ace-437f-bc96-057dd64f42ee rd_NO_LUKS rd_NO_LVM LANG=en_US.UTF-8 rd_NO_MD SYSFONT=latarcyrheb-sun16 crashkernel=auto KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM rhgb quiet initrd /boot/initramfs-2.6.32-573.12.1.el6.x86_64.img... 重启并查看reboot 重启后通过uname -a来查看内核版本[root@iZ2853cmjatZ ~]# uname -aLinux iZ2853cmjatZ 3.10.99-1.el6.elrepo.x86_64 #1 SMP Fri Mar 4 11:53:07 EST 2016 x86_64 x86_64 x86_64 GNU/Linux 大功告成！","tags":[{"name":"Linux","slug":"Linux","permalink":"http://elbarco.cn/tags/Linux/"},{"name":"CentOS6.5","slug":"CentOS6-5","permalink":"http://elbarco.cn/tags/CentOS6-5/"}]},{"title":"基于Redis的Tomcat集群Session共享","date":"2016-03-10T08:18:06.000Z","path":"2016/03/10/基于Redis的Tomcat集群Session共享/","text":"目前的web应用集群，使用Nginx做负载均衡，将upstream配置成ip_hash的方式，这种模式下，Nginx会根据来源IP和后端配置来做hash分配，确保固定IP只访问一个后端。 upstream YOUR_NAME &#123; ip_hash; server 192.168.8.15:8080; server 192.168.8.17:8080;&#125; 但是，由于固定某个IP只能访问单独的一个后端，如果宕机或者需要升级程序时做停机重启，正在操作的用户就会退出到登录页面，不仅用户体验很差，而且正在做的操作不能保证成功，容易产生脏数据等。 从Nginx upstream配置说起首先，来看一下Nginx upstream的几种负载均衡策略： 1）轮询upstream YOUR_NAME &#123; server 192.168.8.15:8080; server 192.168.8.17:8080;&#125; 2）权重：该策略可解决服务器性能不等的情况下轮询比率的调配upstream YOUR_NAME &#123; server 192.168.8.15:8080 weight=2; server 192.168.8.17:8080 weight=3;&#125; 3）ip_hashupstream YOUR_NAME &#123; ip_hash; server 192.168.8.15:8080; server 192.168.8.17:8080;&#125; 4）fair：需要安装Upstream Fair Balancer Module。该策略根据后端服务的响应时间来分配，响应时间短的后端优先分配upstream YOUR_NAME &#123; server 192.168.8.15:8080; server 192.168.8.17:8080; fair;&#125; 5）一致性Hash：需要安装Upstream Consistent Hash Module，该策略可以根据给定的字符串进行Hash分配，具体参见官方Wiki。 由此可见，我们迫切的需要使用轮训的方式来做负载均衡，那对于大规模集群部署的web应用来讲，轮训的方式就要Session必须进行共享。 Session共享机制在集群系统下实现Session共享机制一般有如下两种方案： 应用服务器间的Session复制共享（如Tomcat自带的Session共享） 基于缓存数据库的Session共享（如使用Memcached、Redis） 应用服务器间的Session复制共享Session复制共享，主要是指集群环境下，多台应用服务器之间同步Session，使其保持一致，对外透明。如果其中一台服务器发生故障，根据负载均衡的原理，Web服务器（Apache/Nginx）会遍历寻找可用节点，分发请求，由于Session已同步，故能保证用户的Session信息不会丢失。 此方案的不足之处： 技术复杂,必须在同一种中间件之间完成(如Tomcat-Tomcat之间). Session复制带来的性能损失会快速增加.特别是当Session中保存了较大的对象,而且对象变化较快时, 性能下降更加显著. 这种特性使得Web应用的水平扩展受到了限制。 Session内容序列化（Serialize），会消耗系统性能。 Session内容通过广播同步给成员，会造成网络流量瓶颈，即便是内网瓶颈。 基于缓存数据库的Session共享基于缓存数据库的Session共享是指使用如Memcached、Redis等Cache DB来存取Session信息：应用服务器接受新请求将Session信息保存到Cache DB中，当应用服务器发生故障，Web服务器（Apache/Nginx）会遍历寻找可用节点，分发请求，当应用服务器发现Session不在本机内存，则会去Cache DB中查找，如果找到，则复制到本机，这样就实现了Session的共享和高可用。 我选用的是Redis而不是Memcached，是因为Redis具有更丰富的数据结构，比如可以为Key指定过期时间，从而不需要我们定期的刷新缓存。而Memcached做不到，所有就有了这样一个合理的方案—— 在GitHub有这样一个开源工具tomcat-redis-session-manager，可以帮我们实现基于Redis的Session共享，然而直接拿来用的话，Session的key直接就是SessionID，没有一个统一的前缀，所以经过一些小改造，代码已托管到这里，可以通过Tomcat/conf/server.xml的最下面的中增加sessionCookieName配置你想要的Redis中key的前缀，如下所示： &lt;Context docBase=\"/root/YOUR_WEB_APP\" path=\"\" reloadable=\"true\" sessionCookieName=\"YOURJSessionID\" /&gt; 闲话少说，下面开始讲解如何使用：1）下载源码编译成Jar包，讲 tomcat-redis-session-manager-1.2.jar 、jedis-2.6.1.jar、commons-pool2-2.2.jar拷贝到Tomcat目录下的lib中（Jedis、commons-pool2版本任意）2）在Tomcat的conf目录下，编辑context.xml。如果你是用Redis单点，则可以仿照如下配置：&lt;Valve className=\"com.orangefunction.tomcat.redissessions.RedisSessionHandlerValve\" /&gt;&lt;Manager className=\"com.orangefunction.tomcat.redissessions.RedisSessionManager\" host=\"192.168.8.38\" port=\"6379\" database=\"1\" maxInactiveInterval=\"60\" /&gt; 如果是Redis集群环境，则可仿照如下配置：&lt;Valve className=\"com.orangefunction.tomcat.redissessions.RedisSessionHandlerValve\" /&gt;&lt;Manager className=\"com.orangefunction.tomcat.redissessions.RedisSessionManager\" database=\"1\" maxInactiveInterval=\"60\" sentinelMaster=\"mymaster\" sentinels=\"192.168.8.43:26379,192.168.8.45:26379,192.168.8.47:26379\"/&gt; 参数均可选，详见上面tomcat-redis-session-managerGithub中的说明。 关于maxInactiveInterval，即失效时间，这里做一些说明： 即使在这里配置的maxInactiveInterval是60s，如果web.xml配置了session的失效时间，则以web.xml为准。另，如果有一下三处配置了Session的失效时间，则下面的配置覆盖上面的配置: TOMCAT_HOME/conf/web.xml WebApplication/webapp/WEB-INF/web.xml 写在代码中的值 : HttpSession.setMaxInactiveInterval(int) 即实际生效顺序是:HttpSession.setMaxInactiveInterval(int) &gt; $WebApplication/webapp/WEB-INF/web.xml &gt; $TOMCAT_HOME/conf/web.xml 启动Tomcat，访问应用，即可在Redis中看到效果。 关于测试，可以将Nginx Upstream配置为轮询后，仅留一台应用服务器启动，登陆操作，然后启动另外一台，停止第一台服务，继续操作，发现并未受任何影响，即可。 参考nginx upstream的几种配置方式：http://alwaysyunwei.blog.51cto.com/3224143/1239182Load Balancing via Nginx Upstream :http://nginx.org/en/docs/http/load_balancing.htmlTomcat7基于Redis的Session共享：https://yq.aliyun.com/articles/1298Tomcat Session Timeout Web.xml: http://stackoverflow.com/questions/13463036/tomcat-session-timeout-web-xml","tags":[{"name":"Reids","slug":"Reids","permalink":"http://elbarco.cn/tags/Reids/"},{"name":"Tomcat","slug":"Tomcat","permalink":"http://elbarco.cn/tags/Tomcat/"}]},{"title":"如何在CentOS7上安装和配置VNCServer","date":"2016-03-09T09:22:56.000Z","path":"2016/03/09/如何在CentOS7上安装和配置VNCServer/","text":"原文地址：传送门转载自：Linux.cn 这是一个关于怎样在你的 CentOS 7 上安装配置 VNC 服务的教程。当然这个教程也适合 RHEL 7 。在这个教程里，我们将学习什么是 VNC 以及怎样在 CentOS 7 上安装配置 VNC 服务器。 我们都知道，作为一个系统管理员，大多数时间是通过网络管理服务器的。在管理服务器的过程中很少会用到图形界面，多数情况下我们只是用 SSH 来完成我们的管理任务。在这篇文章里，我们将配置 VNC 来提供一个连接我们 CentOS 7 服务器的方法。VNC 允许我们开启一个远程图形会话来连接我们的服务器，这样我们就可以通过网络远程访问服务器的图形界面了。 VNC 服务器是一个自由开源软件，它可以让用户可以远程访问服务器的桌面环境。另外连接 VNC 服务器需要使用 VNC viewer 这个客户端。 一些 VNC 服务器的优点： 远程的图形管理方式让工作变得简单方便。 剪贴板可以在 CentOS 服务器主机和 VNC 客户端机器之间共享。 CentOS 服务器上也可以安装图形工具，让管理能力变得更强大。 只要安装了 VNC 客户端，通过任何操作系统都可以管理 CentOS 服务器了。 比 ssh 图形转发和 RDP 连接更可靠。 那么，让我们开始安装 VNC 服务器之旅吧。我们需要按照下面的步骤一步一步来搭建一个可用的 VNC。 首先，我们需要一个可用的桌面环境（X-Window），如果没有的话要先安装一个。 注意：以下命令必须以 root 权限运行。要切换到 root ，请在终端下运行“sudo -s”，当然不包括双引号（“”） 这里我操作时，运维给准备了一台CentOS 7的环境已经安装了桌面。所以第一步我直接跳过，而是SSH到服务器，直接进行VNC的安装，不过还是保留原文的全部步骤吧。 安装 X-Window首先我们需要安装 X-Window，在终端中运行下面的命令，安装会花费一点时间。 # yum check-update# yum groupinstall &quot;X Window System&quot; # yum install gnome-classic-session gnome-terminal nautilus-open-terminal control-center liberation-mono-fonts ### 设置默认启动图形界面# unlink /etc/systemd/system/default.target# ln -sf /lib/systemd/system/graphical.target /etc/systemd/system/default.target # reboot 在服务器重启之后，我们就有了一个工作着的 CentOS 7 桌面环境了。 现在，我们要在服务器上安装 VNC 服务器了。 安装 VNC 服务器现在要在我们的 CentOS 7 上安装 VNC 服务器了。我们需要执行下面的命令。 # yum install tigervnc-server -y 配置 VNC然后，我们需要在 /etc/systemd/system/ 目录里创建一个配置文件。我们可以将 /lib/systemd/sytem/vncserver@.service 拷贝一份配置文件范例过来。 # cp /lib/systemd/system/vncserver@.service /etc/systemd/system/vncserver@:1.service 接着我们用自己最喜欢的编辑器打开 /etc/systemd/system/vncserver@:1.service ，找到下面这几行，用自己的用户名替换掉 。举例来说，我的用户名是 linoxide 所以我用 linoxide 来替换掉 ： ExecStart=/sbin/runuser -l &lt;USER&gt; -c &quot;/usr/bin/vncserver %i&quot;PIDFile=/home/&lt;USER&gt;/.vnc/%H%i.pid 替换成 ExecStart=/sbin/runuser -l linoxide -c &quot;/usr/bin/vncserver %i&quot;PIDFile=/home/linoxide/.vnc/%H%i.pid 如果是 root 用户则 ExecStart=/sbin/runuser -l root -c &quot;/usr/bin/vncserver %i&quot;PIDFile=/root/.vnc/%H%i.pid 好了，下面重启 systemd. # systemctl daemon-reload``` 最后还要设置一下用户的 VNC 密码。要设置某个用户的密码，必须要有能通过 sudo 切换到用户的权限，这里我用 `linoxide` 的权限，执行`su linoxide`就可以了。```shell# su linoxide$ sudo vncpasswd 确保你输入的密码多于6个字符。 开启服务用下面的命令（永久地）开启服务： $ sudo systemctl enable vncserver@:1.service 启动服务。 $ sudo systemctl start vncserver@:1.service 防火墙设置我们需要配置防火墙来让 VNC 服务正常工作。 $ sudo firewall-cmd --permanent --add-service vnc-server$ sudo systemctl restart firewalld.service 现在就可以用 IP 和端口号（LCTT 译注：例如 192.168.1.1:1 ，这里的端口不是服务器的端口，而是视 VNC 连接数的多少从1开始排序）来连接 VNC 服务器了。 用 VNC 客户端连接服务器好了，现在已经完成了 VNC 服务器的安装了。要使用 VNC 连接服务器，我们还需要一个在本地计算机上安装的仅供连接远程计算机使用的 VNC 客户端。 你可以用像 Tightvnc viewer 和 Realvnc viewer 的客户端来连接到服务器。 要用更多的用户连接，需要创建配置文件和端口，请回到第3步，添加一个新的用户和端口。你需要创建 vncserver@:2.service 并替换配置文件里的用户名和之后步骤里相应的文件名、端口号。请确保你登录 VNC 服务器用的是你之前配置 VNC 密码的时候使用的那个用户名。 VNC 服务本身使用的是5900端口。鉴于有不同的用户使用 VNC ，每个人的连接都会获得不同的端口。配置文件名里面的数字告诉 VNC 服务器把服务运行在5900的子端口上。在我们这个例子里，第一个 VNC 服务会运行在5901（5900 + 1）端口上，之后的依次增加，运行在5900 + x 号端口上。其中 x 是指之后用户的配置文件名 vncserver@:x.service 里面的 x 。 在建立连接之前，我们需要知道服务器的 IP 地址和端口。IP 地址是一台计算机在网络中的独特的识别号码。我的服务器的 IP 地址是96.126.120.92，VNC 用户端口是1。 执行下面的命令可以获得服务器的公网 IP 地址（译注：如果你的服务器放在内网或使用动态地址的话，可以这样获得其公网 IP 地址）。 # curl -s checkip.dyndns.org|sed -e &apos;s/.*Current IP Address: //&apos; -e &apos;s/&lt;.*$//&apos; 总结好了，现在我们已经在运行 CentOS 7 / RHEL 7 的服务器上安装配置好了 VNC 服务器。VNC 是自由开源软件中最简单的一种能实现远程控制服务器的工具，也是一款优秀的 Teamviewer Remote Access 替代品。VNC 允许一个安装了 VNC 客户端的用户远程控制一台安装了 VNC 服务的服务器。 关闭 VNC 服务。 # systemctl stop vncserver@:1.service 禁止 VNC 服务开机启动。 # systemctl disable vncserver@:1.service 关闭防火墙。 # systemctl stop firewalld.service","tags":[{"name":"Linux","slug":"Linux","permalink":"http://elbarco.cn/tags/Linux/"},{"name":"CentOS7","slug":"CentOS7","permalink":"http://elbarco.cn/tags/CentOS7/"}]},{"title":"一个成功的Git分支模型","date":"2016-03-08T02:29:22.000Z","path":"2016/03/08/一个成功的Git分支模型/","text":"前言从大公司跳到了小团队，版本控制软件从Git换到了SVN，然而前段时间，头儿让我研究下如何搭建私有Git服务器，如何“优雅”地使用Git。 针对如何搭建私有Git服务器，我选用的是GitLab，有一键安装包，也有很多Step by Step的教程，可自行Google。本文就上面提出的后两个问题，参考文章《A successful Git branching model》讲述如何合理的使用Git branch进行开发和版本管理。 先来张图： 详细展开主要分支在这个模型中，中央仓库持有两个生命周期无限长的主要分支： master develop 我们认为，origin/master这个主要分支上，源码的HEAD永远保持生产就绪的状态。origin/develop这个主要分支的源码HEAD则永远代表了最新提交的开发变更，所以也被称为是“集成分支”。该分支可以用于每晚的自动化构建所使用。 当develop分支的代码能够到达一个稳定点，并且已经准备好进行版本发布，所以的变更应当合并到master上，并且用版本号标注。具体操作后详细谈到。 因此，每当变更最终合并到master分支，这就是一个新的生产版本。对待这个分支，要极其严格，所以理论上来讲，可以使用一个Git hook脚本来进行自动化构建，每当有新内容提交到master，脚本自动将软件发布到成产环境。 支持性分支在这个模型中，有各类支持性分支来协助团队成员的并行开发，方便跟踪功能特性，准备生产版本和快速修复生产问题。与主要分支不同的是，这三个支持性分支是有有限生命周期的，最终会被移除。 这里使用的三类分支分别是： 功能特性分支（Feature branches） 发布用分支（Release branches） 补丁分支（Hotfix branches） 这三类分支目的明确，所以对于这些分支的源分支和合并的目标分支具有十分严格的规则。当然，这三类分支也仅仅是分支而已，并没有特别的地方。 功能特性分支 分支来源：develop合并目标：develop命名惯例：除master、develop、release-*或者hotfix-*之外的任何名字均可 功能特性分支（或者有时被称作专题分支）被用于开发接下来或者将来版本的新功能、新特性。当开始开发一项功能时，目标发布用分支并未明确，但只要功能在开发中，这个分支就存在，最终会合并回develop（意味着即将发布的版本中一定会包含该功能）或者被废弃（这当然是一种令人十分失望的情况）。 功能特性分支仅存在与开发的代码仓库，并不在origin。 创建一个功能特性分支 当着手开发新功能时，先在开发分支上检出新分支： $ git checkout -b myfeature developSwitched to a new branch \"myfeature\" 将完成的功能合并到开发分支上 完成的功能特性被合并到develop分支上，表示该功能要添加到即将发布的版本中： $ git checkout developSwitched to branch 'develop'$ git merge --no-ff myfeatureUpdating ea1b82a..05e9557(Summary of changes)$ git branch -d myfeatureDeleted branch myfeature (was 05e9557).$ git push origin develop --no-ff表示合并总是创建新的提交对象，这样可以避免在合并分支时丢失历史信息，对比图如下： 显而易见，这就是证据啊，证据！:joy: 发布用分支 分支来源：develop合并目标：develop和master命名惯例：release-* 发布用分支用于支持生产环境新版本，如修改少数的缺陷，准备版本发布的元数据（如版本号，构建日期等）。做完这些操作之后，develop分支便可以为了下个大版本接受这些新功能了。 将发布用分支从develop分支上检出的关键时刻是在开发几乎完全可以反映新功能理想状态的时候。此时，至少下个版本要发布的功能所在的功能分支要合并到develop上，而功能发布在将来的版本中则可以暂时不合并，等待下一次发布用分支的检出。 在发布用分支拉出时，就需要给其分配一个版本号。而此后的develop分支上的变更都将反映这个版本。 创建一个发布用分支 发布用分支在develop分支上检出。举例来讲，目前我们的生产环境版本是1.1.5，马上就要发布一个大版本。develop分支已经准备就绪，我们决定将下一个版本的版本号为1.2.所以我们拉出一个发布用分支，命名需要反映新的版本号： $ git checkout -b release-1.2 developSwitched to a new branch \"release-1.2\"$ ./bump-version.sh 1.2Files modified successfully, version bumped to 1.2.$ git commit -a -m \"Bumped version number to 1.2\"[release-1.2 74d9424] Bumped version number to 1.21 files changed, 1 insertions(+), 1 deletions(-) 创建完新分支之后，变更版本号（这里的bump-version.sh脚本用于修改文件版本号，当然，针对不同的场景，也可手动变更版本号）。 该分支会存在一段时间，这段时间内，该分支允许修改缺陷（而不是在develop上面）。在该分支上禁止添加新特性。最终，该分支必须合并到develop、 完成一个发布用分支 当发布用分支已经准备就绪可以发布一个现实的版本，我们仍然有很多工作要做。首先，将发布用分支合并到master（切记，所以提交到master内容一定是一新版本）。接着，提交到master上的变更必须添加标记（如使用版本号等进行标记），用于将来参考。最后，在这个发布用分支上进行的更改需要合并回develop，以保证将来的版本包含缺陷的修复。 在Git中的前两步： $ git checkout masterSwitched to branch 'master'$ git merge --no-ff release-1.2Merge made by recursive.(Summary of changes)$ git tag -a 1.2 此时，版本已发布，并且已标记。 Tips: 你可以使用-s或者-u &lt;key&gt;来加密标记。 为了保留发布用分支的变更，需要合并回develop分支： $ git checkout developSwitched to branch 'develop'$ git merge --no-ff release-1.2Merge made by recursive.(Summary of changes) 这一步可能也会产生冲突，所以，解决冲突并且提交。 此时，我们可以移除该发布用分支： $ git branch -d release-1.2Deleted branch release-1.2 (was ff452fe) 补丁分支 分支来源：master合并目标：develop和master命名惯例：hotfix-* 这类分支与发布用分支很类似，不过补丁分支的产生是为了快速响应生产环境中的紧急问题。当线上遭遇紧急缺陷需要立刻解决，则需要在对应标记的master分支上拉出一个补丁分支。 在某一位或者几位开发者修复线上问题的同时，develop分支可以继续进行。 创建一个补丁分支 补丁分支在master上拉出，举例来说，1.2版本是目前的线上版本，由于一个严重bug造成宕机的情况出现，但是目前develop分支上的变更还不够稳定，此时，我们可以使用补丁分支，先来解决紧急问题： $ git checkout -b hotfix-1.2.1 masterSwitched to a new branch \"hotfix-1.2.1\"$ ./bump-version.sh 1.2.1Files modified successfully, version bumped to 1.2.1.$ git commit -a -m \"Bumped version number to 1.2.1\"[hotfix-1.2.1 41e61bb] Bumped version number to 1.2.11 files changed, 1 insertions(+), 1 deletions(-) 不要忘记增加版本号。 然后，修复bug并提交变更。 $ git commit -m \"Fixed severe production problem\"[hotfix-1.2.1 abbe5d6] Fixed severe production problem5 files changed, 32 insertions(+), 17 deletions(-) 结束使用一个补丁分支 修复bug之后，补丁分支必须合并到master，同时，也需要合并到develop，确保在下一个版本中包含bug的修复。此时的操作与发布用分支完全一致。 首先，更新master并且标注版本： $ git checkout masterSwitched to branch 'master'$ git merge --no-ff hotfix-1.2.1Merge made by recursive.(Summary of changes)$ git tag -a 1.2.1 接着，合并到develop： $ git checkout developSwitched to branch 'develop'$ git merge --no-ff hotfix-1.2.1Merge made by recursive.(Summary of changes) 这个规则存在一个例外情况：如果发布用分支当前存在，则需要将补丁分支合并到发布用分支，而不是develop，因为该发布用分支最终会合并到develop（如果develop分支立刻需要这个bug得到修复，而等不到发布用分支结束，则你需要小心谨慎的将修正合并到未准备就绪的develop分支上）。 最后，移除这个临时分支： $ git branch -d hotfix-1.2.1Deleted branch hotfix-1.2.1 (was abbe5d6). 结语这个模型看起来并没有什么特别令人震惊的地方，但是却十分合理。在StackOverflow上问题What does Bump Version stand for?中，有解答者提到了该文，并十分推荐。 原文作者Twitter：@nvie","tags":[{"name":"Git","slug":"Git","permalink":"http://elbarco.cn/tags/Git/"}]},{"title":"英雄联盟中的随机行为优化","date":"2016-03-07T05:48:24.000Z","path":"2016/03/07/英雄联盟中的随机行为优化/","text":"原文地址：传送门原创翻译，转载请注明出处 对于像英雄联盟这样不断演进的产品的开发者而言，需要不断的致力于与系统的熵作斗争，因为他们将越来越多的内容添加到资源有限的服务器中。新的内容带了新的隐性成本——不仅是更多的实施成本，同时也包括由于创造了更多的纹理、仿真和处理造成的内存和性能成本。如果我们忽略（或者错误估算）了这些成本，则整体游戏性能不佳，可玩性减少。故障使人厌恶，延迟使人愤怒，帧率下降让人沮丧。 我是致力于提高英雄联盟性能团队中的一员。我们为客户端和服务器做快照，发现问题 (性能相关和其他)，然后修复问题。同时，我们将在这个过程中学到的东西反馈其他团队，并且给他们提供工具，使他们在影响用户之前来检测并定位他们自己的性能问题。我们不断的提高英雄联盟的性能为艺术家和设计师添加新的东西提供了空间：当他们使游戏更庞大更优秀的同时，我们使之更快。 这是关于我们团队如何优化英雄联盟性能系列的第一篇文章，后续我们将不断持续更新。这是一项回报丰厚的挑战，这篇文章将深入介绍我们在粒子系统中遇到的一些有趣的挑战——正如在下图中，你可以看到粒子系统在游戏中扮演了十分重要的角色。 上图是在英雄联盟游戏中高粒子密度的一个例子。 优化，并不是在程序集中重写大量的代码——尽管有些时候是这样的。我们仅变更那些不仅能够提高性能，而且维护正确性的代码，如果有可能的话，还会提高代码质量。最后一项略显挑剔：任何不易读、不易维护的代码都会产生技术债务，这个我们稍后再谈。 优化已有的代码库，我们采用了三个基本步骤：鉴别、理解和迭代。 步骤一：鉴别 在开始之前，我们首先需要确认哪些代码需要进行优化。即使有些代码看起来明显性能较差，但是由于其对整体性能影响极小，优化这类代码收益极少（尤其当花费在上面的时间和精力在其他方面可以做到更好的收益）。我们使用代码检测工具和采样分析器来帮助识别非性能部分的基本代码。 步骤二：理解 一旦我们得知代码库的哪部分代码性能较差，我们便会详细的查看这部分代码以求完全理解代码。理解代码意味着理解这些代码的含义及原本的目的。接着，我们就能知悉为何这些代码产生瓶颈了。 步骤三：迭代 当我们理解了为何特定部分代码执行较慢及代码本意要执行的内容，我们就有了足够的信息来设计和开发一套可行的解决方案。使用鉴别步骤中的工具和得到的快照数据，我们将新代码和旧代码的性能做了比较。如果解决方案效果出众，我们会彻底的进行测试以确保不引入来新的bug，那么接下来就可以击掌庆贺了，因为我们已经为其他内部测试做好了充分的准备。在大多数情况下，新的代码不见的足够快，所以我们不断迭代解决方案，知道新的代码能达到优化的目的。 现在，让我们看下在英雄联盟代码库中这几个步骤的实施细节，并以最近优化的粒子系统逐步介绍。 步骤1:鉴别拳头的工程师使用大量的分析工具来检查游戏客户端和服务器的性能。我们先查看来客户端的帧率和通过Waffles得到的高级分析信息（通过工具的特定函数获得的输出信息），这个内部工具可以让我们在内部构建的客户端与服务器保持联通。此外，Waffles还具备其他功能，如在测试过程中触发调试、检查游戏内部数据如导航分格和暂停或者减缓游戏过程。 Waffles截图 Waffles提供了一个实时展示界面，并提供详细的性能信息。上图是Waffles如何展现客户端性能表现的经典例子，上边图形（绿色柱状图）以毫秒为单位表示了帧率——越高的柱状图表示越低的帧率。非常慢的帧率在游戏中是可以感受得到的。柱状图下面是重要功能的分层视图，通过点击任何绿色柱状图，工程师都会看到影响该帧率的详细信息。通过这里，我们可以看出些端倪，即哪部分代码运行时导致性能较差的关键。 我们使用一个简单的宏在代码库内手工检测一些重要函数来提供这份性能相关的信息。在对外发布的游戏版本中，这个宏并没有被打包编译，但在测试版本打包中，这个宏作为一个很小的class存在，它创建了一个事件，存放于配置文件缓冲区。该事件包含一个字符串识别码、一个线程ID、一个时间戳和其他必要的信息（比如它还可以存储在其生命周期内所有发生的内存配置数）。当对象超出范围后，析构器会在配置缓冲区中更新该事件自构造以来的运行时间。在随后的时间，可以输出和解析此配置文件缓冲区——理想的情况是在另一个进程进行以尽量减少对游戏本身的影响。 Chrome Tracing 在这个例子中，我们将分析缓冲区输出到文件，并且读入到构建在Chrome浏览中可视化工具中（关于跟踪工具的更多信息，可以点击这里，你可以在自己的Chrome浏览器中通过在地址栏敲入”chrome://tracing/“进行尝试。这个扩展程序被设计用来进行网页性能分析，输入格式时JSON，所以你可以轻松的根据你自己的数据构造输入）。通过图形化后的结果，我们可以看到哪些是执行较慢的函数，或者在那里不断有大量的小函数被调用：这些都是次优代码的迹象。 让我来展示详细操作：上面的视图是Chrome Tracing的视图，图中展示了客户端上两个运行的线程。上部分的是主线程，执行大多数的处理工作，底部的是粒子线程，用来执行粒子处理。每一个着色的横条均对应一个函数，横条的长度指示了其执行时间。被调用的函数由竖直栈结构展示，父函数在子函数之上。这个工具提供给我们一种非常神奇方式来可视化执行复杂度以及帧的签名时间。当我们发现一个次优代码区域，我们可以放大粒子区域以求查看更多细节。 Chrome Tracing放大效果图 让我们放大图形中间部分。从上面的线程中我可以看到一个非常场的等待，只有当下面的粒子模拟函数执行完毕才结束。模拟功能包含大量不同函数（着色的横条）的调用。每一类都是粒子系统的更新功能，用于将位置、 方向和每个粒子在该系统中其他可见性状态进行更新。一个明显的优化方式是将模拟函数改造成多线程方式，即可运行在主线程中，也可以在粒子线程中执行，对于本例，我们仅关注与优化模拟代码本身。 既然现在我们知道去何处查看性能问题，我们可以切换到样本分析。这类分析周期性的读取和存储程序计数器和运行中的进程的栈信息（可选）。一段时间后，这个信息可以给出一个随机概述，概述中描述了代码库内的耗时。较慢的函数会得到更多的样本，更有用的是，用时最长的单个函数会累积更多的样本。在这里，我们不仅可以看到哪些函数执行最慢，同时可以看到哪几行代码执行最慢。如今有很多不错的样本分析工具可供选择，从免费的Very Sleepy到更多特性支持的商业软件，如Intel的VTune。 通过在游戏客户端上运行VTune来检查粒子线程，我们可以看到如下列表中运行最慢的函数。 VTune中的Hot Functions 上面的表格展现了一些粒子相关的函数。作为参考，最上面两个较大的函数用于为每个粒子更新矩阵和位置、方向相关的状态。举例来说，我们来看在第三和第九项AnimatedVariableWithRandomFactor&lt;&gt;中的Evaluate函数，函数很小（并且容易理解），但是相对而言比较耗时。 步骤2:理解现在，我们选择了一个需要优化的函数，则需要理解这个函数要做的事情和为什么这么做的原因。在本例中，AnimatedVariables被英雄联盟美术师用来定义粒子特征是如何随着时间变化。一旦一个美术师为一个特定的粒子可见性指定关键帧值后，代码中便会插入这些数据来产生一条曲线。插值方法是线性插值或一阶或二次集成。动画曲线被大量的使用——尽在召唤师峡谷（译者注：英雄联盟的地图之一，也是最热门的地图）中就有接近40000的动画曲线——涵盖了从粒子颜色扩展到旋转速度方方面面。Evaluate函数在每场游戏中会被调用数以亿计次。此外，LOL中的粒子系统是游戏体验中很重要的一部分，所以它们的行为不能做出任何改变。 这个类其实已经做过了优化，通过查表的方式，对每个timestep所需要的值都预先计算过并存储在一个数组中，所以在读取这些数值时不必再次计算，所以减少了计算的耗时。这是一个明智的选择，因为曲线的一阶和二次集成是一个昂贵的进程。为每个系统中的每个粒子上的动画变量进行这个操作会使得处理过程大大减少。 动画变量曲线的查询表 在查询性能问题时，通过找到最坏的场景来放大问题往往是一个十分有用的技巧。为了模拟粒子处理减缓，我开始了一场单个玩家的游戏，游戏中有9个中期级别的电脑，并且在下路挑起了一场混乱的团战。接着，我在团战期间在客户端上运行了VTune，记录了大量的数据用于分析。这些数据给出了在Evaluate代码中的归因样本（如下图所示）。 下图中我截取了第91-95行代码，为了更好的说明第90行调用Evaluate的情形。 VTune中的分析样本 对于不熟悉VTune的人来说，其实这个试图展示的就是解析期间所收集的代码。右侧的红色横条指示了命中次数，横条越长就意味着命中次数越多，而命中次数越多表示这一行执行越慢。挨着横条的时间是处理这行代码所用的预估时间。你也可以就某个特定函数的到一个准确视图来查看是什么因素“贡献”了执行缓慢。 如果就红色的横条来看，第95行代码就是问题所在。但是这段代码所做的仅仅是在Vector3f中复制出拼写错误的查询表，为什么这个函数成为最慢的部分呢？为什么12字节的复制这么慢？ 答案在于现代CPU访问内存的方式中。CPU非常忠实的遵循了摩尔定律，每年都会提速60%，而内存速度每年的增速只有可怜的10%。 图出自《计算机体系结构：量化研究方法》By John L. Hennessy, David A. Patterson, Andrea C. Arpaci-Dusseau 缓存可以减小性能差距，运行英雄联盟的大多数CPU都有3级缓存，一级缓存最快但容量最小，三级缓存最慢但容量相对最大。从一级缓存读取数据只需要4个周期，而读取主内存却需要大约300个周期甚至更多。你可以在300个周期内做大量处理工作。 最初查询表的解决方案的问题在于，虽然从查询表中的顺序读取值的操作是非常快的(由于硬件预取)，但是我们正在处理的颗粒并不是按照时间顺序存储，所以实际查找顺序是随机的。这通常会导致CPU等待从主存储设备读取数据时产生延迟。虽然300个周期比一级或者二级集成代价更低，但我们还是需要知道这个函数在游戏中的使用频率如何，因为毕竟这个函数在游戏中被大量的使用。 为了探求真相，我们在代码中添加一些额外的内容来收集AnimatedVariables的数量和类型。结果表示，在38000个AnimatedVariables中： 37500个是线性插值；100个是一级，400个是二级 31500个仅有一个关键值；2500个有3个关键值；1500有2个或者4个关键值 所以最常见的途径是针对单键值。因为代码总是生成查询表，这就产生了一个不需要传播的单数值表。也就意味着每次查询（总是返回相同值）一般会产生缓存丢失，进而导致大量的内存和CPU周期浪费。 通常来讲，代码成为瓶颈一半有四个原因： 调用频率过高 算法选择不佳：如O(n^2)vsO(n) 做了不必要的工作或者太频繁的执行必要的操作 数据较差：或者是数据量太大，或者是数据分布和访问模式较差 这里产生的问题原因不是由于代码设计不好或者开发质量导致。解决方案是好的，但是在被美术师大量使用之后，普通路径是针对单值的，而这些简单的问题在使用过程中是很不明显的。 顺便说一句，我学会了作为一名程序员最重要的事情之一便是尊重你正在处理的代码。代码有可能看起很疯狂，但是这样写的目的可能是基于一个好的出发点。在没有完全理解代码如何使用和其为何设计之前不要错误的认为这些代码是丑陋愚蠢的。 来自：http://codesoftly.com/2010/03/ha-code-entropy-explained.html 步骤3:迭代现在我们了解了哪部分代码执行较慢、这部分代码本意是什么和为何执行较慢，是时候开始构想解决方案了。每个常见的执行路径都是为单独变量设计，我们还知道数量少的键的线性插值非常快（在少量高速缓存中作简单的计算），所以我们需要在考虑这种情况的基础上进行重新设计。最后，我们可以回到前面罕见集成曲线的预计算查询表上。 在某些情况下，当我们不使用查询表时，首先构造这些表是没有意义的，所以会释放大量意义非凡的内存（大多数表具有256个条目或者更多，每个条目可达12字节的大小，这相当于大约每张表3kb）。所以现在，我们可以使用额外的一些内存来添加缓存的条目和存储的单值的数量。 之前的代码看起来是这个样子的： template &lt;typename T&gt;class AnimatedVariable&#123; // &lt;snip&gt;private: std::vector&lt;float&gt; mTimes; std::vector&lt;T&gt; mValues;&#125;;template &lt;typename T&gt;class AnimatedVariablePrecomputed&#123; // &lt;snip&gt;private: std::vector&lt;T&gt; mPrecomutedValues;&#125;; AnimatedVariablePrecomputed对象在AnimatedVariable中进行构造，从它的指定大小插值和构建一个表。Evaluate()仅在预计算对象中被调用。 我们修改了一下AnimatedVariable类，现在看起来是这个样子的： template &lt;typename T&gt;class AnimatedVariable&#123; // &lt;snip&gt;private: int mNumValues; T mSingleValue; struct Key &#123; float mTime; T mvalue; &#125;; std::vector&lt;Key&gt; mKeys; AnimatedVariablePrecomputed&lt;T&gt; *mPrecomputed;&#125;; 我们添加了一个缓存值mSingleValue，和一个整数mNumValues，用于告诉我们何时才使用mSingleValue。如果mNumValues是1（即对应单值的情况），Evaluate()会直接返回mSingleValue的值——不需要其他多余的处理。你还可以注意到插入时间和值构造的Key能够减少缓存未命中的情况。 指向此类的数据向量大小现在范围从24到36个字节不等，具体取决于模板类型（同时也依赖与平台，std::vector&lt;&gt;的大小也会不同）。 Evaluate()之前的代码看起来是这样子的： template &lt;typename T&gt;T AnimatedVariablePrecomputed&lt;T&gt;::Evaluate(float time) const&#123; in numValues = mPrecomputedValues.size(); RIOT_ASSERT(numValues &gt; 1); int index = static_cast&lt;int&gt;(time * numValues); // clamp to valid table entry to handle the 1.0 boundary or out of bounds input index = Clamp(index, 0, numValues - 1); return mPrecomputedValues[index];&#125; 修改后的Evaluate()方法代码如下，这是在VTune中展示的。你可以看到三个可能的执行case：单值（红色部分），线性插值（蓝色部分）和预计算查询（绿色部分）。 在VTune中展示的优化过的代码片段 修改后的代码执行速度大约快了3倍：在最慢的函数列表中该函数从第三位降到了第22位！不仅执行更快，同时还降低了内存的使用，大约减少了750kb。这还不算完，不仅函数执行更快，占内存更少，同时提高了线性插值的准确度。可谓一石三鸟。 这里并没有提到的内容（尽管文章已经足够长了）是我如何通过不断迭代找到了这个解决方案。我最初的尝试减少在粒子生命周期内样本表的大小。这个方案几乎有效——但有些移动较快的粒子由于样本表的减少，变的参差不齐。幸运的是，这个现象很快就被发现了，使得我依然能够将方案更换为本文中提到的方法。当然还有一些其他的代码修改，但是对于性能提高并没有直接效果，也有些代码的修改甚至造成了代码执行更慢。 总结本文中介绍的是英雄联盟游戏代码库中代码优化的一个典型案例。虽然变动更小，但是这个改动使得内存节约了750kb，粒子线程比较之前运行快了1到2毫秒，这使得主线程执行的更快。 当程序员寻求优化的时候，虽然看似显而易见，但这里提到的三个阶段都常常会被忽视。这里只是为了强调一下： 鉴别：分析应用并找出性能最差的部分 理解：理解代码的本意和执行缓慢的原因 迭代：基于上面两个阶段的到的成果进行代码的修改、迭代，并重新分析。重复这三个步骤直到足够快。 上面提到的解决方案不见得是最快的解决方案，但至少方向是正确的——性能提升的安全路径是通过迭代改进。 本文作者：Tony Albrecht","tags":[{"name":"翻译","slug":"翻译","permalink":"http://elbarco.cn/tags/翻译/"}]},{"title":"开山第一篇","date":"2016-03-03T01:30:24.000Z","path":"2016/03/03/开山第一篇/","text":"关于博客用过好多博客，如Cnblog、CSDN、ITeye等等，后来觉得用Github更Geek一点，于是学习gizak搞了一个介个。后来，在郭师哥的怂恿下，搞了台阿里云主机把玩，因为有之前的经验，所以比较愉快选择了Hexo来搭建自己的博客。拖延了好久，上周末终于下定决心好好弄一弄。 Hexo的主题十分丰富，官方的主题向这里(自备梯子)看齐。至于我，选用的是indigo，因为Material Design的风格很舒服，而且移动设备适配也很好，功能基本满足，个性化定制也方便（主要是修改起来方便……(:3 」∠)）。刚搭建完成的时候，我将整个博客的源码都放在了GitHub上面，看这个项目，dev分支是备份，master分支是第一次生成的博客内容。Feel free to build your own blog based on that. 关于我90后，男，单身狗，程序猿。目前帝都某创业型互联网公司就职，云计算相关的Java攻城狮，所以到底是🐶还是🦁️，傻傻分不清楚。喜欢做技术，热爱互联网，拥抱开源。一个人惯了，也爱宅。爱好十分广泛，美剧、电影、音乐、旅行、折腾。其他关键字，强迫症（尽管我是射手座不是处女座）、轻微人格分裂、偶尔犯二……各位看官，随便感受下就好。这里的我，无关紧要。 关于域名目前域名为0x4b5.top，数字表示了我的生日，无他。正在备案的域名elbarco.cn，通过后将正式启用。El barco（音译：埃尔巴科），西班牙语船的意思，朋友们喊我小船，估计是因为我是张帆。家人取的这名重名率极高，选个域名无从下手，所以才想到了用El barco，BTW，本域名与什么elbarco.com毫无关系，特此声明。也希望在这里，我能做那沉舟侧畔千帆中的一员，有所分享，有所进步。 最后，感谢各位看官老爷。","tags":[{"name":"杂","slug":"杂","permalink":"http://elbarco.cn/tags/杂/"}]}]