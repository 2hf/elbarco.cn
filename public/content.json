[{"title":"Hi, 2019","date":"2019-01-29T07:35:23.000Z","path":"2019/01/29/hi-2019/","text":"Hi, 2019, nice to meet you! Happy Chinese New Year! 🐷","tags":[{"name":"2019","slug":"2019","permalink":"http://elbarco.cn/tags/2019/"}]},{"title":"Nova中的块设备映射","date":"2018-08-25T05:54:51.000Z","path":"2018/08/25/block-device-mapping-in-nova/","text":"原文地址：Block Device Mapping原创翻译，转载请注明出处。 一、概述Nova中存在一个块设备的概念，这些块设备用于暴露给实例使用。实例的块设备可以有多种，租户或者用户使用哪种块设备取决于特定的部署已经设置的使用限制。块设备映射（Block device mapping）用于管理实例所有的块设备和保存数据。 当我们讨论块设备时，我们通常会涉及下列事情： API或者CLI命令行层面，在实例boot请求中指定块设备的请求结构或者syntax； Nova内部用于记录、持久化块设备数据的数据结构，最终记录会写入到block_device_mapping表中。然而在Nova内部，在表示同一数据时，却有几种略微不同的数据格式。除了代码中的BlockDeviceMapping对象，我们还有： API格式，见BlockDeviceDict类，来处理API层面BDM的校验和格式转换，姑且称之为API BDMs virt driver格式，由nova.virt.block_device中的类来定义，比如DriverBlockDevice、DriverVolumeBlockDevice等。这些类，除了暴露不同的格式之外，同时提供了处理不同类型的块设备的一些功能函数，比如挂在卷需要同是和cinder和virt driver的代码交互，下文中称之为Driver BDMs 二、数据格式及其历史Nova早期的代码中的block device mapping，基本上是仿照了EC2 API中的数据结构。在Havana版本中，block device mapping的代码有了改进和提高，比如在API中暴露额外的详情和特性。为此，V2 API中增加了一个新的扩展，称之为BlockDeviceMappingV2Boot，实际上是在实例的boot API请求中，增加了字段block_device_mapping_v2。 Block device mapping V1（又称之为 legacy）目前Nova代码中使用和存储数据使用的是新的数据结构，但是为了处理会用遗留格式的请求，仍然需要处理v1格式，代码的转换在nova.block_device模块。在V1中，使用device name作为key，并且仅接受： Cinder卷或者快照的UUID Type，类型，用于区分Cinder卷和快照。 Size，大小，可选项 delete_on_termination标志位，可选项 间奏曲-块设备名称的问题使用设备名称作为每个实例的主要的识别号，并把它们暴露给API，对于Nova来说，是有问题的，主要是因为Nova支持的几种hypervisor及其自己的drivers不能保证guest OS分配的设备就是用户在Nova中请求的设备。另外，在公共的NovaAPI中暴露这样的详情，显然也是不理想的。 解决这个问题，方案是允许用户不指定块设备名称，从而交给Nova（在virt driver的帮助下）来决定设备名称。 此外，指定设备名称常用于卷启动功能，为实例指定匹配root device设备名称，通常是/dev/vda。 目前，用户不鼓励在请求时指定设备名称。 Block device mapping V2引入新的格式，是为了解决上面提到的问题，并且支持使用简单的格式所做不到的灵活性以及额外的功能。新的格式是一个含有字典的列表，包含下列字段（除了前面提到的已有的选项）： source_type，可以有以下值： image volume snapshot blank dest_type，可以有以下值: local volume guest_format，用于告诉Nova在挂载时如何格式化设备，通常用于blank local images，如果值为swap，则表示一个swap设备 device_name，设备名称最好还是不要设置，除非用户希望重载image metadata中指定的设备。在Libvirt中，\b即使指定了设备名称，在实例内部的设备名称最终由driver来设置（按照字母序，比如指定某个设备名称是/dev/sde，但是在实例内部会按照字母序设置为/dev/sdb或者/dev/sdc等等） disk_bus和device_type，\b可能某些hypervisor会支持的low level details。disk_bus可能的值有ide、usb、virtio、scsi，device_type可以是disk、cdrom、floppy、lun（//TODO，挖个坑，lun了解一下？）。以上值不是全部的\b列表，一般设置为空即可。 boot_index，定义了hypervisor\b boot实例尝试存储设备的顺序。每个可用于boot device的设备，应当设置一个唯一的index值，从0开始，依次递增。有些hypervisor不支持booting from multiple devices，所以只会考虑boot_index为0的设备。设置该值为None或者负数（比如-1），\b则表示这个设备不可以用于启动。通常的做法是，对于boot device，\bboot_index设置为0，其他设备设置为None（或不设置）。 有效的source/destination组合source_type和dest_type的组合用于定义块设备条目的类型，目前支持下列组合： image-&gt;local，预留的条目，用于表示实例在Glance的镜像启动。 volume-&gt;volume，表示挂载到实例的Cinder卷，可以被标注为启动设备 snapshot-&gt;volume，表示在Cinder的卷快照创建一个卷，并挂载到实例上，可以被标注为启动设备 image-&gt;volume，表示在Glance中下载镜像到Cinder的卷，并且把卷挂载给实例，可以被标注为\bbootable blank-&gt;volume，创建一个空的Cinder卷，并把它挂载到实例上，需要设置size blank-&gt;local，取决于guest_format，通常表示hypervisor本地存储的ephemeral blank disk或者swap disk（实例只能有一个swap磁盘） Nova不会在一个请求中同时允许BDMv1和BDMv2的混合使用，会在接收一个boot请求时做基础的校验。","tags":[{"name":"OpenStack","slug":"OpenStack","permalink":"http://elbarco.cn/tags/OpenStack/"},{"name":"Nova","slug":"Nova","permalink":"http://elbarco.cn/tags/Nova/"}]},{"title":"初识Virtio Balloon","date":"2018-07-16T03:22:07.000Z","path":"2018/07/16/virtio-balloon/","text":"原文地址：Virtio balloon原创翻译，转载请注明出处。 本文主要介绍了什么是KVM virtio balloon driver。 首先，如果你从没听过这个概念，那什么是balloon driver呢？它是一种给予guest实例或者从guest实例中获取RAM的方法。（理论上来讲），如果你的guest实例需要更多的RAM，你可以通过balloon driver给他分配更多的内存，或者如果宿主机需要在guest实例中取走一下内存，balloon driver也可以做到。这些操作的执行不需要暂停或者重启guest实例。 virtio_balloon是一个在guest实例中的内核驱动。这个driver表现的像一种奇怪的进程，要么扩展自己的内存使用，要么压缩自己的内存使用接近什么也没有，如下图所示： 当balloon driver扩张时，运行在guest实例中的应用会突然少了很多可用内存，然后guest实例会像没有多少内存时做的那样，交换内存并启动OOM killer（balloon本身是不可交换并且不会被杀掉的）。 那么这个“浪费”内存的内核驱动程序有什么意义呢？有两点——第一，驱动通过virtio通道与宿主机通讯，宿主机给其下发指令，比如扩展到指定的大小、现在开始缩小。guest实例配合操作，但是并不直接控制balloon。第二，balloon中的内存也从guest实例中取消映射，并传回宿主机，因此宿主机可以将这部分内存传递给其他的guest实例使用。看起来就像guest虚机的内存缺失了一块一样： Libvirt有两个配置项，currentMemory和maxMemory（详见Memory Allocation）： maxMemory（或&lt;memory&gt;）是在guest实例启动阶段分配的内存。KVM和Xen的guest虚机目前（译者注：原文发布时间是2010-07-17 14:33）无法找过这个内存上限。currentMemory控制了请求分配给guest实例上应用程序的内存。balloon填充剩余的内存，并且把这部分内存还给宿主机，用于宿主机的在其他地方使用。 该项配置可以手动调整，或者通过编辑XML文件，或者通过virsh setmem命令。","tags":[{"name":"Virtio","slug":"Virtio","permalink":"http://elbarco.cn/tags/Virtio/"},{"name":"KVM","slug":"KVM","permalink":"http://elbarco.cn/tags/KVM/"},{"name":"Virtualization","slug":"Virtualization","permalink":"http://elbarco.cn/tags/Virtualization/"}]},{"title":"Nova Placement API与Nova调度全解析","date":"2018-02-23T08:47:50.000Z","path":"2018/02/23/nova-placement-api/","text":"是什么由于历史遗留原因，Nova认为资源全部是由计算节点提供，所以在报告某些资源使用时，Nova仅仅通过查询数据库中不同计算节点的数据，简单的做累加计算得到使用量和可用资源情况，这一定不是严谨科学的做法，于是，在N版中，Nova引入了Placement API，这是一个单独的RESTful API和数据模型，用于管理和查询资源提供者的资源存量、使用情况、分配记录等等，以提供更好、更准确的资源跟踪、调度和分配的功能。 有什么代码目录由于Nova Placement API是单独剥离出来的RESTful API，同时也有自己单独的Endpoint，并且与Nova API服务启动在不同的端口，单独提供服务，那么，在代码目录上来看，也是相对独立的，其代码实现均在/nova/api/openstack/placement/下，那么我看来看一下Nova Placement API的代码目录结构： F:nova ZH.F$ tree -C api/openstack/placementapi/openstack/placement├── __init__.py├── auth.py├── deploy.py├── handler.py├── handlers│ ├── __init__.py│ ├── aggregate.py│ ├── allocation.py│ ├── allocation_candidate.py│ ├── inventory.py│ ├── resource_class.py│ ├── resource_provider.py│ ├── root.py│ ├── trait.py│ └── usage.py├── lib.py├── microversion.py├── policy.py├── requestlog.py├── rest_api_version_history.rst├── schemas│ ├── __init__.py│ ├── aggregate.py│ ├── allocation.py│ ├── allocation_candidate.py│ ├── inventory.py│ ├── resource_class.py│ ├── trait.py│ └── usage.py├── util.py├── wsgi.py└── wsgi_wrapper.py 其中，在api/openstack/placement/schemas目录下，可以看到基本数据模型的schema，不过resource privoder的schema定义在了api/openstack/placement/handlers/resource_provider.py中。下面，对照schema，我们对其中的一些概念进行了解。 Nova Placement API中的一些概念Resource Provider即资源提供者，通过其schema可以看到结构比较简单，只包含UUID和RP（Resource Provider简写，下同）的一些基本信息，比如name： GET_RPS_SCHEMA_1_0 = &#123; \"type\": \"object\", \"properties\": &#123; \"name\": &#123; \"type\": \"string\" &#125;, \"uuid\": &#123; \"type\": \"string\", \"format\": \"uuid\" &#125; &#125;, \"additionalProperties\": False,&#125; 资源提供者可能是一个计算节点，也可能是一个共享存储池或者一个IP分配池子，那么不同的RP，提供的资源多种多样，于是就引入了Resource Class，即资源类型的概念。 Resource Class即资源类型，比如计算节点提供的资源可能是CPU、内存、PCI设备、本地临时磁盘等等。每种被消费的资源都会按照类别进行标注和跟踪。 之所以引入这个概念，目的是解决Nova中hard-coded的资源类型扩展性问题，比如CPU资源，可能记录在Instance对象的vcpus字段中，那么之后再增加新的资源类型，都需要修改数据表，而修改数据表的过程都会停机维护，给系统带来许多downtime，这是不可接受的。 Placement API提供了一些标准资源类别，如： VCPU MEMORY_MB DISK_GB PCI_DEVICE NUMA_SOCKET NUMA_CORE NUMA_THREAD IPV4_ADDRESS … 注：数据来自BP:Introduce resource classes 除了以上标准资源类别，Placement API还在O版中为RP增加了自定义Resource Class的能力，比如自动以的FPGA、裸机调度等等。 Inventory即库存，存量。用于记录超配比、资源总量、存量、步长（step_size）、最小和最大单位等信息，可以看一下它的schema： BASE_INVENTORY_SCHEMA = &#123; \"type\": \"object\", \"properties\": &#123; \"resource_provider_generation\": &#123; \"type\": \"integer\" &#125;, \"total\": &#123; \"type\": \"integer\", \"maximum\": db.MAX_INT, \"minimum\": 1, &#125;, \"reserved\": &#123; \"type\": \"integer\", \"maximum\": db.MAX_INT, \"minimum\": 0, &#125;, \"min_unit\": &#123; \"type\": \"integer\", \"maximum\": db.MAX_INT, \"minimum\": 1 &#125;, \"max_unit\": &#123; \"type\": \"integer\", \"maximum\": db.MAX_INT, \"minimum\": 1 &#125;, \"step_size\": &#123; \"type\": \"integer\", \"maximum\": db.MAX_INT, \"minimum\": 1 &#125;, \"allocation_ratio\": &#123; \"type\": \"number\", \"maximum\": db.SQL_SP_FLOAT_MAX &#125;, &#125;, \"required\": [ \"total\", \"resource_provider_generation\" ], \"additionalProperties\": False&#125; 其中的resource_provider_generation字段，是一个一致性视图的标志位，在获取RP列表时的generation功能是相同的，这就是CAS（Compare and swap），即乐观锁技术——当多个线程尝试使用CAS同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。 Usage即用量，使用情况。可以查看某个RP的使用情况，也可以查看项目下某用户的资源使用情况。 Aggregate在Ocata版本，社区开始将nova-scheduler服务与Placement API进行集成，并在scheduler进行了一些修改，使用Placement API进行满足一些基本资源请求条件的计算节点过滤。添加了aggregates，来提供resource provider的分组机制。 Allocation即已分配量，某一个RP对某一个资源消费者（即某个实例）所分配的资源。 Allocation-candidate即分配的候选者（资源提供者），举个例子，用户说，我需要1个VCPU，512MB内存，1GB磁盘的资源，Placement你帮我找找看看，有没有合适的资源。然后Placement就要做各种处理，反馈给用户，哪些是可以分配的候选资源提供者。 Trait字面意思，特征，特性。ResourceProvider和Allocation可以在定量的角度，控制和管理boot虚机请求，然而我们还需要从定性的角度来区分资源，最经典的例子是当我们创建虚机时，需要向不同的RP请求磁盘资源，用户可能请求80GB的磁盘，但也可能请求80GB的SSD。这就是Trait的意义。 数据库及数据表目前我安装的Pike版本的Packstack环境中，能看到有一个nova_placement数据库，但是没有任何表（也许是社区希望能把placement相关的表放到这个数据库中？），Placement对应的数据库用的还是nova_api： MariaDB [nova_placement]&gt; use nova_api;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedMariaDB [nova_api]&gt; show tables;+------------------------------+| Tables_in_nova_api |+------------------------------+| aggregate_hosts || aggregate_metadata || aggregates || allocations || build_requests || cell_mappings || consumers || flavor_extra_specs || flavor_projects || flavors || host_mappings || instance_group_member || instance_group_policy || instance_groups || instance_mappings || inventories || key_pairs || migrate_version || placement_aggregates || project_user_quotas || projects || quota_classes || quota_usages || quotas || request_specs || reservations || resource_classes || resource_provider_aggregates || resource_provider_traits || resource_providers || traits || users |+------------------------------+ 可以从表明上看到那些是Placement相关的表，这里就不展开了。 初始化及加载方式我们前面提到Nova Placement API是单独的RESTful API，那么是如何进行初始化的呢？带着这个问题，我们先查看nova的setup.cfg，其中配置了wsgi_scripts如下： wsgi_scripts = nova-placement-api = nova.api.openstack.placement.wsgi:init_application nova-api-wsgi = nova.api.openstack.compute.wsgi:init_application nova-metadata-wsgi = nova.api.metadata.wsgi:init_application 其中可以看到nova-placement-api的初始化来自 nova.api.openstack.placement.wsgi.init_application ，代码如下： def init_application(): # initialize the config system conffile = _get_config_file() config.parse_args([], default_config_files=[conffile]) # initialize the logging system setup_logging(conf.CONF) # dump conf if we're at debug if conf.CONF.debug: conf.CONF.log_opt_values( logging.getLogger(__name__), logging.DEBUG) # build and return our WSGI app return deploy.loadapp(conf.CONF) 其中在最后构造WSGI app并返回，即调用了deploy.loadapp(conf.CONF)： def loadapp(config, project_name=NAME): application = deploy(config, project_name) return applicationdef deploy(conf, project_name): \"\"\"Assemble the middleware pipeline leading to the placement app.\"\"\" ... application = handler.PlacementHandler() ... for middleware in (microversion_middleware, fault_wrap, request_log, context_middleware, auth_middleware, cors_middleware, req_id_middleware, ): if middleware: application = middleware(application) return application 而这里的handler.PlacementHandler()就是我们的Placement的API入口： class PlacementHandler(object): \"\"\"Serve Placement API. Dispatch to handlers defined in ROUTE_DECLARATIONS. \"\"\" def __init__(self, **local_config): # NOTE(cdent): Local config currently unused. self._map = make_map(ROUTE_DECLARATIONS) def __call__(self, environ, start_response): # All requests but '/' require admin. if environ['PATH_INFO'] != '/': ... 可以看到，PlacementHandler在__init__中根据路由定义构造了map，同时在__call__中对请求进行dispatch。这就是一个典型的WSGI应用： WSGI application is a callable object (a function, method, class, or an instance with a __call__ method) that accepts two positional arguments: WSGI environment variables and a callable with two required positional arguments which starts the response; 找到了初始化，那么Placement API加载和启动是如何实现的？ 首先，nova-placement-api是单独的脚本，在httpd中启动，与keystone（在12年就完成了WSGI化，参见&gt;&gt;传送门）类似，通过systemctl status httpd是可以看到的： [root@f-packstack ~(keystone_admin)]# systemctl status httpd● httpd.service - The Apache HTTP Server Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled; vendor preset: disabled) Active: active (running) since Fri 2018-02-02 09:17:51 CST; 1 weeks 0 days ago Docs: man:httpd(8) man:apachectl(8) Process: 4087 ExecReload=/usr/sbin/httpd $OPTIONS -k graceful (code=exited, status=0/SUCCESS) Main PID: 1309 (httpd) Status: &quot;Total requests: 0; Current requests/sec: 0; Current traffic: 0 B/sec&quot; CGroup: /system.slice/httpd.service ├─ 1309 /usr/sbin/httpd -DFOREGROUND ├─ 4108 keystone-admin -DFOREGROUND▽ ├─ 4109 keystone-admin -DFOREGROUND ├─ 4110 keystone-admin -DFOREGROUND ├─ 4111 keystone-admin -DFOREGROUND▽ ├─ 4112 keystone-main -DFOREGROUND ├─ 4113 keystone-main -DFOREGROUND ├─ 4114 keystone-main -DFOREGROUND ├─ 4115 keystone-main -DFOREGROUND ├─ 4116 placement_wsgi -DFOREGROUND ├─ 4117 placement_wsgi -DFOREGROUND ├─ 4118 placement_wsgi -DFOREGROUND ├─ 4119 placement_wsgi -DFOREGROUND ├─ 4121 /usr/sbin/httpd -DFOREGROUND ├─ 4122 /usr/sbin/httpd -DFOREGROUND… 知道是在httpd启动的，我们去查看配置文件目录： [root@f-packstack ~(keystone_admin)]# ll /etc/httpd/conf.d/total 36-rw-r-----. 1 root root 136 Jan 12 17:46 00-nova-placement-api.conf-rw-r--r--. 1 root root 943 Jan 12 17:48 10-keystone_wsgi_admin.conf-rw-r--r--. 1 root root 938 Jan 12 17:48 10-keystone_wsgi_main.conf-rw-r--r--. 1 root root 941 Jan 12 17:49 10-placement_wsgi.conf-rw-r--r--. 1 root root 697 Jan 12 17:48 15-default.conf-rw-r--r--. 1 root root 2926 Oct 20 04:39 autoindex.conf-rw-r--r--. 1 root root 366 Oct 20 04:39 README-rw-r--r--. 1 root root 1252 Oct 20 00:44 userdir.conf-rw-r--r--. 1 root root 824 Oct 20 00:44 welcome.conf 其中，10-placement_wsgi.conf 中定义了WSGIScriptAllias： ... WSGIProcessGroup placement-api WSGIScriptAlias /placement &quot;/var/www/cgi-bin/nova/nova-placement-api”... 也就是说，url为/placement/xxx的请求会使得httpd服务运行定义在/var/www/cgi-bin/nova/nova-placement-api中的WSGI应用，在这个文件中，我们会看到： from nova.api.openstack.placement.wsgi import init_applicationif __name__ == \"__main__”: import argparse import socket import sys import wsgiref.simple_server as wss … server = wss.make_server(args.host, args.port, init_application())... 也就对应了前面提到的PlacementHandler中的nova.api.openstack.placement.wsgi.init_application，至此，我们就了解了Nova Placement API的初始化和加载方式。 API路由定义上一小节提到了PlacementHandler初始化时，根据路由定义构造了map映射，我们就来看下文件api/openstack/placement/handler.py中的APIROUTE_DECLARATIONS: # URLs and Handlers# NOTE(cdent): When adding URLs here, do not use regex patterns in# the path parameters (e.g. &#123;uuid:[0-9a-zA-Z-]+&#125;) as that will lead# to 404s that are controlled outside of the individual resources# and thus do not include specific information on the why of the 404.ROUTE_DECLARATIONS = &#123; '/': &#123; 'GET': root.home, &#125;, # NOTE(cdent): This allows '/placement/' and '/placement' to # both work as the root of the service, which we probably want # for those situations where the service is mounted under a # prefix (as it is in devstack). While weird, an empty string is # a legit key in a dictionary and matches as desired in Routes. '': &#123; 'GET': root.home, &#125;, '/resource_classes': &#123; 'GET': resource_class.list_resource_classes, 'POST': resource_class.create_resource_class &#125;, '/resource_classes/&#123;name&#125;': &#123; 'GET': resource_class.get_resource_class, 'PUT': resource_class.update_resource_class, 'DELETE': resource_class.delete_resource_class, &#125;, '/resource_providers': &#123; 'GET': resource_provider.list_resource_providers, 'POST': resource_provider.create_resource_provider &#125;, '/resource_providers/&#123;uuid&#125;': &#123; 'GET': resource_provider.get_resource_provider, 'DELETE': resource_provider.delete_resource_provider, 'PUT': resource_provider.update_resource_provider &#125;, '/resource_providers/&#123;uuid&#125;/inventories': &#123; 'GET': inventory.get_inventories, 'POST': inventory.create_inventory, 'PUT': inventory.set_inventories, 'DELETE': inventory.delete_inventories &#125;, '/resource_providers/&#123;uuid&#125;/inventories/&#123;resource_class&#125;': &#123; 'GET': inventory.get_inventory, 'PUT': inventory.update_inventory, 'DELETE': inventory.delete_inventory &#125;, '/resource_providers/&#123;uuid&#125;/usages': &#123; 'GET': usage.list_usages &#125;, '/resource_providers/&#123;uuid&#125;/aggregates': &#123; 'GET': aggregate.get_aggregates, 'PUT': aggregate.set_aggregates &#125;, '/resource_providers/&#123;uuid&#125;/allocations': &#123; 'GET': allocation.list_for_resource_provider, &#125;, '/allocations': &#123; 'POST': allocation.set_allocations, &#125;, '/allocations/&#123;consumer_uuid&#125;': &#123; 'GET': allocation.list_for_consumer, 'PUT': allocation.set_allocations_for_consumer, 'DELETE': allocation.delete_allocations, &#125;, '/allocation_candidates': &#123; 'GET': allocation_candidate.list_allocation_candidates, &#125;, '/traits': &#123; 'GET': trait.list_traits, &#125;, '/traits/&#123;name&#125;': &#123; 'GET': trait.get_trait, 'PUT': trait.put_trait, 'DELETE': trait.delete_trait, &#125;, '/resource_providers/&#123;uuid&#125;/traits': &#123; 'GET': trait.list_traits_for_resource_provider, 'PUT': trait.update_traits_for_resource_provider, 'DELETE': trait.delete_traits_for_resource_provider &#125;, '/usages': &#123; 'GET': usage.get_total_usages, &#125;,&#125; 怎么用如何部署在官方文档中提到，placement api服务必须在升级到14.0.0，即N版后，升级到15.0.0，即O版之前进行部署。nova-compute服务中的resource tracker需要获取placement的资源提供者存量和分配信息（这部分信息将在O版中由nova-scheduler使用）。 部署API服务 - Placement API目前还是在nova中进行开发，但是设计上是相对独立的，以便将来分离出来成为单独的项目。作为一个单独的WSGI应用，可使用Apahce2或者Nginx部署API服务。 同步数据库 - 升级N版时，需要手动执行nova-manage api_db sync命令进行数据库同步，这样Placement相关的数据表就会被创建出来 在keystone中创建具有admin角色的placement service user，同时更新服务目录，配置单独的endpoint. 配置nova.conf中[placement]部分，并重启nova-compute服务。不过对于我们P版，经过了O版的一系列功能补齐，尤其是在O版中，如果在nova.conf中不配置[placement]部分的内容，就无法启动nova-compute服务。 The nova-compute service will fail to start in Ocata unless the [placement] section of nova.conf on the compute is configured. 更多部署相关的可参见官方文档，&gt;&gt;传送门。 OSC Placement Plugin从前面的API路由定义，我们可以看到，目前支持了这么些功能，那么我们可以简单的用一下，第一个想到的是cURL命令，我们可以使用该命令模拟发起请求，调用Placement API，比如查看resource providers list，首先我们获取token： # 首先得到auth tokencurl -d '&#123;\"auth\": &#123;\"tenantName\": \"admin\", \"passwordCredentials\": &#123;\"username\": \"admin\", \"password\": \"1234qwer\"&#125;&#125;&#125;' \\-H \"Content-type: application/json\" \\http://localhost:5000/v2.0/tokens...&#123; \"issued_at\": \"2018-02-07T07:40:07.000000Z\", \"expires\": \"2018-02-07T08:40:07.000000Z\", \"id\": \"gAAAAABaeq1XrNDoU_F_iRk8uC0lOxYpyzLMW_YRs_ggJHuF1OpGHBN-pymQut-Bp2Er-J4XkYfQkMdJbRlBIBhq4wfhZMHZvag1itnL6Q-TSWhOn7uZpdQsYqqJDmwgtzCm-hcpg17IwN5FZSanCbcy6S96YZ0Zci5STWNka40861Mn8UQ2yRE\", \"tenant\": &#123; \"description\": \"admin tenant\", \"enabled\": true, \"id\": \"6387fc88b3064149a12eb5b58669e0b2\", \"name\": \"admin\" &#125;&#125;# token的获取方式，还可以用OSC命令：openstack token issue | grep ' id' | awk '&#123;print $4&#125;'...#得到token之后，构造请求，查看resource providers list：curl -X GET /-H 'x-auth-token:gAAAAABaeq1XrNDoU_F_iRk8uC0lOxYpyzLMW_YRs_ggJHuF1OpGHBN-pymQut-Bp2Er-J4XkYfQkMdJbRlBIBhq4wfhZMHZvag1itn17IwN5FZSanCbcy6S96YZ0Zci5STWNka40861Mn8UQ2yRE’ /http://192.168.122.105:8778/placement/resource_providers#得到resources providers list&#123; \"resource_providers\": [ &#123; \"generation\": 30, \"uuid\": \"4cae2ef8-30eb-4571-80c3-3289e86bd65c\", \"links\": [ &#123; \"href\": \"/placement/resource_providers/4cae2ef8-30eb-4571-80c3-3289e86bd65c\", \"rel\": \"self\" &#125;, &#123; \"href\": \"/placement/resource_providers/4cae2ef8-30eb-4571-80c3-3289e86bd65c/inventories\", \"rel\": \"inventories\" &#125;, &#123; \"href\": \"/placement/resource_providers/4cae2ef8-30eb-4571-80c3-3289e86bd65c/usages\", \"rel\": \"usages\" &#125; ], \"name\": \"f-packstack\" &#125; ]&#125; 其中的generation字段，是一个一致性视图的标志位，跟获取RP的inventories中的resource_provider_generation功能是相同的，其实算作是乐观锁技术，即CAS，Compare and swap，当多个线程尝试使用CAS同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。 下面来一个获取aggregate和inventories的示例，注意，aggregate的API是在1.1版本中实现的，所以要在请求头指定OpenStack-API-Version: placement 1.1： curl -g -i -X GET http://192.168.122.105:8778/placement/resource_providers/4cae2ef8-30eb-4571-80c3-3289e86bd65c/aggregates \\-H \"User-Agent: python-novaclient\" \\-H \"Accept: application/json\" \\-H \"X-Auth-Token: gAAAAABaf5nafUZyFTl_pztozfB65wkP0c26HQqrxRgAiJGsxY8g743LxFOZEI3bF_l37xh0UajbF5nQ1kLYGAonOGphV4AivXgYMUOJ84uGrHjpC60NlmNzzQ3lJGVJb-pNxQw74WsMOc9I0D2B5Mzmf2OgDeictae5f0UFgTR9DFb_vaWCWQ4\" \\-H \"OpenStack-API-Version: placement 1.1\"HTTP/1.1 200 OKDate: Fri, 15 Sep 2017 09:35:21 GMTServer: Apache/2.4.18 (Ubuntu)Content-Length: 18Content-Type: application/jsonOpenStack-API-Version: placement 1.1vary: OpenStack-API-Versionx-openstack-request-id: req-ab28194f-8389-40a1-9a2b-a94dbc792573Connection: close&#123;\"aggregates\": []&#125;curl -g -i -X GET http://192.168.122.105:8778/placement/resource_providers/4cae2ef8-30eb-4571-80c3-3289e86bd65c/inventories \\-H \"User-Agent: python-novaclient\" \\-H \"Accept: application/json\" \\-H 'x-auth-token:gAAAAABae6lX26bp4PEVHCac0cjFnNl18W8DjeQKXDYvuKP4drRJ8t6DC-9uzcCm4E9Xf7NjqSqkRX6WGsE3qHmpAt7GmIu1SrLCtyEOVM2IQP5XLNrwMekGGrzQ_ADOaSTc9XpPpCYyYwzT-zCAvWG-T9T6Ip4l3zHWLwNBBPrm35gBZVZeslQ' \\&#123; \"resource_provider_generation\": 30, \"inventories\": &#123; \"VCPU\": &#123; \"allocation_ratio\": 16, \"total\": 4, \"reserved\": 0, \"step_size\": 1, \"min_unit\": 1, \"max_unit\": 128 &#125;, \"MEMORY_MB\": &#123; \"allocation_ratio\": 1.5, \"total\": 8095, \"reserved\": 512, \"step_size\": 1, \"min_unit\": 1, \"max_unit\": 8095 &#125;, \"DISK_GB\": &#123; \"allocation_ratio\": 1, \"total\": 49, \"reserved\": 0, \"step_size\": 1, \"min_unit\": 1, \"max_unit\": 49 &#125; &#125;&#125; 贴心的社区妥妥的想到了如何可以方便用户操作Placement API，所以开发了一个OpenStackClient Plugin，即osc-placement，需要我们手动安装使用： $ pip install osc-placement 有了OSC placement commands，我们不再需要使用curl命令模拟HTTP请求，并且可以非常轻松的进行操作： [root@f-packstack ~(keystone_admin)]# openstack --debug resource provider list...http://192.168.122.105:8778 \"GET /placement/resource_providers HTTP/1.1\" 200 185RESP: [200] Date: Thu, 08 Feb 2018 05:59:56 GMT Server: Apache/2.4.6 (CentOS) OpenStack-API-Version: placement 1.0 vary: OpenStack-API-Version,Accept-Encoding x-openstack-request-id: req-c6077c19-ca05-4cab-95fa-6129ff989400 Content-Encoding: gzip Content-Length: 185 Keep-Alive: timeout=15, max=100 Connection: Keep-Alive Content-Type: application/jsonRESP BODY: &#123;\"resource_providers\": [&#123;\"generation\": 30, \"uuid\": \"4cae2ef8-30eb-4571-80c3-3289e86bd65c\", \"links\": [&#123;\"href\": \"/placement/resource_providers/4cae2ef8-30eb-4571-80c3-3289e86bd65c\", \"rel\": \"self\"&#125;, &#123;\"href\": \"/placement/resource_providers/4cae2ef8-30eb-4571-80c3-3289e86bd65c/inventories\", \"rel\": \"inventories\"&#125;, &#123;\"href\": \"/placement/resource_providers/4cae2ef8-30eb-4571-80c3-3289e86bd65c/usages\", \"rel\": \"usages\"&#125;], \"name\": \"f-packstack\"&#125;]&#125;GET call to placement for http://192.168.122.105:8778/placement/resource_providers used request id req-c6077c19-ca05-4cab-95fa-6129ff989400+--------------------------------------+-------------+------------+| uuid | name | generation |+--------------------------------------+-------------+------------+| 4cae2ef8-30eb-4571-80c3-3289e86bd65c | f-packstack | 30 |+--------------------------------------+-------------+------------+clean_up ListResourceProvider:END return value: 0 当然还有很多其他的命令，有兴趣的可以尝试玩一下。 划重点：Nova调度与Placement API的结合首先来一张图，来认识一下在P版中，创建一台虚机过程中各个服务之间的调用/调度关系：可以看到，在nova-scheduler与Placement API的交互过程中，有两部分： Get allocation candidates Claim Resources下面，我们结合代码详细的讲述一下调度过程。 Get allocation candidates目前在调度时，nova-conductor在nova.conductor.manager.ComputeTaskManager#_schedule_instances中调用了方法nova.scheduler.client.SchedulerClient#select_destinations： @utils.retry_select_destinationsdef select_destinations(self, context, spec_obj, instance_uuids, return_objects=False, return_alternates=False): return self.queryclient.select_destinations(context, spec_obj, instance_uuids, return_objects, return_alternates) 其中SchedulerClient又调用了SchedulerQueryClient，即调用了nova.scheduler.client.query.SchedulerQueryClient#select_destinations方法： def select_destinations(self, context, spec_obj, instance_uuids, return_objects=False, return_alternates=False): return self.scheduler_rpcapi.select_destinations(context, spec_obj, instance_uuids, return_objects, return_alternates) 在该方法中发起RPC调用，调用了nova.scheduler.manager.SchedulerManager#select_destinations方法： @messaging.expected_exceptions(exception.NoValidHost)def select_destinations(self, ctxt, request_spec=None, filter_properties=None, spec_obj=_sentinel, instance_uuids=None, return_objects=False, return_alternates=False): LOG.debug(\"Starting to schedule for instances: %s\", instance_uuids) ... # 其中USES_ALLOCATION_CANDIDATES默认值为True， # 即表示使用Nova Placement API来选取资源分配候选者 if self.driver.USES_ALLOCATION_CANDIDATES: res = self.placement_client.get_allocation_candidates(ctxt, if res is None: alloc_reqs, provider_summaries, allocation_request_version = ( None, None, None) else: (alloc_reqs, provider_summaries, allocation_request_version) = res if not alloc_reqs: LOG.debug(\"Got no allocation candidates from the Placement \" \"API. This may be a temporary occurrence as compute \" \"nodes start up and begin reporting inventory to \" \"the Placement service.\") raise exception.NoValidHost(reason=\"\") else: # Build a dict of lists of allocation requests, keyed by # provider UUID, so that when we attempt to claim resources for # a host, we can grab an allocation request easily alloc_reqs_by_rp_uuid = collections.defaultdict(list) for ar in alloc_reqs: for rp_uuid in ar['allocations']: alloc_reqs_by_rp_uuid[rp_uuid].append(ar) # Only return alternates if both return_objects and return_alternates # are True. return_alternates = return_alternates and return_objects # self.driver在这里，我们配置使用的是FilterScheduler， # 即又调用了nova.scheduler.filter_scheduler.FilterScheduler#select_destinations # 这个我们后面会提到 selections = self.driver.select_destinations(ctxt, spec_obj, instance_uuids, alloc_reqs_by_rp_uuid, provider_summaries, allocation_request_version, return_alternates) # If `return_objects` is False, we need to convert the selections to # the older format, which is a list of host state dicts. if not return_objects: selection_dicts = [sel[0].to_dict() for sel in selections] return jsonutils.to_primitive(selection_dicts) return selections 我们先来说，这里调用的Placement API，发起一个GET请求，获取Allocation Candidates。 注：没有找到这个API对应的OSC命令，所以我们使用curl命令进行模拟。另，Allocation candidates API requests are availiable starting from version 1.10. # 获取token[root@f-packstack ~(keystone_admin)]# openstack token issue | grep ' id' | awk '&#123;print $4&#125;'gAAAAABajn5nIXMCkZQBwcl7LdqeCV8pOuFSN4ltIUa9GcJ_PO4x920rpw5fwz43BZ8rkKIVlWF1OHfDNs1GRhqhoUHPNkEU6SRNK8G1BFKoHKD4nDJESGhSMrGwDGTIsYeaANqM2D_48tUo_pY0eqCD8iEcRDHi-QCH-c_t_m44So0cHvlXtdE# 使用curl命令发起GET请求，请求参数是resources=DISK_GB:1,MEMORY_MB:512,VCPU:1curl -g -i -X GET http://192.168.122.105:8778/placement/allocation_candidates?resources=DISK_GB:1,MEMORY_MB:512,VCPU:1 \\-H \"User-Agent: python-novaclient\" \\-H \"Accept: application/json\" \\-H \"X-Auth-Token: gAAAAABajn5nIXMCkZQBwcl7LdqeCV8pOuFSN4ltIUa9GcJ_PO4x920rpw5fwz43BZ8rkKIVlWF1OHfDNs1GRhqhoUHPNkEU6SRNK8G1BFKoHKD4nDJESGhSMrGwDGTIsYeaANqM2D_48tUo_pY0eqCD8iEcRDHi-QCH-c_t_m44So0cHvlXtdE\" \\-H \"OpenStack-API-Version: placement 1.10\"HTTP/1.1 200 OKDate: Thu, 22 Feb 2018 08:55:27 GMTServer: Apache/2.4.6 (CentOS)OpenStack-API-Version: placement 1.10vary: OpenStack-API-Version,Accept-Encodingx-openstack-request-id: req-234db1eb-1386-4e89-99bd-c9269270c603Content-Length: 381Content-Type: application/json&#123; \"provider_summaries\": &#123; \"4cae2ef8-30eb-4571-80c3-3289e86bd65c\": &#123; \"resources\": &#123; \"VCPU\": &#123; \"used\": 2, \"capacity\": 64 &#125;, \"MEMORY_MB\": &#123; \"used\": 1024, \"capacity\": 11374 &#125;, \"DISK_GB\": &#123; \"used\": 2, \"capacity\": 49 &#125; &#125; &#125; &#125;, \"allocation_requests\": [ &#123; \"allocations\": [ &#123; \"resource_provider\": &#123; \"uuid\": \"4cae2ef8-30eb-4571-80c3-3289e86bd65c\" &#125;, \"resources\": &#123; \"VCPU\": 1, \"MEMORY_MB\": 512, \"DISK_GB\": 1 &#125; &#125; ] &#125; ]&#125; Placement经过一系列查询之后，返回了一些信息，其中allocation_requests就是我们的请求参数，即我们需要这么些资源，麻烦Placement给看看有合适的RP没？然后Placement帮我们找到了UUID为4cae2ef8-30eb-4571-80c3-3289e86bd65c的RP，还很贴心的在provider_summaries列出了这个RP当前使用的资源量以及存量。实际上这两个查询分别对应了下面的两个SQL语句： -- 1.查询符合要求的Resource ProviderSELECT rp.idFROM resource_providers AS rp -- vcpu信息join -- vcpu总存量信息 INNER JOIN inventories AS inv_vcpu ON inv_vcpu.resource_provider_id = rp.id AND inv_vcpu.resource_class_id = %(resource_class_id_1)s -- vcpu已使用量信息 LEFT OUTER JOIN ( SELECT allocations.resource_provider_id AS resource_provider_id, sum(allocations.used) AS used FROM allocations WHERE allocations.resource_class_id = %(resource_class_id_2)s GROUP BY allocations.resource_provider_id ) AS usage_vcpu ON inv_vcpu.resource_provider_id = usage_vcpu.resource_provider_id -- memory信息join -- memory总存量信息 INNER JOIN inventories AS inv_memory_mb ON inv_memory_mb.resource_provider_id = rp.id AND inv_memory_mb.resource_class_id = %(resource_class_id_3)s -- memory已使用量信息 LEFT OUTER JOIN ( SELECT allocations.resource_provider_id AS resource_provider_id, sum(allocations.used) AS used FROM allocations WHERE allocations.resource_class_id = %(resource_class_id_4)s GROUP BY allocations.resource_provider_id ) AS usage_memory_mb ON inv_memory_mb.resource_provider_id = usage_memory_mb.resource_provider_id -- disk信息join -- disk总存量信息 INNER JOIN inventories AS inv_disk_gb ON inv_disk_gb.resource_provider_id = rp.id AND inv_disk_gb.resource_class_id = %(resource_class_id_5)s -- disk已使用量信息 LEFT OUTER JOIN ( SELECT allocations.resource_provider_id AS resource_provider_id, sum(allocations.used) AS used FROM allocations WHERE allocations.resource_class_id = %(resource_class_id_6)s GROUP BY allocations.resource_provider_id ) AS usage_disk_gb ON inv_disk_gb.resource_provider_id = usage_disk_gb.resource_provider_idWHERE-- vcpu满足上限/下限/步长条件coalesce(usage_vcpu.used, %(coalesce_1)s) + %(coalesce_2)s &lt;= (inv_vcpu.total - inv_vcpu.reserved) * inv_vcpu.allocation_ratio ANDinv_vcpu.min_unit &lt;= %(min_unit_1)s ANDinv_vcpu.max_unit &gt;= %(max_unit_1)s AND%(step_size_1)s % inv_vcpu.step_size = %(param_1)s AND-- memory满足上限/下限/步长条件coalesce(usage_memory_mb.used, %(coalesce_3)s) + %(coalesce_4)s &lt;= (inv_memory_mb.total - inv_memory_mb.reserved) * inv_memory_mb.allocation_ratio ANDinv_memory_mb.min_unit &lt;= %(min_unit_2)s ANDinv_memory_mb.max_unit &gt;= %(max_unit_2)s AND%(step_size_2)s % inv_memory_mb.step_size = %(param_2)s AND-- disk满足上限/下限/步长条件coalesce(usage_disk_gb.used, %(coalesce_5)s) + %(coalesce_6)s &lt;= (inv_disk_gb.total - inv_disk_gb.reserved) * inv_disk_gb.allocation_ratio ANDinv_disk_gb.min_unit &lt;= %(min_unit_3)s ANDinv_disk_gb.max_unit &gt;= %(max_unit_3)s AND%(step_size_3)s % inv_disk_gb.step_size = %(param_3)s-- 2.查询该Resource Provider的用量和存量SELECT rp.id AS resource_provider_id, rp.uuid AS resource_provider_uuid, inv.resource_class_id, inv.total, inv.reserved, inv.allocation_ratio, `usage`.usedFROM resource_providers AS rp -- inventory信息，每个rp的总量 INNER JOIN inventories AS inv ON rp.id = inv.resource_provider_id -- allocation信息 LEFT OUTER JOIN ( -- 每个rp和class的已使用量 SELECT allocations.resource_provider_id AS resource_provider_id, allocations.resource_class_id AS resource_class_id, sum(allocations.used) AS used FROM allocations WHERE allocations.resource_provider_id IN (%(resource_provider_id_1)s) AND allocations.resource_class_id IN ( %(resource_class_id_1)s, %(resource_class_id_2)s, %(resource_class_id_3)s ) -- 按照rp_id和rp_class_id进行分组 GROUP BY allocations.resource_provider_id, allocations.resource_class_id ) AS `usage` ON `usage`.resource_provider_id = inv.resource_provider_id AND `usage`.resource_class_id = inv.resource_class_id-- 查询指定id及class的resourceWHERE rp.id IN (%(id_1)s) AND inv.resource_class_id IN ( %(resource_class_id_4)s, %(resource_class_id_5)s, %(resource_class_id_6)s ) Schedule by fitlers在nova-scheduler获取到allocation candidates之后，还需要使用FilterScheduler对选取的宿主（候选）节点根据启用的过滤器和权重进行计算和过滤。 目前Nova中实现的调度器有以下几种： FilterScheduler（过滤调度器）：默认载入的调度器，根据指定的过滤条件以及权重挑选最佳节点 CachingScheduler：与FilterScheduler功能类似，只不过为了追求的更高的调度性能，将主机资源信息缓存到本地内存中，目前的master代码中标注为[DEPRECATED] ChanceScheduler（随机调度器）：随机选择，真·佛系。不过也在master代码中被标注了[DEPRECATED] FakeScheduler：用于测试，无实际功能 But how does filter scheduler work? 我们依然从代码入手，来张序列图先看为敬： 在FilterScheduler的泳道中，可以看到，大体上分三步： 调度器缓存刷新、状态更新：通过nova.scheduler.host_manager.HostState来维护内存中一份主机状态，并返回可见的计算节点信息 Filtering：实用配置文件指定各种的filters去过滤掉不符合条件的hosts。在配置文件中有两个配置availale_filters和enabled_filters，前者用于指定所有可用的filters，配置为available_filters=nova.scheduler.filters.all_filters；后者表示对于可用的filter，nova-scheduler会使用哪些，配置如enabled_filters=RetryFilter,AvailabilityZoneFilter,RamFilter,DiskFilter等。O版中Nova支持的filters多达27个，实现均位于nova/scheduler/filters目录下，能够处理各类信息，比如主机可用资源、启动请求的参数（如镜像信息、请求重试次数等）、虚机亲和性和反亲和性（与其他虚机是否在同一宿主节点上）等 Weighing：对所有符合条件的host计算权重并排序，从而选出最佳的一个宿主节点。所有的Weigher实现均位于nova/scheduler/weights目录下，比如DiskWeigher： class DiskWeigher(weights.BaseHostWeigher): # 可以设置maxval和minval属性指明权重的最大值和最小值 minval = 0 # 权重的系数，最终排序时需要将每种Weigher得到的权重分别乘上它对应的这个 # 系数，有多个Weigher时才有意义，这里的disk_weight_multiplier # 配置文件默认值为 1.0 def weight_multiplier(self): return CONF.filter_scheduler.disk_weight_multiplier # 计算权重值，按照注释描述，free_disk_mb更大者胜出 def _weigh_object(self, host_state, weight_properties): \"\"\"Higher weights win. We want spreading to be the default.\"\"\" return host_state.free_disk_mb Claim Resources前面我们提到，在获取到Allocation Candidates（即可用于资源分配的候选host）并经过过滤器过滤和权重计算之后，nova-scheduler开始尝试进行Claim resources，即在创建之前预先测试一下所指定的host的可用资源是否能够满足创建虚机的需求。我们来一起看一下nova.scheduler.utils.claim_resources的代码： def claim_resources(ctx, client, spec_obj, instance_uuid, alloc_req, allocation_request_version=None): ... return client.claim_resources(ctx, instance_uuid, alloc_req, project_id, user_id, allocation_request_version=allocation_request_version) 在该方法中，最终调用的还是传入的client的claim_resources()方法，即nova.scheduler.client.report.SchedulerReportClient#claim_resources： @safe_connect@retriesdef claim_resources(self, context, consumer_uuid, alloc_request, project_id, user_id, allocation_request_version=None): \"\"\"Creates allocation records for the supplied instance UUID against the supplied resource providers. 即对指定的实例创建该实例在指定RP上的分配记录 :param context: The security context :param consumer_uuid: The instance's UUID. :param alloc_request: The JSON body of the request to make to the placement's PUT /allocations API :param project_id: The project_id associated with the allocations. :param user_id: The user_id associated with the allocations. :param allocation_request_version: The microversion used to request the allocations. :returns: True if the allocations were created, False otherwise. \"\"\" ar = copy.deepcopy(alloc_request) # If the allocation_request_version less than 1.12, then convert the # allocation array format to the dict format. This conversion can be # remove in Rocky release. if versionutils.convert_version_to_tuple( allocation_request_version) &lt; (1, 12): ar = &#123; 'allocations': &#123; alloc['resource_provider']['uuid']: &#123; 'resources': alloc['resources'] &#125; for alloc in ar['allocations'] &#125; &#125; allocation_request_version = '1.12' url = '/allocations/%s' % consumer_uuid payload = ar # We first need to determine if this is a move operation and if so # create the \"doubled-up\" allocation that exists for the duration of # the move operation against both the source and destination hosts r = self.get(url, global_request_id=context.global_id) if r.status_code == 200: current_allocs = r.json()['allocations'] if current_allocs: payload = _move_operation_alloc_request(current_allocs, ar) payload['project_id'] = project_id payload['user_id'] = user_id r = self.put(url, payload, version=allocation_request_version, global_request_id=context.global_id) if r.status_code != 204: # NOTE(jaypipes): Yes, it sucks doing string comparison like this # but we have no error codes, only error messages. if 'concurrently updated' in r.text: reason = ('another process changed the resource providers ' 'involved in our attempt to put allocations for ' 'consumer %s' % consumer_uuid) raise Retry('claim_resources', reason) else: LOG.warning( 'Unable to submit allocation for instance ' '%(uuid)s (%(code)i %(text)s)', &#123;'uuid': consumer_uuid, 'code': r.status_code, 'text': r.text&#125;) return r.status_code == 204 在这里是发起了一个PUT请求，尝试为consumer_id先声明所需要的资源，并根据返回的HTTP status code来判断是否声明资源成功。一旦能成功声明所需要的资源，就等于找到将该虚机调度到哪一个宿主节点，可以继续后面实际资源的创建等一系列流程，Placement API的工作到这里就暂告一段落了。但是对于scheduler，还有去consumer host的资源，即更新host state等内存中的信息等等。 目前社区Placement的发展通过订阅openstack-dev或者参加nova的weekly meeting，是可以非常及时的获取社区趋势和把握社区的开发进度。那么对Nova Schedule Team来讲，目前这两个月的进度，华为的姜逸坤都给出了比较详尽的记录和整理： Nova Scheduler Team Meeting跟踪（一月） Nova Scheduler Team Meeting跟踪（二月） 目前看起来，调度相关的team还在紧锣密鼓的继续完善Placement的功能，热火朝天向Rocky版本迈进。 有哪些不足目前看起来不足主要集中在使用中的bug及功能的待完善。比如目前还在开发的Nested Resource Providers；为获取Allocation candidates增加limit，控制每次取到的资源候选分配者的数量等等；还有比如主机迁移失败导致两个RP中都有占用的情况等等。像把Placement单独抽离出来，这也是社区有意向要做的事情。 参考[1].Placement API[2].Placement API Reference[3].Yikun’s blog","tags":[{"name":"OpenStack","slug":"OpenStack","permalink":"http://elbarco.cn/tags/OpenStack/"},{"name":"Nova","slug":"Nova","permalink":"http://elbarco.cn/tags/Nova/"}]},{"title":"Nova架构体系概览","date":"2018-01-16T03:11:07.000Z","path":"2018/01/16/learning-nova-architecture-overview/","text":"What’s Nova我们首先来看一下OpenStack的逻辑架构图： 接触过云计算，接触过OpenStack的童鞋都会有所了解，IaaS中最重要的就是计算、存储和网络。Nova，作为OpenStack核心项目，承担起了提供计算资源的重任，即，为用户提供了计算实例，这些实例又称为虚拟机。从上面的逻辑架构图中也可以看到，有了虚拟机之后，才可以外接存储、网络，亦或是有类似Trove（OpenStack中Database-as-a-Service的项目，详见&gt;&gt;&gt;传送门）这种在虚机中运行数据库服务的PaaS项目。当然，虚机也需要像认证服务（Keystone）、镜像（Glance）、网络（Neutron）、存储（Cinder、Swift）这些项目的支持，可谓是你中有我，我中有你。 目前在OpenStack Nova项目的页面（详见&gt;&gt;&gt;传送门）显示的生产环境的应用率高达95%，可以说是很“强势”。 本文就作为Nova学习系列的开篇文章，先熟悉一下Nova架构体系及代码结构。 ComponentsNova项目也是有好几个组件构成，组件的关系架构图如下所示，其中网络模块一个是Nova-networking，一个是Neutron，当然现在大部分使用的都是Neutron，所以我们只关注第二张图就OK了： 可以看到，在Nova中，有这么几个主要的服务： DB：用于数据存储的基础设施数据库 API: 即nova-api服务，通过oslo.messaging队列或者HTTP，接收响应终端用户的计算服务API请求或者与其他组件进行通讯 Scheduler: 即nova-scheduler服务，用于调度每台实例具体落到哪个计算服务节点上 Compute: 即nova-compute服务，管理hypervisor与虚机的通讯，通过虚拟机管理程序API对虚拟机实例进行创建、终止等操作的一个工作守护进程 Conductor: 即nova-conductor服务，处理需要协同合作的请求，比如创建实例和调整实例等操作；同时还扮演了数据库代理的角色或者是处理对象转换 在这个图中，值得注意的一点是Nova的几个主要服务组件之间，是通过oslo.messaging进行RPC调用，与外部服务之间通过HTTP的方式、RESTFul接口进行通讯和交互。 在Compute中，有一个“Hypervisor”，这又是什么呢？ 我们讲，OpenStack其实是一个云管平台，即其本身不提供虚拟化功能，还是要依赖于操作系统底层的虚拟化技术，其中Hypervisor是虚拟化技术的核心。它是一种运行在物理服务器和操作系统之间的中间软件层，可允许多个操作系统和应用共享一套基础物理硬件，因此也可以看作是虚拟环境中的“元”操作系统，它可以协调访问服务器上的所有物理设备和虚拟机，也叫虚拟机监视器（Virtual Machine Monitor）—— In computing, a hypervisor, also called virtual machine monitor (VMM), is a piece of software/hardware platform-virtualization software that allows multiple operating systems to run on a host computer concurrently. 目前常见的Hypervisor有QEMU、KVM、XEN、VMware等，其中，KVM是集成到Linux内核的Hypervisor，是X86架构且硬件支持虚拟化技术（Intel VT或AMD-V）的Linux的全虚拟化解决方案。它是Linux的一个很小的模块，利用Linux做大量的事，如任务调度、内存管理与硬件设备交互等。最为热门，也最为常用。 此外，需要提一下qemu-kvm—— QEMU将KVM整合进来，通过ioctl调用/dev/kvm接口，将有关CPU指令的部分交由内核模块来做。KVM负责CPU虚拟化+内存虚拟化，实现了CPU和内存的虚拟化，但KVM不能模拟其他设备。QEMU模拟IO设备（网卡，磁盘等），KVM加上QEMU之后就能实现真正意义上服务器虚拟化。因为用到了上面两个东西，所以称之为qemu-kvm，来张图： 在KVM这一层之上，是libvirt，它提供统一、稳定、开放源码的对各种虚拟机进行管理的工具（守护进程libvirtd、默认命令行管理工具virsh）和应用程序接口（API）。一些常用的虚拟机管理工具（如virsh、virt-install、virt-manager等）和云计算框架平台（如OpenStack等）都在底层使用libvirt的应用程序接口。 在Nova的Compute服务中，通过不同的驱动来支持多种Hypervisor，与各种Hypervisor驱动的关系可以用下面的一张图来表示： Instance Life Cycle Management Process在实例的生命周期管理流程中，常见的一些操作如下： 创建：nova boot boot from image boot from volume 重启：nova reboot 软重启，默认情况 硬重启：nova reboot –hard，具体不同还需要用代码说话 启动：nova start 停止：nova stop 挂起：nova suspend 暂停：nova pause，pause与suspend的区别在于pause将instance的运行状态保存在计算节点的内存中，而suspend保存在磁盘上。pause的优点在于恢复的速度比suspend快，缺点是如果计算节点重启，内存数据丢失，则无法resume。suspend就不存在该问题 恢复：nova resume 调整实例：nova resize 迁移实例： nova live-migration nova migrate，代码与nova resize相同，如果在resize时未提供flavor id，则仅migrate实例 重建：nova rebuild 快照：nova image-create，对运行的虚机创建一个快照镜像，直接上传到Glance中，可用于恢复主机或以此镜像为模板创建新的主机 备份：nova backup，通过创建一个backup类型的快照来备份主机 删除：nova delete，立即关闭主机并删除实例 后续需要对上面这些主要流程进行梳理，包括上面描述的几个命令的不同或者相似之处，我们让代码来说话。 Reference[1]. 虚拟化类型[2]. Under the Hood with Nova, Libvirt and KVM[3]. OpenStack系列–Nova[4]. OpenStack Compute(Nova)","tags":[{"name":"OpenStack","slug":"OpenStack","permalink":"http://elbarco.cn/tags/OpenStack/"},{"name":"Nova","slug":"Nova","permalink":"http://elbarco.cn/tags/Nova/"}]},{"title":"Understanding message with RabbitMQ","date":"2017-12-01T10:17:31.000Z","path":"2017/12/01/understanding-message-with-rabbitmq/","text":"本文为先导文章，对消息的一些概念，AMQP的架构、基本知识点进行一个梳理和学习，为OpenStack中基于AMQP实现RPC调用的后续文章做个铺垫。 AMQP概述AMQP(Advanced Message Queuing Protocol)，即高级消息队列协议，是一种应用层网络协议，它为特定客户端应用(application)与消息中间件代理(messaging middleware broker)之间的通信提供支持。本文针对AMQP 0-9-1 模型作一个简单的介绍，该模型即rabbitmq所使用的模型。 RabbitMQ中的消息流用过RabbitMQ的同学肯定对下面这个图会非常理解：总体来讲，消息的生产者，产生消息，将消息发到消息队列RabbitMQ；消息的消费者在队列中取得消息执行后续操作，这就是RabbitMQ中的消息流。 然而在在将消息推送到MQ或者在MQ中消费时，我们要连接到MQ上。在连接的时候，客户端会创建一个TCP连接到RabbitMQ broker上。一旦连接成功，则客户端会创建一个AMQP channel。AMQP的channel是在TCP连接上的虚拟频道，当我们发布消息，订阅一个队列或者是接收消息，均在频道中完成——为什么需要AMQP channel呢？因为TCP会话的建立和销毁对于操作系统来讲，是十分昂贵的。我们假设说，我们的客户端连接到MQ上进行消息消费，短时间内产生大量的TCP连接，消费完成后，又要将这些TCP连接销毁，这不仅会造成了TCP连接的巨大浪费，而且操作系统每秒钟创建的连接数量有限。很快我们就会遇到性能瓶颈。于是，AMQP channel就诞生了，在一个TCP连接上使用多个频道，每个频道都会被分配一个唯一ID作为标识，在保证每个线程的私有连接的前提下，显著的提高性能，下面是一个生动的示意图： 从队列说起现在，已经对消息整个的生产、消费过程有了大概的了解，我们再进到内部去看下，消息究竟在RabbitMQ内部是如何流转的。 从概念上来讲，消息的成功流转离不开三部分：exchange，queue和binding： Exchange是生产者发布消息的地方 Queue是消息结束并被消费者接收的地方 Binding就是消息如何从特定的Exchange被路由到指定队列的一系列规则 获取队列中的消息我们先来说说队列。在队列中获取消息有两种方式： 使用AMQP命令basic.consume来启动一个队列的消费者（订阅者），如果你的消费者需要处理一个队列的大量消息或者要求一旦有消息达到队列能够立刻自动的接收到消息，则需要使用这种方式； 使用AMQP命令basic.get直接访问队列获取一条消息。使用该命令后会使得消费者接收队列的下一条消息，并且在下次调用basic.get之前不会再接收队列的消息，即订阅队列，接收单条消息，取消订阅。千万不要在循环中使用basic.get以求替代basic.consume，要合理的进行订阅来提高吞吐。 消息队列无订阅者或有多个订阅者如果消息队列没有订阅的消费者，消息会在队列中等待。 如果一个RabbitMQ消息队列有多个消费者，那么队列中的消息将以轮询的方式服务于消费者，即，每条消息只会发送给订阅该队列的某一个消费者。 消息确认消费者接收到的每条消息都需要得到确认——每个消费者可以选择要么显示的通过使用AMQP命令basic.ack发送确认通知给RabbitMQ，或者可以选择在订阅到队列的时候设置参数auto_ack为true，指定了该参数后，RabbitMQ会在消费者接收到消息后自动认为消息已经确认收到了。注意，这里的消息确认，不是告知消息的发送者，而是告诉RabbitMQ消费者已经收到了消息，可以安全的将该消息在队列中移除了。 如果处理消息比较集中和耗时，可以考虑延迟确认消息，直到处理结束。 消息拒绝如果消费者在处理某条消息的时候没有发送确认信息（如断开连接等），则RabbitMQ会认为该消费者不具备接收消息的条件，会将该消息重新发送给下一个订阅者。但是这种消息拒绝的方式会增加服务器负担。 我们还可以使用basic.reject来拒绝RabbitMQ发送给消费者的消息。 注：此外，对RabbitMQ来说，还可以使用basic.nack，这是RabbitMQ中对reject命令特殊的扩展实现。 如果设置reject命令的参数requeue为true，则RabbitMQ会将消息发送给下一个订阅的消费者，否则RabbitMQ会立刻在队列中删除这条消息而不发送给新的消费者。 当然，不想处理消息的时候还可以通过确认消息已收到来处理，在收到某些格式不正确的消息并确认没有消费者能处理时，这么操作十分有效。 注，在RabbitMQ的某些新版本中，会支持一个特殊的dead letter队列，即无法投递的消息队列。如果使用reject命令并设置参数requeue为false，则消息会被丢到该队列。 创建队列消息的消费者或者生产者都可以使用AMQP命令queue.declare来创建队列。但是消费者不能在已经在相同频道上订阅到其他队列的前提下声明或创建队列，必须先取消订阅将频道至于一种“可传输”的模式。 创建队列时，一般由消费者指定队列的名字，如果没有指定，则RabbitMQ会随机生成一个名字，在queue.declare的返回值中体现出来。随机队列名在一些临时的匿名队列场景下非常有用，比如基于AMQP应用的RPC调用。 在创建队列时，有两个参数很有用： exclusive - 设置为true，则队列会设置为私有状态，常用于控制队列只允许有一个消费者的情况； auto-delete - 队列在最后一个消费者取消订阅后自动删除，如果只需要一个临时队列提供给一个消费者，结合auto-delete和exclusive两个参数，当消费者断开连接时，队列自动被删除。 创建一个队列，恰好这个队列已经存在，RabbitMQ会直接返回成功。这个特性可以用于判断队列是否存在，在创建队列时，指定queue.declare的参数passive为true即可；如果队列不存在，则直接返回一个错误信息并不创建队列。 小节结语队列是AMQP消息的基石—— 为等待被消费的消息提供了栖息地； 完美的适用于负载均衡，只需要使很多消费者订阅同一个队列即可——因为RabbitMQ会使用轮询的方式处理消息； RabbitMQ中所有消息的终点 开，往消息队列开……前面我们对消息队列Queue进行了比较详尽的介绍，那现在的问题是，消息是怎么抵达消息队列的？这时候，就需要exchange和binding了。 所有的消息均要先发送到exchang（路由），然后基于特定的规则，RabbitMQ会决定将消息发往哪个队列。这些规则被称为routing keys，一个队列可以说“通过一个routing key，绑定到一个exchange上”。 如果遇到了多个队列该怎么办？这里就要提到四种exchange类型，分别是direct、fanout、topic和headers，每一种都实现了不同的路由算法。headers允许通过匹配AMQP消息的header而不是routing key，所以我们这里不去深究和探讨了。 Direct exchange字面意思，直接路由。如果routing key匹配，则消息会被发送到响应的队列中，如下图所示： 所有的消息队列必须实现这种方式，包括创建一个名称为空字符串的exchange，如：$channel-&gt;basic_publish($msg, &apos;&apos;, &apos;queue-name&apos;); 第一个参数标识了要发送的消息，第二个参数，一个空字符串，标识了指向默认的exchange，第三个参数就是routing key，也就是声明队列所使用的名称。 如果默认的direct exchange不能满足要求，可以使用exchange.declare命令创建自己所需要的exchange。 Fanout exchange扇区路由，示意图如下： Exchange会将收到的消息组播（multicast）到绑定的消息队列中，即这种模式下，支持应用根据一个（only one）消息做出不同的反应。比如我们考虑这么一个用户场景，在用户上传完图片后，既要更新图片缓存，又要奖励用户操作，那么此时如果使用fanout exchange，只需要将两个consumer都绑定到这个exchange上即可。那么如果还需要在上传图片后增加新的处理，只需要写好消费者的功能代码绑定到exchange上即可，对于消息生产者来讲，代码是完全解耦的。 Topic exchange这种路由方式，可以实现来自不同消息源的消息到达同一队列，示意图如下： Topic exchange与Direct exchange有些类似，都是通过匹配特定的routing key来讲消息发送给绑定到exchange上的queue中。但是对于Topic exchange来讲，有两个特殊的binding key： *，星号，替代/匹配一个单词 #，井号，替代/匹配零个或者多个词 注：如果队列使用的routing key是一个#，则会收到所有消息，忽略routing key的话，这就类似绑定到一个fanout exchange上；如果在routing key中不使用#或者*，则与direct exchange无异。 这里留一个小问题：为什么OpenStack中使用Topic Exchange比较多？ 多租户：虚拟主机（vhost）和隔离每个RabbitMQ server都有能力创建多个虚拟的消息代理，即virtual host，简称vhost。每个vhost都是一个迷你的RabbitMQ server，具有自己独有的Queue、Exchange和Binding，更重要是的是，具有自己的权限。这使得多个应用同时可以安全无忧的使用同一个RabbitMQ服务器。 在RabbitMQ中，默认的vhost=/，在不需要多租户的场景下，默认值就足够了。在创建RabbitMQ用户的时候需要指定至少一个vhost。 注，通过vhost隔离的租户是绝对的，即你不能将vhost A的队列绑定到vhost B的exchange上。 可以使用命令查看vhosts：[root@rabbit1 ~]# rabbitmqctl list_vhostsListing vhosts .../ 使用命令创建一个vhost：[root@rabbit1 ~]# rabbitmqctl add_vhost fCreating vhost &quot;f&quot; ...[root@rabbit1 ~]# rabbitmqctl list_vhostsListing vhosts ...f/ 消息的持久化每个Queue和Exchange，都有一个durable属性，默认值为false，即默认情况下RabbitMQ不会在服务器宕机或者重启后重建Queue或Exchange，所以建议这个值一定要设置成true。 此外，只有Queue和Exchange的durable还不完全够，消息的持久化还需要三个要点： 将其选项delivery mode要设置成2，即persistent，持久的； 消息被发布到durable的Exchange； 消息抵达一个durable的Queue 满足上述三个条件，消息的持久化就稳了。 RabbitMQ通过将持久化的消息写入磁盘日志文件来确保消息在重启时不是丢失，即当发布一个持久化的消息到持久化的exchange，在写入到日志文件之前是不会发送消息的响应。如果持久化的消息被路由到非持久化的队列，则会自动在持久化日志中移除，即无法保证消息在重启时不会丢失。 然而持久化虽好，却不要“贪杯”。因为将消息持久化到磁盘上比直接存储在内存中要慢很多，这就会面临几个问题： 会减少RabbitMQ每秒处理的消息数量，这个降低的比例甚至能达到10倍或者更多； 持久化消息在RabbitMQ的内置集群中表现不佳； 那到底应不应该使用persistent/durable消息呢？首先还是要评估一下性能需求。如果单节点的RabbitMQ需要每秒处理100,000+的数据，那么可能持久化信息就不是一个好的选择。 解决事务的方案：发送方确认模式由于AMQP内部事务对性能有很大瓶颈，现采取发送方确认模式保证事务，将信道设置为confirm模式，所有在此信道上发布的消息都会有一个唯一的ID号，当被投递到匹配的队列时，信道就会发送一个发送方确认模式给生产者应用程序，这个模式是异步的，应用程序可以等待确认的同时继续发送下一条，但如果是持久化的消息，会在写入磁盘之后消息发出。 如果发送内部错误而导致消息丢失，RabbitMQ会发送一条nack(not acknowledged,未确认)消息，这种模式下每分钟可追踪数以百万计的消息投递。","tags":[{"name":"AMQP","slug":"AMQP","permalink":"http://elbarco.cn/tags/AMQP/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://elbarco.cn/tags/RabbitMQ/"}]},{"title":"Python中的Coroutine","date":"2017-10-18T08:15:00.000Z","path":"2017/10/18/python-coroutines/","text":"在展开对eventlet的学习之前，我们先来学习一下Python的Coroutine。 前情回顾在这篇文章中，已经学习过了Python中的Generator和yield关键字，如果对生成器和yield还有疑问，可以通过上面的连接回顾一下。 For…example?这里，就以一个生成器的例子来展开本篇的学习内容吧：def grep(pattern): print 'Looking for \"%s\"' % pattern while True: line = (yield) if pattern in line: print line 首先思考一个问题，执行上面的函数函数的输出是什么？ 协程的执行当我们常用yiled关键字的时候，不可避免的，总会遇到Coroutine，即协程。正如上面的例子，函数能做的不仅是生成值，还可以“消费”（consume）发送给它的值：&gt;&gt;&gt; g = grep('python')&gt;&gt;&gt; g.next()Looking for \"python\"&gt;&gt;&gt; g.send('Awesome, it is dope!')&gt;&gt;&gt; g.send('python generators rock!')python generators rock! 当我们直接调用grep(&#39;python&#39;)时，什么输出也没有产生，因为coroutine只对next()和send()方法进行响应。即，g.next()时，coroutine开始运行（或者通过send(None)来预启动协程），然后使协程提前执行到第一个yield表达式——line = (yield)，此时，协程已经准备好了接收一个值，当我们发送含有python的字符串时，就可以打印出这个字符串。 不过，每次调用.next()有点太麻烦，我们可以用装饰器包装这个coroutine来解决：def coroutine(func): def start(*args, **kwargs): cr = func(*args, **kwargs) cr.next() return cr return start@coroutinedef grep(pattern): ... 协程的关闭协程可能会无限运行，我们可以使用.close()来关闭。另外，close()是可以被捕获的——通过GeneratorExit异常：@coroutinedef grep(pattern): print 'Looking for \"%s\"' % pattern try: while True: line = (yield) if pattern in line: print line except GeneratorExit: print 'Going away. Bye!' 不要忽略这个异常，通过上面的写法可以确保coroutine能够正常清理和退出。执行后效果如下：&gt;&gt;&gt; g = grep('python')&gt;&gt;&gt; g.next()Looking for \"python\"&gt;&gt;&gt; g.send('Awesome, it is dope!')&gt;&gt;&gt; g.send('python generators rock!')python generators rock!&gt;&gt;&gt; g.close()Going away. Bye! 协程中抛出异常在协程中，是允许抛出异常的：&gt;&gt;&gt; g = grep('python')&gt;&gt;&gt; g.next()Looking for \"python\"&gt;&gt;&gt; g.send('Awesome, it is dope!')&gt;&gt;&gt; g.send('python generators rock!')python generators rock!&gt;&gt;&gt; g.throw(RuntimeError, \"It's a RuntimeError!\")Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; File \"&lt;stdin&gt;\", line 4, in grepRuntimeError: It's a RuntimeError! 注意，异常是在yield表达式处产生的，而且跟普通异常一样是可以被捕获和处理的。 简单梳理一下经过上面的例子，我们可以简单的梳理如下： Generator产生数据用于迭代 Coroutine是数据的消费者 不要把这两个概念弄混 More, I want MORE!通通连起来Coroutine还可以用于构造pipeline（管道），即把好多coroutine连起来，通过send()方法来传递数据。 对于pipeline来讲，我们需要一个函数来驱动，我们暂且称之为source。另外还需要一个端点（end-point）来终止整个管道，我们暂且称之为sink，下面举个例子，用coroutine写一个类似tail -f的功能：def follow(the_file, target): # Go to the end of the file the_file.seek(0, 2) while True: line = the_file.readline() if not line: time.sleep(0.1) continue target.send(line)@coroutinedef printer(): while True: line = (yield) print lineif __name__ == '__main__': f = open(\"data.txt\") follow(f, printer()) 这样，我们在执行时，打开data.txt写入信息，就会在控制台看到输出。在这个例子中，follow()用于逐行读取，然后把数据发送到printer()协程中，过程如图： 在这里，source就是follow()，sink就是printer()。 管道中的过滤器在这个例子基础上，我们还可增加一个协程用做过滤器（filter），只要对对前面的grep()稍作改造，然后调用的时候注意一下：@coroutinedef grep(pattern, target): print 'Looking for \"%s\"' % pattern try: while True: line = (yield) if pattern in line: target.send(line) except GeneratorExit: print 'Going away. Bye!'if __name__ == '__main__': f = open(\"data.txt\") follow(f, grep('python', printer())) 启动后，grep()这个协程负责只有在data.txt中写入行含有python才会把当前行数据发送到printer()，由其在控制台打印出来，过程如图： 注：coroutine和generator的关键区别在于生成器使用迭代器在管道中拉取数据；协程通过send()向管道中推送数据。 管道连接更多的管道有了协程，我们可以将数据发送到更多的地方…… 那么我们就来一个🌰，下列代码实现了一个广播的coroutine，将数据推送到批量的coroutines中：@coroutinedef broadcast(targets): while True: item = (yield) for target in targets: target.send(item) 根据调用方式的不同，实际上会产生两种效果广播的情况——①发送到不同的printer()follow(f, broadcast([grep('python', printer()), grep('hello', printer()), grep('world', printer())]) ) ②发送到相同的printer()p = printer()follow(f, broadcast([ grep('python', p), grep('hello', p), grep('world', p)])) 不过在本例中，效果是一样的…… 从数据处理到并发编程到目前为止，我们前面聊的coroutine的应用都是在处理数据，那么如果我们把数据发送给线程、发送给进程……协程程序自然而然就会涉及到线程和分布式系统的问题。 看到这，估计也累了，那暂且先挖个坑，未完待续…… 参考[1].A Curious Course on Coroutines and Concurrency","tags":[{"name":"Python","slug":"Python","permalink":"http://elbarco.cn/tags/Python/"},{"name":"Coroutines","slug":"Coroutines","permalink":"http://elbarco.cn/tags/Coroutines/"}]},{"title":"学习和认识Trove Strategies","date":"2017-10-17T07:28:57.000Z","path":"2017/10/17/introduction-to-trove-strategies/","text":"在Trove的上篇文章中，我们简单的介绍了一下Trove的架构和各个组件，最近看到一张图，感觉非常清晰，列到这里： 看图，可以简单回顾一下，Trove这个基于OpenStack中的计算、存储、网络、镜像等之上的一个DBaaS项目，图中可以看到它的几个组件，以及它跟其他OpenStack组件的交互。 Trove strategies对于Trove来讲，其目标就是提供一个数据库无关的功能集，并且可以在框架内实现扩展，这里我们就引入了Trove的Strategies，即策略。 策略是Trove中的一个设计结构，允许开发人员在Trove作为整体框架的前提下，建立指定抽象的新的实现来扩展Trove，举个🌰来说明—— 我们知道，无论是MySQL、PostgreSQL等关系型数据库，还是其他的非关系型数据库比如MongoDB、Redis，均提供备份的功能。然而不同数据库的备份功能又有所差别，甚至同一个数据库具有不同的集中方式来产生一个备份。下面结合MySQL备份的消息流转图来说明： MySQL备份功能中的策略 客户端通过Trove API发起备份的请求，通过阅读代码我们知道调用的API的方法是trove.backup.service.BackupController#create，API通过消息队列发起对Task Manager中create_backup的异步调用（见trove.taskmanager.api.API#create_backup）:def create_backup(self, backup_info, instance_id): LOG.debug(\"Making async call to create a backup for instance: %s\" % instance_id) self._cast(\"create_backup\", self.version_cap, backup_info=backup_info, instance_id=instance_id) 然后Task Manager中（见trove.taskmanager.manager.Manager#create_backup）:def create_backup(self, context, backup_info, instance_id): with EndNotification(context, backup_id=backup_info['id']): instance_tasks = models.BuiltInstanceTasks.load(context, instance_id) instance_tasks.create_backup(backup_info) 其中又调用了BuildInstanceTasks中的create_backup方法，这里实际发起了对Guest Agent中备份创建的调用：def create_backup(self, backup_info): LOG.info(_(\"Initiating backup for instance %s.\") % self.id) self.guest.create_backup(backup_info) 对于MySQL来讲，Guest Agent中我们肯定要去找trove.guestagent.datastore.mysql_common.manager.MySqlManager，那么对应的方法就是：def create_backup(self, context, backup_info): \"\"\" Entry point for initiating a backup for this guest agents db instance. The call currently blocks until the backup is complete or errors. If device_path is specified, it will be mounted based to a point specified in configuration. :param backup_info: a dictionary containing the db instance id of the backup task, location, type, and other data. \"\"\" with EndNotification(context): backup.backup(context, backup_info) 我们在去看看backup.backup(context, backup_info)（见trove.guestagent.backup.backup）：from trove.guestagent.backup.backupagent import BackupAgentAGENT = BackupAgent()def backup(context, backup_info): \"\"\" Main entry point for starting a backup based on the given backup id. This will create a backup for this DB instance and will then store the backup in a configured repository (e.g. Swift) :param context: the context token which contains the users details :param backup_id: the id of the persisted backup object \"\"\" return AGENT.execute_backup(context, backup_info) 其中，对实际执行备份，使用的是trove.guestagent.backup.backupagent.BackupAgent中的execute_backup：CONFIG_MANAGER = CONF.get('mysql' if not CONF.datastore_manager else CONF.datastore_manager)STRATEGY = CONFIG_MANAGER.backup_strategyBACKUP_NAMESPACE = CONFIG_MANAGER.backup_namespaceRESTORE_NAMESPACE = CONFIG_MANAGER.restore_namespaceRUNNER = get_backup_strategy(STRATEGY, BACKUP_NAMESPACE)class BackupAgent(object): ... def execute_backup(self, context, backup_info, runner=RUNNER, extra_opts=EXTRA_OPTS, incremental_runner=INCREMENTAL_RUNNER): LOG.debug(\"Running backup %(id)s.\", backup_info) ... self.stream_backup_to_storage(context, backup_info, runner, storage, parent_metadata, extra_opts) 注意BACKUP_NAMESPACE指明了策略的命名空间，这里的RUNNER，才是实际加载备份策略的地方，当然，要根据/trove/common/cfg.py中的配置：cfg.StrOpt('backup_namespace', default='trove.guestagent.strategies.backup.mysql_impl', help='Namespace to load backup strategies from.', deprecated_name='backup_namespace', deprecated_group='DEFAULT'),cfg.StrOpt('backup_strategy', default='InnoBackupEx', help='Default strategy to perform backups.', deprecated_name='backup_strategy', deprecated_group='DEFAULT'), 当然，这部分配置我们是可以在trove-guestagent.conf中配置的：# ========== Datastore Specific Configuration Options ==========...[mysql]# For mysql, the following are the defaults for backup, and restore:backup_strategy = InnoBackupExbackup_namespace = trove.guestagent.strategies.backup.mysql_impl... 那我们就知道，其实MySQL的备份使用的是trove.guestagent.strategies.backup.mysql_impl.InnoBackupEx：class InnoBackupEx(base.BackupRunner): \"\"\"Implementation of Backup Strategy for InnoBackupEx.\"\"\" __strategy_name__ = 'innobackupex' ... @property def cmd(self): cmd = ('sudo innobackupex' ' --stream=xbstream' ' %(extra_opts)s ' + self.user_and_pass + MySqlApp.get_data_dir() + ' 2&gt;/tmp/innobackupex.log' ) return cmd + self.zip_cmd + self.encrypt_cmd ... 这里额外说一下，究竟是在哪里触发执行的备份呢？ 在前面self.stream_backup_to_storage(context, backup_info, runner, storage, parent_metadata, extra_opts)中，使用了with来执行runner：def stream_backup_to_storage(self, context, backup_info, runner, storage, parent_metadata=&#123;&#125;, extra_opts=EXTRA_OPTS): ... try: with runner(filename=backup_id, extra_opts=extra_opts, **parent_metadata) as bkup: LOG.debug(\"Starting backup %s.\", backup_id) ... 即这里，调用了InnoBackupEx的构造方法，而InnoBackupEx是继承自BackupRunner，则：class BackupRunner(Strategy): \"\"\"Base class for Backup Strategy implementations.\"\"\" __strategy_type__ = 'backup_runner' __strategy_ns__ = 'trove.guestagent.strategies.backup' # The actual system call to run the backup cmd = None is_zipped = CONF.backup_use_gzip_compression is_encrypted = CONF.backup_use_openssl_encryption encrypt_key = CONF.backup_aes_cbc_key def __init__(self, filename, **kwargs): self.base_filename = filename self.process = None self.pid = None kwargs.update(&#123;'filename': filename&#125;) self.command = self.cmd % kwargs super(BackupRunner, self).__init__() ... def _run(self): LOG.debug(\"BackupRunner running cmd: %s\", self.command) self.process = subprocess.Popen(self.command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, preexec_fn=os.setsid) self.pid = self.process.pid def __enter__(self): \"\"\"Start up the process.\"\"\" self._run_pre_backup() self._run() return self ... 看到这里，我们就知道备份执行是在哪里触发的了——首先，在构造方法中，通过self.command = self.cmd % kwargs获取了InnoBackupEx中的cmd，然后因为我们使用with来调用，则会调用__enter__(self)方法，其中的_run(self)中使用Popen协程来真正的执行。后面补充一篇对eventlent和Popen的学习心得。 其他策略应用场景除了上面例子中的MySQL备份功能，Trove的Strategies还可以通过扩展Guest Agent、API和Task Manager等来实现集群的功能，比如下面列举了MongoDB中的扩展：cfg.StrOpt('api_strategy', default='trove.common.strategies.cluster.experimental.' 'mongodb.api.MongoDbAPIStrategy', help='Class that implements datastore-specific API logic.'),cfg.StrOpt('taskmanager_strategy', default='trove.common.strategies.cluster.experimental.mongodb.' 'taskmanager.MongoDbTaskManagerStrategy', help='Class that implements datastore-specific task manager ' 'logic.'),cfg.StrOpt('guestagent_strategy', default='trove.common.strategies.cluster.experimental.' 'mongodb.guestagent.MongoDbGuestAgentStrategy', help='Class that implements datastore-specific Guest Agent API ' 'logic.'), 当用户请求APItrove.cluster.service.ClusterController#create，对MongoDB创建集群时，经过各种调用，会去执行trove.common.strategies.cluster.experimental.mongodb.api.MongoDbCluster#create来创建MongoDB的Cluster。后面的具体流程我们这里就在赘述了，感兴趣的童鞋可以去阅读一下代码。不过目前在MongoDB集群功能中，还没有区分Replica set和Shard，需要紧跟上游或者自己实现（有位同事最近在做这项功能，还是很有希望提交上游的……这就跑题了😂） 结语本文主要以MySQL备份的代码过程为例，讲解了Trove中的Strategies的应用，了解其基本原理和几个使用场景，同时在看代码的过程中，遇到的新的知识点eventlet和eventlet.green.subprocess.Popen，值得我们去学习和研究一番。 参考[1].OpenStack Trove by Amrith Kumar[2].OpenStack Architecture[3].openstack/trove","tags":[{"name":"OpenStack","slug":"OpenStack","permalink":"http://elbarco.cn/tags/OpenStack/"},{"name":"Trove","slug":"Trove","permalink":"http://elbarco.cn/tags/Trove/"}]},{"title":"Git中使用rebase命令更新本地分支","date":"2017-09-28T06:34:49.000Z","path":"2017/09/28/git-rebase-local-branch-with-upstream-branch/","text":"最近在开发trove中，因为误提交，本地项目的devel分支已经与上游的的devel分支不一致了。为了更好的创建分支，或者后面进行cherry-pick准备打包，都需要将本地的分支与上游的分支做一下rebase。 注：上游指的是eayunstack/trove，本地指的是2hf/trove 首先，我们要添加upstream远程仓库：$ git remoteorigin$ git remote add upstream git@github.com:eayunstack/trove.git$ git remote -vorigin git@github.com:2hf/trove.git (fetch)origin git@github.com:2hf/trove.git (push)upstream git@github.com:eayunstack/trove.git (fetch)upstream git@github.com:eayunstack/trove.git (push) 然后更新upstream：$ git fetch upstream 此时远程仓库已经准备就绪了，这时候我们就可以rebase本地的分支了。两种做法：# option one$ git checkout devel$ git rebase -i upstream/devel# option two$ git checkout devel$ git reset --hard upstream/devel 注：如果之前2hf/devel分支已经做过push了，为了保持与上游一致，需要git push -f。","tags":[{"name":"Git","slug":"Git","permalink":"http://elbarco.cn/tags/Git/"},{"name":"rebase","slug":"rebase","permalink":"http://elbarco.cn/tags/rebase/"}]},{"title":"Python中的装饰器——装饰器参数篇","date":"2017-09-20T08:52:14.000Z","path":"2017/09/20/python-introduction-to-decorator-arguments/","text":"先举个例子在上一篇文章中，我们提到的装饰器的例子有一个共同的特点，就是只接收被装饰的方法作为参数。但是在很多时候，装饰器本身接收更多的参数是非常有用的。但是如何做到呢？我们先回想一下基本的装饰器，它在内部声明一个方法，然后将这个内部方法返回，即被装饰器返回的callable。如果要使装饰器接收更多的参数，我们就要再包装一层——即接收参数的”装饰器”并不是真正的装饰器，而是一个返回装饰器的函数，而真正的装饰器负责接收被装饰的方法作为参数，然后装饰方法，返回一个callable。 还是用前面json_output的例子，简单改造一下：import functoolsimport jsonclass JSONOutputError(Exception): def __init__(self, message): self._message = message def __str__(self): return self._messagedef json_output(indent=None, sort_keys=False): \"\"\"Run the decorated function, serialize the result of that function to JSON, and return the JSON string. \"\"\" def actual_decorator(decorated): @functools.wraps(decorated) def inner(*args, **kwargs): try: result = decorated(*args, **kwargs) except JSONOutputError as ex: result = &#123; 'state': 'error', 'message': str(ex), &#125; return json.dumps(result, indent=indent, sort_keys=sort_keys) return inner return actual_decorator 可以看到，在json_output中，传入了两个参数indent和sort_keys, 返回的是装饰器actual_decorator，而在inner中用到了传入的两个参数，用于JSON格式化时的缩进和Key的排序展示，来看这个装饰器的效果：@json_output(indent=4, sort_keys=True)def do_nothing(): return &#123;'status': 'done','a': '1'&#125;# output &#123; \"a\": \"1\", \"status\": \"done\"&#125; 还有这种操作？通过上面的例子，可以看到，装饰器是actual_decorator而不是json_output，那么问题来了，如果json_output不是装饰器而只是一个返回装饰器的函数，为什么可以像装饰器一样使用？ 问题的关键在于操作的顺序。具体来说，json_output(indent=4,sort_keys=True)的调用在前，@操作符应用在后，那么这个函数的结果就会被当作装饰器使用。即先调用json_output，其中定义了装饰器actual_decorator，并且由json_output返回，则此时再应用@操作符就等价于：@actual_decoratordef do_nothing(): return &#123;'status': 'done','a': '1'&#125; 这不就相当于在一个函数上应用装饰器嘛！ 重要的一点是要意识到，当我们引入新的json_output函数时，实际上引入了一个后向不兼容的修改。 为什么这么说？因为现在这里有一个预期的额外函数调用。如果这里我们不想给json_output传递参数，那么我们依然要调用这个函数，即程序必须这么写：@json_output()def do_nothing(): return &#123;'status': 'done','a': '1'&#125; 号外：一定要注意圆括号！因为这表示函数是被调用，然后函数结果被应用@。 上面的代码，不等同于，注意，是不等同于下面的写法：@json_outputdef do_nothing(): return &#123;'status': 'done','a': '1'&#125;# print do_nothing() outputTypeError: actual_decorator() takes exactly 1 argument (0 given) 这里有两个问题：其一是比较让人疑惑，因为一旦习惯于见到没有括号的装饰器，见到类似json_output这种就会觉得反常；其二是如果旧的装饰器已经应用了其他很多地方，如果修改了这个装饰器类似上面的例子，那么其他应用的地方要一并修改，因为这是一个后向不兼容的更改。 理想情况下，我们希望装饰器在程序中对于下列三种应用方式都能兼容： @json_output @json_output() @json_output(indent=4, sort_keys=True) 实时证明，这是可行的，只需要让装饰器根据参数来改变其行为即可。下面，我们改写一下json_output：def json_output(decorated_=None, indent=None, sort_keys=False): \"\"\"Run the decorated function, serialize the result of that function to JSON, and return the JSON string. \"\"\" if decorated_ and (indent or sort_keys): raise RuntimeError('Unexpected arguments.') def actual_decorator(func): @functools.wraps(func) def inner(*args, **kwargs): try: result = func(*args, **kwargs) except JSONOutputError as ex: result = &#123; 'state': 'error', 'message': str(ex), &#125; return json.dumps(result, indent=indent, sort_keys=sort_keys) return inner if decorated_: return actual_decorator(decorated_) else: return actual_decorator 注意，我们为json_output增加了一个入参，decorated_=None，因为我们不希望同时传递被装饰的方法和关键字参数，所以在json_output中先做检查，来确保我们要么只传递被装饰的方法作为参数，要么只传递关键字参数。接着，actual_decorator是实际的装饰器。最终，如果设置了decorated_，即采用@json_output这种方式去修饰do_nothing方法（等价于do_nothing = json_output(decorated_=do_nothing)），则此时的json_output就是一个装饰器，那么我们应该返回的是actual_decorator(decorated_)；如果没有设置decorated_，则这就是一个方法，即可以通过@json_output()或@json_output(indent=4)的方式应用到do_nothing()上去——# 1def do_nothing(): return &#123;'status': 'done','a': '1'&#125;do_nothing = json_output(decorated_=do_nothing)print do_nothing()# 1 - output&#123;\"status\": \"done\", \"a\": \"1\"&#125;# 2@json_outputdef do_nothing(): return &#123;'status': 'done','a': '1'&#125;print do_nothing()# 2 - output&#123;\"status\": \"done\", \"a\": \"1\"&#125;# 3@json_output()def do_nothing(): return &#123;'status': 'done','a': '1'&#125;print do_nothing()# 3 - output&#123;\"status\": \"done\", \"a\": \"1\"&#125;# 4@json_output(indent=4, sort_keys=True)def do_nothing(): return &#123;'status': 'done','a': '1'&#125;print do_nothing()# 4 - output&#123; \"a\": \"1\", \"status\": \"done\"&#125;# 5def do_nothing(): return &#123;'status': 'done','a': '1'&#125;do_nothing = json_output(decorated_=do_nothing, indent=4)print do_nothing()# 5 - outputTraceback (most recent call last): File \"/decorators/output.py\", line 77, in &lt;module&gt; do_nothing = json_output(decorated_=do_nothing, indent=4) File \"/decorators/output.py\", line 18, in json_output raise RuntimeError('Unexpected arguments.')RuntimeError: Unexpected arguments. 1、2等价，json_output就是一个实实在在的装饰器；3、4中的json_output则是一个带参数的装饰器。 结语本篇中，我们知道了如何给”装饰器”传递更多的参数，同时也学到了如何让”装饰器”更灵活，以适应不同的应用形式。","tags":[{"name":"Python","slug":"Python","permalink":"http://elbarco.cn/tags/Python/"},{"name":"decorator","slug":"decorator","permalink":"http://elbarco.cn/tags/decorator/"}]},{"title":"Python中的装饰器——初识篇","date":"2017-09-20T06:53:15.000Z","path":"2017/09/20/python-introduction-to-decorators-with-examples/","text":"认识装饰器装饰器简单来讲就是一个接收被装饰的函数作为固定参数的函数（或称为callable，比如一个对象具有call方法），并且对被装饰的函数做些处理。举个例子：def decorated_by(func): func.__doc__ += '\\nDecorated by decorated_by.' return funcdef add(x, y): \"\"\"Return the sum of x and y.\"\"\" return x + yif __name__ == '__main__': add = decorated_by(add) print add.__doc__# output Return the sum of x and y.Decorated by decorated_by. 大多数情况下，我们只关心最终被装饰的函数，而持有装饰器函数的引用基本上是没必要的。所以，在Python 2.5中引入了对装饰器的特殊语法——装饰器的应用可以通过在被装饰的函数上面一行添加@字符，其后紧跟装饰器函数名的这种方式：比如针对上面的例子，我们就可以写作：@decorated_bydef add(x, y): \"\"\"Return the sum of x and y.\"\"\" return x + yif __name__ == '__main__': # add = decorated_by(add) add(1, 2) print add.__doc__# outputReturn the sum of x and y.Decorated by decorated_by. 添加@+装饰器名的这种方式就等价于 add = decorated_by(add)。这种方式看起来更简洁明了。 装饰器应用的顺序当@语法被使用时，装饰器会在被装饰的callable被创建后立即调用（即装饰器的代码是在应用到被装饰的函数上时执行，而不是被装饰的函数调用时执行）。就上面的例子而言，就是add函数被创建后，紧接着decorated_by函数被应用。那么如果对于单个callable使用@语法应用多个装饰器时（Python中是支持这种场景的），装饰器的应用顺序有事怎样的？答案就是：从下往上，按顺序执行。举个例子：我们有另外的一个函数叫做also_decorated_by，也是在func.doc后面添加一段话，然后对add函数应用该装饰器：def also_decorated_by(func): func.__doc__ += '\\nAlso decorated by also_decorated_by.' return func@also_decorated_by@decorated_bydef add(x, y): \"\"\"Return the sum of x and y.\"\"\" return x + y 按照从下往上的顺序，我们知道当我们调用add后，执行decorated_by相当于add = decorated_by(add) ，然后对此时的add应用also_decorated_by，就相当于add = also_decorated_by(decorated_by(add))。最终的结果正是：if __name__ == '__main__': # add = decorated_by(add) add(1, 2) print add.__doc__Return the sum of x and y.Decorated by decorated_by.Also decorated by also_decorated_by. 装饰器的应用场景标准库中的很多模块都包含有装饰器，许多常见的工具和框架都将其用于常见的功能。例如，如果要在类上创建一个方法，调用该方法时不需要该类的实例，则可以使用@classmethod或@staticmethod装饰器，这是标准库中的一个简单例子。常用的工具中也是用装饰器，比如常见的Python Web框架Django中，使用@login_required作为装饰器允许开发者指定用户在访问特定页面时必须要登录。另外一个Web框架Flask中使用@app.route来注册指定的URI被访问到时要执行的函数。再比如，Celery中使用一个复杂的@task装饰器来标识一个函数为一个异步任务，这个装饰器的返回实际上是一个Task类的实例，展示出了如何用装饰器来制作方便快捷的API。 为什么要使用装饰器 有了装饰器，你就可以做到在某些特定的地方使用具体的、可复用的功能——如果代码写得好，装饰器就是模块化的、明确的。正是由于装饰器的模块化，使得它们非常适合避免重复的模版设置和拆解代码，同时由于装饰器仅与被装饰的函数本身有交互，所以非常擅长在其他地方注册功能。在Python应用程序和模块中编写装饰器有几个非常好的用例—— 附加功能 - 在被装饰的方法前后执行额外的附加功能 数据清洗或添加 - 对传入被装饰的函数的参数做一下清晰，确保参数类型一致性或者使参数值村从一定的模式，比如@requires_ints 功能注册 动手写几个装饰器纸上得来终觉浅，绝知此事要躬行。写下来就要动手写几个装饰器的例子。 功能注册先上代码打个样： class Registry(object): def __init__(self): self._functions = [] def register(self, decorated): self._functions.append(decorated) return decorated def run_all(self, *args, **kwargs): return_values = [] for func in self._functions: return_values.append(func(*args, **kwargs)) return return_valuesa = Registry()b = Registry()@a.registerdef foo(x=3): return x@b.registerdef bar(x=5): return x@a.register@b.registerdef baz(x=7): return xa_r = a.run_all() # [3, 7]b_r = b.run_all() # [5, 7]a_r = a.run_all(x=4) # [4, 4] 运行时封装代码（wrap code）上面装饰器的例子比较简单，因为被装饰的函数作为参数传递过去时并没有被修改。然而，有些时候当我们运行被装饰的方法时，希望做些额外的功能，那么我们可以通过返回不同的可调用函数（callable）来添加相应的功能，并且（通常）在执行过程中调用装饰方法。举几个例子： 类型检查def requires_int(decorated): def inner(*args, **kwargs): \"\"\" Get any values that may have been sent as keyword arguments. \"\"\" kwarg_values = [i for i in kwargs.values()] for arg in list(args) + kwarg_values: if not isinstance(arg, int): raise TypeError('%s only accepts integers as arguments.' % decorated.__name__) return decorated(*args, **kwargs) return inner@requires_intdef foo(x,y): \"\"\" Return the sum of x and y\"\"\" return x+yif __name__ == '__main__': print foo(1,y='a')# output Traceback (most recent call last): File \"F:/Test/PyTest/decorators/typecheck.py\", line 17, in &lt;module&gt; print foo(1,y='a') File \"F:/Test/PyTest/decorators/typecheck.py\", line 7, in inner decorated.__name__)TypeError: foo only accepts integers as arguments. 上面的例子中，我们使用装饰器@requires_int对foo的参数进行检查，如果入参中有任何不是int类型的参数，就抛出一个异常。 如果此时我们执行help(foo)会发现，得到的结果是：Help on function inner in module __main__:inner(*args, **kwargs) Get any values that may have been sent as keyword arguments. 很奇怪吧，为什么显示的是inner的函数名及doc而不是foo呢？因为此时inner函数已经被赋值给了foo，如果我们执行foo，其实就是执行inner——首先执行了类型检查，然后运行被装饰的方法，因为inner通过return decorated(args, *kwargs)调用了被装饰的函数。如果没有return这个调用，被装饰的方法就会被忽略。 保留原函数的帮助信息经过上面的例子，我们会自然而然的思考一问题，如果当我们运行help()去查看函数的帮助信息时，希望看到的是被装饰的原函数的文档，而不是装饰器的文档，该怎么做呢？这里，我们的解决方案就是使用装饰器@functools.wraps ，它可以将一个函数重要的内部元素拷贝到另一个函数中：import functoolsdef requires_int(decorated): @functools.wraps(decorated) def inner(*args, **kwargs): \"\"\" Get any values that may have been sent as keyword arguments. \"\"\" kwarg_values = [i for i in kwargs.values()]... 那么此时查看help(foo)，我们得到的输出结果是：Help on function foo in module __main__:foo(*args, **kwargs) Return the sum of x and y 注：我这里运行的是Python2.7，如果实在Python3下，看到的结果应该是foo(x, y)。 用户校验import functoolsclass User(object): def __init__(self, username, email): self.username = username self.email = emailclass AnonymousUser(User): def __init__(self): self.username = None self.email = None def __nonzero__(self): return Falsedef require_user(func): @functools.wraps(func) def inner(user, *args, **kwargs): if user and isinstance(user, User): return func(user, *args, **kwargs) else: raise ValueError('A valid user is required to run this.') return inner@require_userdef foo(user): if user.username and user.email: print user.username + ',' + user.email else: print 'None'if __name__ == '__main__': user = User('Tom','tom.smith@tom.com') a_user = AnonymousUser() foo(user) foo(a_user)# outputTom,tom.smith@tom.comTraceback (most recent call last): File \"F:/Test/PyTest/decorators/user.py\", line 39, in &lt;module&gt; foo(a_user) File \"F:/Test/PyTest/decorators/user.py\", line 25, in inner raise ValueError('A valid user is required to run this.')ValueError: A valid user is required to run this. 我们定义了一个User类，一个AnonymousUser类集成User类，注意，在AnonymousUser中，我们使用了nonzero方法，这个方法定义了对类的实例调用bool()时的行为，即在require_user的inner方法中，if user and isinstance(user, User)时的if user，相当于if bool(user)，因为在AnonymousUser中我们定义了nonzero返回False，所以这里就没有办法通过if检查，从而rasie了异常。 格式化输出除了为函数的输入清理数据，装饰器的另一个作用就是清理/格式化输出数据。比如我们希望函数的输出以JSON格式，在每个相关的函数的最后手动去格式化输出为JSON显得特别冗余和笨重。此时，装饰器就站出来了。 import functoolsimport jsonclass JSONOutputError(Exception): def __init__(self, message): self._message = message def __str__(self): return self._messagedef json_output(decorated): \"\"\"Run the decorated function, serialize the result of that function to JSON, and return the JSON string. \"\"\" @functools.wraps(decorated) def inner(*args, **kwargs): try: result = decorated(*args, **kwargs) except JSONOutputError as ex: result = &#123; 'status': 'error', 'message': str(ex), &#125; return json.dumps(result) return inner@json_outputdef do_nothing(): return &#123;'status': 'done'&#125;@json_outputdef error(): raise JSONOutputError('This function is erratic')@json_outputdef other_error(): raise ValueError('The grass is always greener..')if __name__ == '__main__': # print do_nothing() print error()# output&#123;\"status\": \"error\", \"message\": \"This function is erratic\"&#125; 打印日志最后一个例子，打印日志，记录调用了哪个方法、什么时间调用、方法执行时长以及方法的返回值：import functoolsimport loggingimport timelogging.basicConfig(evel=logging.DEBUG)LOG = logging.getLogger(__name__)def logged(method): @functools.wraps(method) def inner(*args, **kwargs): start = time.time() return_value = method(*args, **kwargs) end = time.time() delta = end - start LOG.warn('Called method %s at %.02f; execution time %.02f ' 'seconds; result %r.' % (method.__name__, start, delta, return_value)) return return_value return inner@loggeddef sleep_and_return(return_value): time.sleep(2) return return_valueif __name__ == '__main__': print sleep_and_return(27)# output WARNING:__main__:Called method sleep_and_return at 1505889774.61; execution time 2.00 seconds; result 27.27 小节结语本篇文章的重点是简单的认识一下装饰器，了解一下装饰器的简单应用。 目前上面的例子，装饰器除了被装饰的函数作为参数之外，都不接收其他的参数，但是很多情况下，装饰器本身接收其他参数是很有必要的。我们后面再展开。","tags":[{"name":"Python","slug":"Python","permalink":"http://elbarco.cn/tags/Python/"},{"name":"decorator","slug":"decorator","permalink":"http://elbarco.cn/tags/decorator/"}]},{"title":"Python中的生成器和yield关键字","date":"2017-09-18T08:49:46.000Z","path":"2017/09/18/python-generators-and-yield-keyword/","text":"前言我们都知道yield语句用于定义生成器，替代函数的return语句来向其调用者提供结果，并且不破坏局部变量。与函数不同的是，每次调用时，生成器会以新的变量集开始，继续执行它被关闭的执行。 关于Python生成器Python中的生成器的目的是能够即时的按照我们的要求逐个计算一系列结果。举个最简单的例子，生成器可以用作列表，列表中的每个元素会在用到的时候的方式被计算（lazily）：&gt;&gt;&gt; # 定义列表&gt;&gt;&gt; the_list = [2**i for i in range(5)]&gt;&gt;&gt; # 类型检查，确实是一个列表&gt;&gt;&gt; type(the_list)&lt;type 'list'&gt;&gt;&gt;&gt; for j in the_list:... print j... 124816&gt;&gt;&gt; # 列表长度为5&gt;&gt;&gt; len(the_list)5&gt;&gt;&gt; # 定义一个生成器，注意是'()'而不是'[]'&gt;&gt;&gt; the_gen = (x+x for x in range(5))&gt;&gt;&gt; # 类型检查，确实是一个生成器&gt;&gt;&gt; type(the_gen)&lt;type 'generator'&gt;&gt;&gt;&gt; # 遍历生成器中的元素，并打印&gt;&gt;&gt; for j in the_gen:... print j... 02468&gt;&gt;&gt; # 看起来好像跟列表似的，那如果我们来检查一下长度……&gt;&gt;&gt; len(the_gen)Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;TypeError: object of type 'generator' has no len()&gt;&gt;&gt; 从上面的例子，可以看出，遍历列表和遍历生成器是一样的。不过，尽管生成器可遍历的，但是却不是一个数据集合，因此没有长度的属性。数据集合（比如列表、元组、集合等）将数据存储在内存中，所以我们需要时就可以获得；生成器即时的计算结果，然后下一次迭代时就把上一次结果“忘掉了”，所以生成器没有对自己结果集的任何概述。 正因为生成器有这个特性——不需要同时在内存中保留数据集合中的全部元素——所以非常适合内存敏感的任务。当我们不需要完整的结果时，逐个的计算结果值的做法就显得十分有用，对调用者即时的返回中间结果，直到满足一些要求然后停止处理。 使用Python的yield关键字这里我们有一个很好的例子，就是当我们在搜索时，我们不需要等所有的结果都被查找出来。比如在文件系统中搜索时，用户更希望能即时的看到结果，而不是等搜索工具遍历每个文件，然后返回所有的结果。再比如，用Google搜索的用户会一直翻到最后一页吗？ 这里我们就可以使用yield关键字/语句来定义一个生成器。yield指令应当放在生成器立即返回结果给调用者并且等待下次调用发生的地方。举个例子，我们先定义一个用于在大文件中逐行搜索关键字的生成器：def search(keyword, filename): print 'Generator started' f = open(filename,'r') for line in f: if keyword in line: yield line f.close()# 在data.txt中搜索yield关键字the_gen = search('yield', 'data.txt')# 检查the_gen的类型print type(the_gen)# 也可以用the_gen.next()或next(the_gen)遍历for i in the_gen: print i 最终，我们得到的the_gen的类型是&lt;type &#39;generator&#39;&gt;，遍历the_gen得到data.txt中包含yield关键字的每一行，输出结果为：&lt;type &apos;generator&apos;&gt;Generator startedUsing the Python &quot;yield&quot; keywordThe yield instruction should be put into a place... Since the yield keyword is only used with generators... 更多的例子生成器的应用有很多，比如扮演传送带的角色，一个比较好的例子即缓冲区：获取大量的数据并将其以小数据块进行处理：def buffered_read(): while True: buffer = fetch_big_chunk() for small_chunk in buffer: yield small_chunk 最后我们再看一个经典例子——给定数字N，使用生成器给出前N个斐波那契序列（Fibonacci）数字：def fibonacci(n): curr = 1 prev = 0 counter = 0 while counter &lt; n: yield curr prev, curr = curr, prev + curr counter += 1f = fibonacci(6) for i in f: print i112358 直到counter = n，停止while循环。 参考[1].Python generators and the yield keyword[2].What does the “yield” keyword do?","tags":[{"name":"Python","slug":"Python","permalink":"http://elbarco.cn/tags/Python/"},{"name":"yield","slug":"yield","permalink":"http://elbarco.cn/tags/yield/"}]},{"title":"OpenStack DBaaS组件Trove简介","date":"2017-07-25T08:02:28.000Z","path":"2017/07/25/introduction-to-trove/","text":"Trove架构Trove包含下面几个主要组件： API Server Message Bus Task Manager Guest Agent Conductor API ServerAPI endpoint（trove-api）本质上是一个HTTP的Web服务，具备处理鉴权、授权、与数据存储相关的基本命令和控制功能。根据数据库不同，API还有一些不同的扩展。 API Server与两个系统沟通——与Task Manager沟通，来处理复杂的异步任务；直接与Guest Agent沟通来处理简单的任务比如获取MySQL用户列表等，这部分操作均是同步的。API Server不做任何重大/复杂的事情，它的任务就是接收请求，将其转化为消息，校验它们，并将这些消息转发到任务管理器（Task Manager）和访客代理（Guest Agent）。 一个RESTful风格的组件 入口 - Trove/bin/trove-api 使用WSGI launcher，由Trove/etc/trove/api-paste.ini配置 定义了过滤器管道、令牌认证、速率限制等 定义了app_factory为trove.common.api:app_factory提供给trove应用 API 类（WSGI Router）将REST路径连接到相应的Controller上 Controller的实现在相关的模块下（如versions/instance/flavor/limits）的service.py中 Controller通常将实现重定向到models.py中的一个类 另一些组件（Task Manager，Guest Agent）的api模块通过RabbitMQ发送请求 Message Bus这部分组件仿照了Nova架构。Message Bus其实就是一个消息队列。 一个典型的消息传递事件从API服务器接收到用户的请求开始。API服务器认证用户确保用户具备执行响应命令的权限。对请求中涉及到的对象的可用性进行评估，如果可用，将请求路由到相关Worker的排队引擎。Workers不断根据自己的角色监听消息队列，当这种监听产生一个工作请求时，Worker将对该任务进行任务分配并开始执行。完成任务后，Worker将响应发送到消息队列，由API服务器接受并中继到始发用户的队列。在整个过程中，数据库记录根据需要会被查询、添加或者删除。 Task ManagerTask Manager（trove-taskmanager）就是干粗活累活的家伙，比如配置一台实例，管理实例生命周期和在实例上进行操作。任务管理器接收来自API Server的消息，通过同意消息进行响应，并开始执行任务。有几个复杂的任务，比如重新分配数据库规格和创建实例等，他们均需要通过HTTP请求调用OpenStack的服务，同时也需要轮询服务，知道实例变为活动状态，并且还向客户代理发送消息。任务管理器处理在多个分布式系统中发生的进程流。 任务管理器是有状态的，它在其系统内部运行复杂的流程。如果在状态处理期间任务管理器节点脱机，则操作将失败。任务流系统将最终实现为长时间运行运行的任务。（The Task Flow system will be eventually implemented for long running tasks.） 这是一个监听RabbitMQ topic的服务 入口 - Trove/bin/trove-taskmanager 作为一个RpcService运行，通过Trove/etc/trove/trove-taskmanager.conf.sample配置文件进行配置，定义了trove.taskmanager.manager.Manager作为manager，基本上这是通过队列到达的请求的入口点 如上所述，使用TaskManager的api模块，使用_cast()或者_call()（同步/异步）将对该组件的请求从另一个组件推送到MQ中，并放置方法命作为一个参数 Trove/openstack/common/rpc/dispatcher.py 中的RpcDispatcher.dispatch()通过反射的方式调用Manager中合适的方法 然后，Manager将该处理重定向到models.py模块中的一个对象，它使用context和instance_id从相关类加载一个对象 实际的处理一般在models.py中完成 Guest Agent客户代理（Guest Agent，trove-guestagent）运行在客户实例内部，负责管理和执行数据存储本身的操作。它负责使数据存储在线，这可能是一个复杂的任务。热支持（Heat support）将来将成为Trove的默认配置和仪器引擎，从而减少了将数据存储库联机的任务。Guest Agent还通过Conductor（指挥器）向API Server发送心跳信息。 每个数据存储器都实现有一个客户端代理，负责为该数据存储器执行特定人物。比如Redis的客户代理行为与MySQL的客户代理行为就会不同。不过他们必须履行诸如创建和调整规格的基础操作。 与Task Manager类似，服务运行起来监听RabbitMQ topic Guest Agent在每个数据库实例中运行，所以使用专有的RabbitMQ topic（通过实例ID来标识） 入口 - Trove/bin/trove-guestagent 作为一个RpcService运行，通过Trove/etc/trove/trove-guestagent.conf.sample配置文件进行配置，定义了trove.guestagent.manager.Manager作为manager，基本上这是通过队列到达的请求的入口点 如上所述，使用Guest Agent的api模块，使用_cast()或者_call()（同步/异步）将对该组件的请求从另一个组件推送到MQ中，并放置方法命作为一个参数 Trove/openstack/common/rpc/dispatcher.py 中的RpcDispatcher.dispatch()通过反射的方式调用Manager中合适的方法 然后，Manager将对对象的处理重定向到dbaas.py中 实际处理一般在dbaas.py中完成 Conductor指挥器（Conductor）是运行在宿主机上的饿一个服务，负责接收客户实例中的消息，并在宿主机上更新信息，比如，实例的状态和当前备份的状态。有了指挥器，用户的实例不需要直接连接到宿主机的数据库。指挥器通过Message Bus监听RPC消息，并执行相关的操作。指挥器与客户代理有些类似，因为它是一个监听RabbitMQ主题的服务，不同的是Conductor运行在宿主机上，而非客户实例内部。客户代理通过将消息放入配置的消息队列——conductor_queue，默认为trove-conductor——来与指挥器进行信息交互。 入口 - Trove/bin/trove-conductor 作为一个RpcService运行，通过Trove/etc/trove/trove-conductor.conf.sample配置文件进行配置，定义了trove.conductor.manager.Manager作为Manager 如上面的客户代理类似，请求通过其他组件使用_cast()（异步的）推送到消息队列。一般来讲，消息格式为{&quot;method&quot;: &quot;&lt;method_name&gt;&quot;, &quot;args&quot;: {&lt;arguments&gt;}} 实际的数据库更新操作由trove/conductor/manager.py完成 “heartbeat”操作更新实例的状态，通常由Guest Agent来报告实例状态，如从NEW到BUILDING到ACTIVE等等 “update_backup”方法修改备份的详情，包括它的当前状态、备份大小、类型和校验码（checksum） 代码仓库 Trove Server (https://github.com/openstack/trove) Trove Integration (https://github.com/openstack/trove-integration) Trove Client (https://github.com/openstack/python-troveclient) 安装部署 How to install trove as part of devstack: trove/installation How to use trove-integration: trove/trove-integration How to set up unit tests to run with tox: trove/unit-testing How to set up a testing environment and run redstack tests after installation: trove/integration-testing How to set up your Mac dev environment to debug: trove/dev-env Releasing python-troveclient trove/release-python-troveclient Creating release notes with Reno trove/create-release-notes-with-reno 说明翻译自Trove wiki","tags":[{"name":"OpenStack","slug":"OpenStack","permalink":"http://elbarco.cn/tags/OpenStack/"},{"name":"Trove","slug":"Trove","permalink":"http://elbarco.cn/tags/Trove/"},{"name":"Translation","slug":"Translation","permalink":"http://elbarco.cn/tags/Translation/"}]},{"title":"由一次服务连接MongoDB超时引发的思考","date":"2017-06-28T09:12:28.000Z","path":"2017/06/28/mongodb-connecttimeout-and-sockettimeout/","text":"起因今天某个业务操作突然执行失败，查询服务日志发现，服务在些写数据的时候，连接被重置，和MongoTimeoutException，截取部分日志如下：Caused by: com.mongodb.MongoException$Network: Operation on server xx.xx.xx.xx:27017 failed at com.mongodb.DBTCPConnector.doOperation(DBTCPConnector.java:215) ~[mongo-java-driver-2.13.0.jar:na] ... 63 common frames omittedCaused by: java.net.SocketException: Connection reset at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:118) ~[na:1.7.0_80] ... 72 common frames omitted..........Caused by: com.mongodb.MongoTimeoutException: Timed out after 0 ms while waiting for a server that matches &#123;serverSelectors=[ReadPreferenceServerSelector&#123;readPreference=primary&#125;, LatencyMinimizingServerSelector&#123;acceptableLatencyDifference=15 ms&#125;]&#125;. Client view of cluster state is &#123;type=ReplicaSet, servers=[&#123;address=xx.xx.xx.xx:27017, type=ReplicaSetArbiter, averageLatency=1.3 ms, state=Connected&#125;, &#123;address=xx.xx.xx.xx:27017, type=Unknown, state=Connecting&#125;, &#123;address=xx.xx.xx.xx:27017, type=ReplicaSetSecondary, averageLatency=1.1 ms, state=Connected&#125;] at com.mongodb.BaseCluster.getServer(BaseCluster.java:82) ~[mongo-java-driver-2.13.0.jar:na] ... 49 common frames omitted 可以看到我们的服务在连接MongoDB时超时——没找到primary节点，在0ms后Timeout，抛出异常，即下面这段异常才是暴露问题的地方：Caused by: com.mongodb.MongoTimeoutException: Timed out after 0 ms while waiting for a server that matches ...Client view of cluster state is...&#123;address=xx.xx.xx.xx:27017, type=Unknown, state=Connecting&#125;...] 分析首先，通过在日志中找到最关键的那一部分，分析得出当时业务异常的问题是业务server到MongoDB Master节点网络稍微有些抖动/延迟，而我们配置的connectTimeout=0ms，所以业务server没有取得replica set中的primary节点，不知道该如何写入数据，才会抛出这个异常。 这里需要将connectTimeout适当调整即可。 探究在我们的上层，无论何时创建MongoClient，driver会建立和service的连接，应用建立连接的等待时长和客户端请求后等待服务器响应的时长，取决于我们的connection timeout和socket timeout两个参数。 Connection timeout这个参数决定了我们的客户端等待建立与服务器建立一个连接的最长时间。一方面，我们希望连接超时时间足够长，这样我们的应用即使在面对较大的服务器负载或者断断续续的网络延迟的情况下，也可以较为可靠的与服务器端建立起一个连接，但是，另一方面，我们又不希望这个值过大，否则应用会hang住，在服务器暂时不可访问的情况下，过度的浪费时间在等待服务器的连接上。 所以设置这个值的大小便是仁者见仁智者见智的事情了。 这个参数的默认值，在Java的dirver中，com.mongodb.MongoClientOptions.Builder#connectTimeout的默认值是1000*10ms，即10s。 那在上面我们的案例中，设置为0ms显然是不可理的，网络延迟是不可能不存在的。 Socket timeout这个参数决定了我们的客户端等待服务端响应的最长时间，这个timeout参数控制了所有类型的请求——query、write、commands、authentication等等。 如果我们将数值设置为30s，则客户端不会等服务端响应超过30s钟。所以通常来讲，我们是不会限制这个时间的，这样可以使数据库的操作响应比较自由。 在大多数的driver中，这个参数都是无限大（或者没有限制的）。这个参数在Java的driver中，com.mongodb.MongoClientOptions.Builder#socketTimeout值是0,用于表示不限制。 参考[1].Do you want a timeout?[2].mongodb connection timed out error[3].Class MongoClientOptions","tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"http://elbarco.cn/tags/MongoDB/"}]},{"title":"Got signal:6 (Aborted) 引起的MongoDB崩溃分析解决","date":"2017-06-23T01:51:41.000Z","path":"2017/06/23/mongodb-crashed-with-the-Got-signal-6-Aborted/","text":"一、背景近日，同事在对MongoDB的读写压力进行测试，再插入大量数据时，常会遇到MongoDB服务莫名崩溃。于是，这边对日志进行了分析——发现，在日志中，有如下的一段backtrace：2017-06-21T11:59:31.290+0800 F - [conn963] Got signal: 6 (Aborted). 0xf5e669 0xf5dce2 0xf5e096 0x3221032660 0x32210325e5 0x3221033dc5 0xda0c59 0x8dd622 0x8de181 0x8b31d7 0x8d1a17 0x8d34d6 0x9bdc64 0x9bebed 0x9bf8fb 0xb9340a 0xaa3480 0x7e99fd 0xf1badb 0x3221407aa1 0x32210e8aad----- BEGIN BACKTRACE -----&#123;&quot;backtrace&quot;:[&#123;&quot;b&quot;:&quot;400000&quot;,&quot;o&quot;:&quot;B5E669&quot;&#125;,&#123;&quot;b&quot;:&quot;400000&quot;,&quot;o&quot;:&quot;B5DCE2&quot;&#125;,&#123;&quot;b&quot;:&quot;400000&quot;,&quot;o&quot;:&quot;B5E096&quot;&#125;,&#123;&quot;b&quot;:&quot;3221000000&quot;,&quot;o&quot;:&quot;32660&quot;&#125;,&#123;&quot;b&quot;:&quot;3221000000&quot;,&quot;o&quot;:&quot;325E5&quot;&#125;,&#123;&quot;b&quot;:&quot;3221000000&quot;,&quot;o&quot;:&quot;33DC5&quot;&#125;,&#123;&quot;b&quot;:&quot;400000&quot;,&quot;o&quot;:&quot;9A0C59&quot;&#125;,&#123;&quot;b&quot;:&quot;400000&quot;,&quot;o&quot;:&quot;4DD622&quot;&#125;,&#123;&quot;b&quot;:&quot;400000&quot;,&quot;o&quot;:&quot;4DE181&quot;&#125;,&#123;&quot;b&quot;:&quot;400000&quot;,&quot;o&quot;:&quot;4B31D7&quot;&#125;,&#123;&quot;b&quot;:&quot;400000&quot;,&quot;o&quot;:&quot;4D1A17&quot;&#125;,&#123;&quot;b&quot;:&quot;400000&quot;,&quot;o&quot;:&quot;4D34D6&quot;&#125;,&#123;&quot;b&quot;:&quot;400000&quot;,&quot;o&quot;:&quot;5BDC64&quot;&#125;,&#123;&quot;b&quot;:&quot;400000&quot;,&quot;o&quot;:&quot;5BEBED&quot;&#125;,&#123;&quot;b&quot;:&quot;400000&quot;,&quot;o&quot;:&quot;5BF8FB&quot;&#125;,&#123;&quot;b&quot;:&quot;400000&quot;,&quot;o&quot;:&quot;79340A&quot;&#125;,&#123;&quot;b&quot;:&quot;400000&quot;,&quot;o&quot;:&quot;6A3480&quot;&#125;,&#123;&quot;b&quot;:&quot;400000&quot;,&quot;o&quot;:&quot;3E99FD&quot;&#125;,&#123;&quot;b&quot;:&quot;400000&quot;,&quot;o&quot;:&quot;B1BADB&quot;&#125;,&#123;&quot;b&quot;:&quot;3221400000&quot;,&quot;o&quot;:&quot;7AA1&quot;&#125;,&#123;&quot;b&quot;:&quot;3221000000&quot;,&quot;o&quot;:&quot;E8AAD&quot;&#125;],&quot;processInfo&quot;:&#123; &quot;mongodbVersion&quot; : &quot;3.0.6&quot;, &quot;gitVersion&quot; : &quot;1ef45a23a4c5e3480ac919b28afcba3c615488f2&quot;, &quot;uname&quot; : &#123; &quot;sysname&quot; : &quot;Linux&quot;, &quot;release&quot; : &quot;2.6.32-642.6.2.el6.x86_64&quot;, &quot;version&quot; : &quot;#1 SMP Wed Oct 26 06:52:09 UTC 2016&quot;, &quot;machine&quot; : &quot;x86_64&quot; &#125;, &quot;somap&quot; : [ &#123; &quot;elfType&quot; : 2, &quot;b&quot; : &quot;400000&quot; &#125;, &#123; &quot;b&quot; : &quot;7FFC4BCCC000&quot;, &quot;elfType&quot; : 3 &#125;, &#123; &quot;path&quot; : &quot;/lib64/libpthread.so.0&quot;, &quot;elfType&quot; : 3 &#125;, &#123; &quot;path&quot; : &quot;/lib64/librt.so.1&quot;, &quot;elfType&quot; : 3 &#125;, &#123; &quot;path&quot; : &quot;/lib64/libdl.so.2&quot;, &quot;elfType&quot; : 3 &#125;, &#123; &quot;path&quot; : &quot;/usr/lib64/libstdc++.so.6&quot;, &quot;elfType&quot; : 3 &#125;, &#123; &quot;path&quot; : &quot;/lib64/libm.so.6&quot;, &quot;elfType&quot; : 3 &#125;, &#123; &quot;path&quot; : &quot;/lib64/libgcc_s.so.1&quot;, &quot;elfType&quot; : 3 &#125;, &#123; &quot;path&quot; : &quot;/lib64/libc.so.6&quot;, &quot;elfType&quot; : 3 &#125;, &#123; &quot;path&quot; : &quot;/lib64/ld-linux-x86-64.so.2&quot;, &quot;elfType&quot; : 3 &#125; ] &#125;&#125; mongod(_ZN5mongo15printStackTraceERSo+0x29) [0xf5e669] mongod(+0xB5DCE2) [0xf5dce2] mongod(+0xB5E096) [0xf5e096] libc.so.6(+0x32660) [0x3221032660] libc.so.6(gsignal+0x35) [0x32210325e5] libc.so.6(abort+0x175) [0x3221033dc5] mongod(_ZN5mongo12SecureRandom6createEv+0x1B9) [0xda0c59] mongod(_ZN5mongo31SaslSCRAMSHA1ServerConversation10_firstStepERSt6vectorISsSaISsEEPSs+0x16F2) [0x8dd622] mongod(_ZN5mongo31SaslSCRAMSHA1ServerConversation4stepERKNS_10StringDataEPSs+0x2F1) [0x8de181] mongod(_ZN5mongo31NativeSaslAuthenticationSession4stepERKNS_10StringDataEPSs+0x27) [0x8b31d7] mongod(+0x4D1A17) [0x8d1a17] mongod(+0x4D34D6) [0x8d34d6] mongod(_ZN5mongo12_execCommandEPNS_16OperationContextEPNS_7CommandERKSsRNS_7BSONObjEiRSsRNS_14BSONObjBuilderEb+0x34) [0x9bdc64] mongod(_ZN5mongo7Command11execCommandEPNS_16OperationContextEPS0_iPKcRNS_7BSONObjERNS_14BSONObjBuilderEb+0xC1D) [0x9bebed] mongod(_ZN5mongo12_runCommandsEPNS_16OperationContextEPKcRNS_7BSONObjERNS_11_BufBuilderINS_16TrivialAllocatorEEERNS_14BSONObjBuilderEbi+0x28B) [0x9bf8fb] mongod(_ZN5mongo8runQueryEPNS_16OperationContextERNS_7MessageERNS_12QueryMessageERKNS_15NamespaceStringERNS_5CurOpES3_+0x77A) [0xb9340a] mongod(_ZN5mongo16assembleResponseEPNS_16OperationContextERNS_7MessageERNS_10DbResponseERKNS_11HostAndPortE+0xB10) [0xaa3480] mongod(_ZN5mongo16MyMessageHandler7processERNS_7MessageEPNS_21AbstractMessagingPortEPNS_9LastErrorE+0xDD) [0x7e99fd] mongod(_ZN5mongo17PortMessageServer17handleIncomingMsgEPv+0x34B) [0xf1badb] libpthread.so.0(+0x7AA1) [0x3221407aa1] libc.so.6(clone+0x6D) [0x32210e8aad]----- END BACKTRACE ----- 除了Got signal: 6 (Aborted)还有点意义，下面的这些trace，完全不知所云。 二、查询分析找到关键词之后，查询这件事情就很简单的了，Google一下，发现在MongoDB的JIRA上，有人提问相同的问题，&gt;&gt;传送门，在下面的回复中，提到了，原因是因为我们在插入数据时，打开的文件数量超过了操作系统的ulimit中的配置，并给出了配置的文档说明，&gt;&gt;&gt;传送门，下面简单的总结一下—— 大多数类Unix的操作系统，如Linux和Mac OS X，提供了一些限制和控制系统资源使用的机制，这里的系统资源比如说：线程、文件、网络连接数等等。这个控制即ulimit，用于避免单用户使用过多的系统资源，当然，有些时候ulimit的一些默认值相对较低，所以会影响一些正常的MongoDB操作。 简单的看一下如何设置资源限制——我们可以使用ulimit命令来检查目前的配置，例如：[root@mongodb-master ~]# ulimit -acore file size (blocks, -c) 0data seg size (kbytes, -d) unlimitedscheduling priority (-e) 0file size (blocks, -f) unlimitedpending signals (-i) 63706max locked memory (kbytes, -l) 64max memory size (kbytes, -m) unlimitedopen files (-n) 1024pipe size (512 bytes, -p) 8POSIX message queues (bytes, -q) 819200real-time priority (-r) 0stack size (kbytes, -s) 10240cpu time (seconds, -t) unlimitedmax user processes (-u) 63706virtual memory (kbytes, -v) unlimitedfile locks (-x) unlimited 在使用ulimit修改具体某个配置项的值时，例如修改open file时，语法为ulimit -n &lt;value&gt;。修改时还要注意，有hard和soft两个选项： 选项 含义 例子 -H 设置硬资源限制，一旦设置不能增加 ulimit -Hs 64，限制硬资源，线程栈大小为64K -S 设置软资源限制，设置后可以增加，但是不能超过硬资源设置 ulimit -Sn 32，限制软资源，32个文件描述符 MongoDB官方手册中给出的mongod和mongos的设置推荐值为： -f (file size): unlimited -t (cpu time): unlimited -v (virtual memory): unlimited -n (open files): 64000 -m (memory size): unlimited -u (processes/threads): 64000 所以对照推荐值，修改我们mongodb-master的ulimit配置即可。具体配置的语法，根据不同的Linux发行版本可能不同，可以阅读手册获得帮助。 注：修改后需要对应重启mongod服务。 三、延伸ulimit作为对资源使用限制的一种方式，是有其作用范围的，它的作用对象是当前shell进程以及其派生的子进程，也就是说，上面我们配置完open file的值后，如果再打开一个shell终端，再次查看ulimit -a会发现open file的值看起来像“恢复原状”（revert）一样。 那么问题来了，刚才我们的设置是否生效还如何检查呢？首先，我们要知道修改后重启的mongod服务的PID，然后使用命令：cat /proc/&lt;PID&gt;/limits来查看当前进程的ulimit配置：[root@mongodb-master ~]# ps -ef | grep mongoroot 4802 1 42 Jun21 ? 20:52:00 /root/mongodb-linux-x86_64-3.0.6/bin/mongod -f /root/mongodb-linux-x86_64-3.0.6/master.confroot 29337 28455 0 15:17 pts/0 00:00:00 grep mongo[root@mongodb-master ~]# cat /proc/4802/limitsLimit Soft Limit Hard Limit Units Max cpu time unlimited unlimited seconds Max file size unlimited unlimited bytes Max data size unlimited unlimited bytes Max stack size 10485760 unlimited bytes Max core file size 0 unlimited bytes Max resident set unlimited unlimited bytes Max processes 63706 63706 processes Max open files 64000 64000 files Max locked memory 65536 65536 bytes Max address space unlimited unlimited bytes Max file locks unlimited unlimited locks Max pending signals 63706 63706 signals Max msgqueue size 819200 819200 bytes Max nice priority 0 0 Max realtime priority 0 0 Max realtime timeout unlimited unlimited us 可以看到，这里我们的配置是生效的，如果服务重启后，对应是否生效，还需要检查和验证。 那么，是否有针对某个具体用户的资源加以限制的方法呢？对于CentOS6来说，可以修改系统的/etc/security/limits.conf配置文件，格式如下：&lt;domain&gt; &lt;type&gt; &lt;item&gt; &lt;value&gt; 其中，&lt;domain&gt;表示用户或者组的名字，还可以使用*作为通配符，不过通配符对root用户可是不生效的，切记。 不过我尝试各种软硬修改配置文件后，并没有发现ulimit -a有丝毫的变化，真的是扎铁了，老心，也许因为我用的是root用户？欢迎邮件交流：zh.f@outlook.com 四、参考[1].Mongodb Crashed with the Got signal: 6 (Aborted)[2].Unix ulimit Settings[3].How do I change the number of open files limit in Linux?[4].Linux ulimit命令[5].ulimit -n not changing - values limits.conf has no effect","tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"http://elbarco.cn/tags/MongoDB/"}]},{"title":"Python中的元组和Packing/Unpacking","date":"2017-03-30T03:02:31.000Z","path":"2017/03/30/packing-and-unpacking-tuples/","text":"什么是元组元组（Tuple）是与列表类似的数据结构，只可被创建，不可被修改，用一对圆括号()包起来，如：&gt;&gt;&gt; x = ('a','b','c') #具有三个元素的元组&gt;&gt;&gt; y = ('a',) #只有一个元素的元组，注意，必须有一个逗号来标识&gt;&gt;&gt; z = () #空元组 元组的操作跟列表非常类似，如+，*，切片等，在元组中同样适用：&gt;&gt;&gt; a = (1,2,3)&gt;&gt;&gt; a[:2](1, 2)&gt;&gt;&gt; a * 1(1, 2, 3)&gt;&gt;&gt; a + (4,5)(1, 2, 3, 4, 5) 元组的Packing/UnpackingPython中允许元组出现在赋值运算符的左侧，这样元组中的每个变量就可以被赋值为右侧对应位置的值，如：&gt;&gt;&gt; (a,b,c,d) = (1,2,3,4)&gt;&gt;&gt; a1&gt;&gt;&gt; c3 上面的写法还可以简化为:&gt;&gt;&gt; a,b,c,d = 1,2,3,4 这个用法还可以非常方便的完成交换两个变量的值：&gt;&gt;&gt; var1, var2 = var2, var1 在Python3中，还提供了一个扩展的unpacking特性——使用*来标注元素来吸收与其他元素不匹配的任何数量的元素，举例如下：&gt;&gt;&gt; x = (1,2,3,4)&gt;&gt;&gt; a, b, *c = x&gt;&gt;&gt; a, b, c(1, 2, [3, 4])&gt;&gt;&gt; a, *b, c = x&gt;&gt;&gt; a, b, c(1, [2, 3], 4)&gt;&gt;&gt; *a, b, c = x&gt;&gt;&gt; a, b, c([1, 2], 3, 4)&gt;&gt;&gt; a, b, c, d, *e = x&gt;&gt;&gt; a, b, c, d, e(1, 2, 3, 4, []) 被标星的元素接收多余的元素作为一个列表，如果没有多余的元素，则会接收一个空列表。 Python中的Packing/Unpacking应用Python中，我们可以使用*（对元组来说）和**（对字典来说）的Packing和Unpacking函数的参数。 * for tuples从下面这个例子说起，我们有一个函数fun()接收四个函数，并打印出来：def fun(a, b, c, d): print(a, b, c, d) 假设我们有一个list：&gt;&gt;&gt; my_list = [1, 2, 3, 4] 调用函数fun()：&gt;&gt;&gt; my_list = [1,2,3,4]&gt;&gt;&gt; fun(my_list)Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;TypeError: fun() missing 3 required positional arguments: 'b', 'c', and 'd'&gt;&gt;&gt; 会得到报错信息，函数认为我们的my_list是单独的一个参数，而函数还需要额外的三个参数。 这时候，我们可以使用*来解包（Unpacking）列表，使之作为四个参数：&gt;&gt;&gt; fun(*my_list)1 2 3 4 这里还有另一个例子，使用内置的range()函数，来演示解包列表的操作： 注，这里使用的是Python3.x，range(3,7)不会直接打印区间的所有值 &gt;&gt;&gt; list(range(3,7))[3, 4, 5, 6]&gt;&gt;&gt; args = [3,7]&gt;&gt;&gt; list(range(args))Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;TypeError: 'list' object cannot be interpreted as an integer&gt;&gt;&gt; list(range(*args))[3, 4, 5, 6] 当我们不知道究竟要传递多少参数给函数时，我们可以使用打包（Packing）把所有的参数打包到一个元组中，用下面的例子来演示打包操作。 我们有函数iSum()来做求和操作def iSum(*args): sum=0 print(args) #把args打印出来，看是否被打包为一个元组 for i in range(0, len(args)): sum = sum + args[i] return sum 测试时，传参个数不等，得到的输出如下：&gt;&gt;&gt; print(iSum(1,2,3,4,5))(1, 2, 3, 4, 5)15&gt;&gt;&gt; print(iSum(10,20,30))(10, 20, 30)60 可以看到入参确实被打包成了一个元组，然后循环遍历元组求和。 如果打包后我们想修改参数，由于元组不可修改，所以需要先转换成列表。下面展示一个打包和解包混合使用的例子。我们有函数func1()用于打印入参，func2()用户修改入参的值：&gt;&gt;&gt; def func1(a,b,c):... print(a,b,c)... &gt;&gt;&gt; def func2(*args):... args = list(args)... args[0] = 'elbarco.cn'... args[1] = 'awesome'... func1(*args)... 调用func2()，传递的三个参数，首先打包为一个元组，然后将元组转换为列表，并修改前两个元素的值，再解包为三个参数，打印出结果，如下所示：&gt;&gt;&gt; func2('Hello','nice','visitors')elbarco.cn awesome visitors ** for dictionaries对于字典（Dictionary），Packing/Unpacking操作使用**。 还是使用上面的func1()，如果要打印字典的值，则需要使用**来解包：&gt;&gt;&gt; dict = &#123;&apos;a&apos;:1,&apos;b&apos;:3,&apos;c&apos;:5&#125;&gt;&gt;&gt; func1(**dict)1 3 5 下面来一个打包的例子：&gt;&gt;&gt; def func3(**elbarco):... print(type(elbarco))... for key in elbarco:... print(&quot;%s = %s&quot; % (key, elbarco[key]))... 传几个参数，用func3()将入参打包为字典，然后在函数中把key和value输出出来，结果如下所示：&gt;&gt;&gt; func3(name='elbarco', location='Beijing', language='Java/Python')&lt;class 'dict'&gt;name = elbarcolocation = Beijinglanguage = Java/Python 以上。 参考1.The Quick Python Book 2nd Edition.Chaptor 5.7.32.Packing and Unpacking Arguments in Python3.Packing and Unpacking Arguments in Python4.Python’s range() Function Explained","tags":[{"name":"Python","slug":"Python","permalink":"http://elbarco.cn/tags/Python/"},{"name":"Tuple","slug":"Tuple","permalink":"http://elbarco.cn/tags/Tuple/"}]},{"title":"从Python中内嵌列表复制说起","date":"2017-03-29T02:00:31.000Z","path":"2017/03/29/nested-lists-and-deep-copies/","text":"写在前面在学习Python3时，看到了列表的拷贝，于是把这个小课题整理在这里，以作记录。 内嵌列表和列表拷贝中的问题Python中的列表是可以嵌入列表的，这个特性的应用场景之一便是用于表示二维矩阵。如下所示：&gt;&gt;&gt; m = [[0, 1, 2], [10, 11, 12], [20, 21, 22]]&gt;&gt;&gt; m[0][0, 1, 2]&gt;&gt;&gt; m[0][1]1&gt;&gt;&gt; m[2][20, 21, 22]&gt;&gt;&gt; m[2][2]22 当然，这一特性可以用于按照我们自己的方式扩展到更多维的矩阵。 大部分情况下，内嵌列表如果只是这样使用，我们需要关心的也就到此为止了。但是因为有变量引用对象，而对象本身又是可被修改的情况，比如列表中内嵌列表，而内嵌列表本身是可被修改的，我们还会遇到下面提到的问题，我们通过例子来演示。 创建一个含有内嵌列表的列表original：&gt;&gt;&gt; nested = [0]&gt;&gt;&gt; original = [nested, 1]&gt;&gt;&gt; original[[0], 1] 列表original的第一个元素指向了列表nested，如图所示： 列表nested的修改可以通过直接修改其本身，也可以通过修改列表original来实现，即：&gt;&gt;&gt; nested[0] = 'zero'&gt;&gt;&gt; original[['zero'], 1]&gt;&gt;&gt; original[0][0] = 0&gt;&gt;&gt; nested[0]&gt;&gt;&gt; original[[0], 1] 如果我们将nested赋值为其他列表，则nested和original之间的连接就会断掉，即：&gt;&gt;&gt; nested = [2]&gt;&gt;&gt; original[[0], 1] 如图所示： 除了上面提到的直接赋值的方式，列表的拷贝我们还可以使用—— 全切片(full slice) &gt;&gt;&gt; x = [0, 1, 2]&gt;&gt;&gt; y = x[:]&gt;&gt;&gt; y[0, 1, 2] +或*运算符 &gt;&gt;&gt; x = [0, 1, 2]&gt;&gt;&gt; y = x + []&gt;&gt;&gt; y[0, 1, 2]&gt;&gt;&gt; z = x * 1&gt;&gt;&gt; z[0, 1, 2] 但是无论上面哪种复制方式，只要列表中存在嵌入列表，就会存在这种问题，我们把这种复制称之为“浅拷贝”，即shallow copy，与之相对的，是“深拷贝”，即deep copy。 对列表的深拷贝对于含有内嵌列表的列表来讲，如果我们需要把内嵌列表也一并拷贝，则需要使用copy模块的deepcopy功能。 &gt;&gt;&gt; original = [[0], 1]&gt;&gt;&gt; shallow = original[:]&gt;&gt;&gt; import copy&gt;&gt;&gt; deep = copy.deepcopy(original) 复制后两个列表的构成其实如下图所示： 在得到列表shallow和列表deep后，我们去尝试修改列表中的值和内嵌列表的值，并看看效果如何：&gt;&gt;&gt; shallow[1]=2 #更改列表中非内嵌列表的值，原列表值不变&gt;&gt;&gt; shallow[[0], 2]&gt;&gt;&gt; original[[0], 1]&gt;&gt;&gt; shallow[0][0]='zero' #更改内嵌列表的值，原列表内嵌列表值改变&gt;&gt;&gt; shallow[['zero'], 2]&gt;&gt;&gt; original[['zero'], 1]&gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; deep[0][0]=5 #对于deep copy的列表，即使修改内嵌列表的值也不会影响原列表&gt;&gt;&gt; deep[[5], 1]&gt;&gt;&gt; original[['zero'], 1]&gt;&gt;&gt; 此外，对于Python来讲，任何列表中嵌入的对象是可修改的，如字典，都会存在这样的问题。 总结和引申思考首先，明确一点，deep copy和shallow copy并不是Python中特有的概念，而是一个与复制对象时对象的成员是否被复制有关的通用的概念。 根据维基百科中的Object copying中的描述，我们总结如下： 我们有变量A和变量B指向不同的内存地址，当B被赋值为A时，两个变量指向了同样的内存地址，之后无论是修改A还是B的内容，都会在另一个变量中立即体现出来，因为两者共享内容。 我们有变量A和B指向了不同的内容地址，当B被赋值为A时，指向A内存地址的内容被复制到B的内存中，之后无论是修改A还是B的内容，A和B都是保持独立的，因为两者不共享内存，即不共享内容。 其他语言，如Java，可以参见StackOverFlow上的这一个讨论：How do I copy an object in Java，后面有机会再详细的梳理一下。 参考1.What’s the difference between a deep copy and a shallow copy2.Object copying3.Shallow and deep copy4.The Quick Python Book, 2nd Edition. Chapter 5.6","tags":[{"name":"Python","slug":"Python","permalink":"http://elbarco.cn/tags/Python/"},{"name":"Deep Copy","slug":"Deep-Copy","permalink":"http://elbarco.cn/tags/Deep-Copy/"},{"name":"Shallow Copy","slug":"Shallow-Copy","permalink":"http://elbarco.cn/tags/Shallow-Copy/"}]},{"title":"并发编程模型之Actor模型","date":"2017-01-21T09:30:30.000Z","path":"2017/01/21/introduction-to-actor-model/","text":"上一篇文章《几个概念：并发、并行、进程、线程和协程》中，对并发和并行的概念做了一个简单的解释，而本文中则从两种并发编程模型讲起，简单的介绍一下Actor模型。 两种并发编程模型并发编程中有两类常见的模型：共享内存和消息传递。 共享内存模型 在并发编程的共享内存模型中，各组件通过读写内存中的共享对象进行交互。 共享内存模型的其他示例： A、B两个在同一个电脑中的处理器（或者同一个处理器的两个核）共享同一物理内存 A、B两个运行在同一电脑上的程序，共享同一文件系统，其中文件可以读写 A、B是同一Java程序中的两个线程，共享相同的Java对象。 消息传递模型 在消息传递模型中，并发模块通过通信信道将消息发送到彼此进行交互。发出的消息会在队列中等待处理。 消息传递模型的示例还有： A和B是通网络中的两台计算机，通过网络通讯 A是一个浏览器，B是一个web服务器，A打开连接请求网页，B发送页面数据给A Actor模型认识Actor模型上面我们认识了两种并发模型，actor模型就属于消息传递模型。actor模型的基本思想是使用actor作为并发基元，可以根据接收的消息做出不同的响应（或动作、行为）： 将有限数量的消息传递给其他的actor 产生有限数量的新的actor 当下一个传入的消息被处理时，改变自己的内部行为 actor模型使用异步消息传递进行通信。特别要指出，actor之间不适用任何中间实体，比如通道，相反的，每个actor拥有可以被寻址的信箱。不要把地址和身份信息弄混淆，每个actor可以有零个、一个或多个地址。当一个actor发送信息时，它必须知道接收方的地址。此外，actor可以给自己发信息，这样他们就可以自己接受信息并且稍后进行处理。注意，这里提到的邮箱并不是概念的一部分，而是一个特性的实现， actor模型可以用于并发系统的建模，正是因为每个actor与其他actor完全独立，没有共享状态，则就没有了竞争状态（race condition），actor之间的通讯和交互完全通过异步消息。 支持actor模型的编程语言有Elixir、Erlang、Scala等，Java语言层面并不支持，但是可以引入Akka，一个用Scala编写的库，用于简化编写容错的、高可伸缩性的Java和Scala的actor模型应用。 写在后面本文重点关注并发编程的两种模型及对Actor模型做一个简单的介绍，Akka的学习会放到后面，由于对Scala不了解，网上看到的例子没有办法贴到这里与大家一起分析。 参考资料[1].MIT.6.005.Reading 17:Concurrency[2].Wikipedia.Actor Model[3].Ruben Vermeersch.Concurrency in Erlang and Scala[4].Marko Dvečko.Introduction to Concurrent Programming","tags":[{"name":"Concurrency","slug":"Concurrency","permalink":"http://elbarco.cn/tags/Concurrency/"},{"name":"Actor Model","slug":"Actor-Model","permalink":"http://elbarco.cn/tags/Actor-Model/"}]},{"title":"几个概念：并发、并行、进程、线程和协程","date":"2017-01-20T02:11:19.000Z","path":"2017/01/20/general-concepts-concurrency-parallelism-process-thread-coroutine/","text":"“有了协程框架再也不用关注线程池调度问题”，在阿里双十一电子书《不一样的技术创新》中看到这样一句话。看到这句话的时候，内心活动是这样的——当我们还在玩线程池的时候，阿里的爸爸们已经在研究调度问题并有解决方案了。所以保持对这句话的质疑和思考，有了今天的整理和学习，为进一步学习协程框架，如Akka起一个头。 并发和并行在多线程编程中，并发（Concurrency）和并行（Parallelism）这两个概念时常会被提到，但是这两个概念却不是一个意思。 并发（Concurrency）并发指的是应用程序同时处理多个任务，多个任务都能同时取得进展。 比如吃饭的时候，电话来了，停下手和嘴去接电话，打完电话继续吃饭，这叫并发。 并行（Parallelism）并行指的是应用可以将任务拆成更小的子任务，而这些子任务可以同时平行着被处理。 比如吃饭的同时，我还能打电话，这就是并行。 对比由上面可以看出，并发依赖于应用如何处理多任务。应用同时只能处理一个任务，处理完在进行下一个任务，这叫顺序地（Sequentially）执行；如果应用同时能处理多个任务，这就叫并发地。 另一方面，并行，则依赖于应用如何处理每个独立的任务。应用可以顺序的将任务从头至尾的执行完，也可以将任务分解成子任务，而子任务可以平行执行。 并发与顺序相对，而并行是并发的子集。 这里还有一个更形象的例子： 进程、线程和协程进程（Process）是计算机中的程序关于某数据集合上的一次运行活动，是系统进行资源分配和调度的基本单位，是操作系统结构的基础。在当代面向线程设计的计算机结构中，进程是线程的容器。程序是指令、数据及其组织形式的描述，进程是程序的实体。 线程（Thread）线程，有时被称为轻量级进程(Lightweight Process，LWP），是程序执行流的最小单元。一个标准的线程由线程ID，当前指令指针(PC），寄存器集合和堆栈组成。 协程（Coroutine, Fiber）协程，又称为微线程，在Lua、Python、Go中有所体现。这里参考廖雪峰的文章，举个例子：如果有两个子程序A和B：def A(): print '1' print '2' print '3'def B(): print 'x' print 'y' print 'z' 对于这两个子程序，一次调用，一次返回，返回结果很有可能是：123xyz 而协程看上去虽然也是子程序，但是在执行过程中，在子程序内部可以中断，然后转而执行别的子程序，在适当的时候在返回来接着执行。比如上面的两个子程序假设由协程执行，那么再执行A的过程中，可以随时终端，去执行B，B也有可能在执行过程中中断再去执行A，结果有可能是：12xy3z 有些类似多线程，但是协程的特点实在一个线程中执行。其优势就是具有极高的执行效率，因为执行过程中不需要县城切换，减少了CPU切换线程的开销。另一方面，避免了多线程的锁机制，不会存在同时的写冲突。 Java与协程Java语言本身不支持协程，通过第三方的库、协程框架可以实现，如注明的akka，kilim等。","tags":[{"name":"Concurrency","slug":"Concurrency","permalink":"http://elbarco.cn/tags/Concurrency/"},{"name":"Parallelism","slug":"Parallelism","permalink":"http://elbarco.cn/tags/Parallelism/"},{"name":"Coroutine","slug":"Coroutine","permalink":"http://elbarco.cn/tags/Coroutine/"}]},{"title":"Galera Cluster for MySQL介绍","date":"2016-12-28T06:27:53.000Z","path":"2016/12/28/introduction-to-galera-cluster-for-mysql/","text":"关于数据库复制数据库的复制，一般指的是在数据库集群中，数据在一个数据库服务节点拷贝到另一个数据库节点。常见的RDBMS的复制方式有两种—— Master/Slave Replication Multi-master Replication 对于主从方式的复制方式，master节点上的写操作会通过数据库日志（如MySQ了的binary log）记录，并通过网络传递给slave节点，然后由slave节点根据master节点传递的日志执行这些变更。而对于多主的复制方式，每个节点都可执行写操作，然后将写操作同步到其他节点。 无论是哪种方式，根据事务在集群中传递的方式，我们又将复制分为两类—— Synchronous Replication - 同步复制，所有的节点在一个单一事务中完成同步，即，在一个事务提交时，所有节点有相同的值。 Asynchronous Replication - 异步复制，主节点的写操作，异步的更新到其他节点中，即，当主节点事务提交时，在很短的时间内，有些节点的值与直接点不一致。 目前，我们的MySQL集群部署方式是双主，但是同一时刻所有的读写压力只在启动一台上，并没有真正意义上实现资源的合理利用，即，仅保证了高可用，但是没有保证负载均衡。 为了实现真正的数据库集群的负载均衡及高可用，我们找到了一个不错的MySQL集群的解决方案，即Galera Cluster for MySQL。它将多个数据库节点组织成一个cluster，并提供以下特性： 同步复制，主备无延迟 支持多主同时读写，保证数据一致性 集群中各个节点保存全量数据 节点添加或删除，集群具备自动监测和配置 行级锁并行复制 不需要写binlog Galera Cluster for MySQL架构使用了Galera之后，客户端和Galera节点之间交互的时序图如下所示： 当客户端执行COMMIT命令，但实际提交未发生前，所有的数据库同一事务中的变更和变更行的主键会被收集到一个write-set中，紧接着，数据库节点就会将write-set发送到所有的其他节点。 之后，write-set会使用主键执行一次验证，这个操作在集群的每个节点上都会进行，验证操作决定了是否可以应用write-set。如果验证未通过，则节点丢掉write-set并且集群回滚；如果验证通过，则事务提交，并且write-set会被应用到集群的其他节点。 上面这中复制方式又称为“基于认证的复制”（Certification Based Replication）。 那么Galera Cluster内部又是如何工作的呢？ 如上图所示，Galera Cluster有四个组件组成： DBMS - Galera Cluster支持MySQL、MariaDB和Percona XtraDB wsrep API Galera Replication Plugin Group Communication plugins 这里就不一一展开具体解释了，详细的可以参见Replication API 部分关键字解释Primary Componet除了单一节点故障之外，集群可能会由于网络原因分裂成几个组件，在这种情况下，为了避免冲突，只有一个组件可以继续修改数据库状态，而这个组件，就称为Primary Component。 Primary Component其实是一个集群，当发生集群分裂的时候，Galera Cluster会执行一个特殊的权重算法，来选举一个组件作为Primary Component，如下图所示： 如果集群具有偶数个节点，则会存在脑裂的风险。如果由于网络导致集群被分裂成恰好数量相等的两个cluster，则每个cluster都有可能保持自己的权重，并且两个都会变成non-primary状态。 所以为了能够实现自动故障转移，需要至少三个节点—— 单交换器的集群应该至少具备3个节点 跨交换机的集群应该至少具备3个交换机 跨网络的集群应该至少具备3个网络 跨数据中心的集群应该至少具备3个数据中心 Replication Configuration wsrep_cluster_name - 集群名称，所有集群中的节点，名称必须一致。 wsrep_cluster_address - 定义集群中的节点IP地址，多个地址使用逗号分割。 wsrep_node_name - 节点名称。 wsrep_node_address - 每个节点自己的IP地址。 wsrep_provider - 定义Galera Replication Plugin的路径，安装之后不确定在哪里的情况下，可以通过find / -name libgalera_smm.so 来查找。 wsrep_provider_options - 定义节点传递给wsrep provider的一些可选配置，如：gcache.size，表示节点缓存write-sets集合的磁盘空间，默认值是128M；gcache.page_size表示页存储中单页大小，整体页面存储的上限是磁盘的大小，默认值是128M。 wsrep_method - 定义了节点在单个状态快照传输（State Snapshot Transfer，指完整的数据从一个集群节点——又称为donor——拷贝到一个新加入的节点——又称为joiner——的过程）中使用的方法或者脚本，支持的方法有mysqldump和rsync两种，在大数据集的场景中，后者比前者更快。 上面是我们/etc/my.cnf.d/wsrep.cnf文件中几个配置项的解释，更多的详细内容，请参见http://galeracluster.com/documentation-webpages/reference.html。","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://elbarco.cn/tags/MySQL/"},{"name":"Galera Cluster","slug":"Galera-Cluster","permalink":"http://elbarco.cn/tags/Galera-Cluster/"},{"name":"DB","slug":"DB","permalink":"http://elbarco.cn/tags/DB/"}]},{"title":"简述StringBuilder和StringBuffer的区别","date":"2016-09-05T06:36:37.000Z","path":"2016/09/05/difference-between-stringbuilder-and-stringbuffer/","text":"从StringBuilder和StringBuffer的不同说起最近在搬砖的时候，发现在拼接字符串的时候，有人习惯使用StringBuffer，有人习惯使用StringBuilder，于是想到了之前在知乎上看到的这个讨论：国内Java面试总是问StringBuffer，StringBuilder区别是啥？档次为什么这么低，果然这在面试中只是一道预热筛选题嘛，不过一下子让我答，却并不能立刻回答上来区别，于是顺手Google了一下，在StringBuilder和StringBuffer的API(JDK1.7)里找到了答案。下面就做一下简述—— 首先，StringBuffer和StringBuilder都是可变字符串，但是前者是线程安全的，因为在调用StringBuffer的操作时是同步的，在源代码中看到的就是方法上加了synchronized关键字：...public synchronized StringBuffer append(String str) &#123; super.append(str); return this;&#125;... 而在StringBuilder的源码中，我们看到的是这样的：public StringBuilder append(String str) &#123; super.append(str); return this; &#125; 上面仅截取部分代码，更多的代码大家可自行查看。 在StringBuffer的API说明中，提到，在JDK5中，开始提供了功能相同，但是非线程安全、不使用synchronized、性能更好的类StringBuilder，在StringBuilderAPI说明中，有提到这么一句话： Instances of StringBuilder are not safe for use by multiple threads. If such synchronization is required then it is recommended that StringBuffer be used. 即只有在同步是必要的情况下，才建议使用StringBuffer。 再论拼接字符串的不同方法和效率至此，区别就简述完了。什么，这就完了？摔……按照面试套路，理论上应该是进入下一话题了，不过这里我们还是要继续，现在就抛出一个非常基础常见的套路问题—— 问：常见的拼接字符串的方法有哪些？ 答案是：String的concat方法、+操作符；StringBuffer和StringBuilder的append方法。 再问：上面几种方法效率如何？ 答案也很简单，当然是StringBuilder&gt;StringBuffer&gt;concat或+操作符。 回答完是什么之后，我们再问问为什么。首先，StringBuffer的每个append操作都是同步的，所以比StringBuilder要慢，那么为什么都比concat或者+效率搞呢？于是又Google一下，找到了这个讨论（Google大法好！Stackoverflow大法好！Orz..），里面提到，在JDK1.6之后，使用”+”操作符时，编译器会自动使用StringBuilder将两个字符append到一起，比如我们代码里是这样写的：String one = \"abc\";String two = \"xyz\";String three = one + two; 在编译的时候，String three会被编译成：String three = new StringBuilder().append(one).append(two).toString(); 乍一看，是效率了很多，但是如果在循环中这样干：String out = \"\";for( int i = 0; i &lt; 10000 ; i++ ) &#123; out = out + i;&#125;return out; 那么在编译时，可能得到的内容就是这样子的：String out = \"\";for( int i = 0; i &lt; 10000; i++ ) &#123; out = new StringBuilder().append(out).append(i).toString();&#125;return out; 此时，我们其实都知道应该这样写：StringBuilder out = new StringBuilder();for( int i = 0 ; i &lt; 10000; i++ ) &#123; out.append(i);&#125;return out.toString(); 这也反映了，编译器一定程度上可以帮助我们优化，但是写出高效的代码，还需要我们自己。 另一个角度较真儿的验证上面的代码是13年答主在JDK1.6中测试的结果，又有一位较真儿的朋友，在不同的JDK版本中进行了测试，全文见Java StringBuilder myth debunked，最终得到了下面的图表： 使用+操作符 使用StringBuilder 使用StringBuilder的基准 这位童鞋贴心的把测试用的代码托管在Github上，有兴趣的可以去看一下。最终这篇文章得出的结论就是——通过对字节码的分析，我们得到了答案，显而易见的是，使用StringBuilder是可以提高性能的。文章开篇还提到这么一句话—— Concatenating two Strings with the plus operator is the source of all evil — Anonymous Java dev 与大家共勉。","tags":[{"name":"Java","slug":"Java","permalink":"http://elbarco.cn/tags/Java/"}]},{"title":"使用Logrotate管理MongoDB日志-后记","date":"2016-07-01T07:23:11.000Z","path":"2016/07/01/使用Logrotate管理MongoDB日志-后记/","text":"发现问题昨天完成了Logrotate管理MongoDB日志的配置工作，手动执行验证通过，但是今天查看日志切换情况，却没有如期待的一般——在日志目录下仅有一个mongodb.log文件——日志没有切换？！ 分析确定执行情况为了确定配置的每天执行的MongoDB日至切换是否执行过，我们首先查看/var/log/cron，下面是截取了部分内容： ...Jul 1 03:01:02 localhost anacron[19152]: Will run job 'cron.daily' in 49 min.Jul 1 03:01:02 localhost anacron[19152]: Jobs will be executed sequentially...Jul 1 03:50:02 localhost anacron[19152]: Job 'cron.daily' started...Jul 1 03:50:02 localhost run-parts(/etc/cron.daily)[19251]: starting logrotateJul 1 03:50:02 localhost run-parts(/etc/cron.daily)[19267]: finished logrotate...Jul 1 03:53:49 localhost anacron[19152]: Job 'cron.daily' terminatedJul 1 03:53:49 localhost anacron[19152]: Normal exit (1 job run) 可以看到，在7月1日凌晨3点50左右确实执行了每日的计划任务，并且cron.daily正常退出。但是Logrotate有没有出错还要继续分析。 查看/var/log/message，在同样的时间段，发现了这样一条错误信息：Jul 1 03:50:02 localhost logrotate: ALERT exited abnormally with [1] 而这段错误信息，正是Logrotate每日执行的计划任务脚本中执行异常退出的提示信息：[root@localhost ~]# cat /etc/cron.daily/logrotate #!/bin/sh/usr/sbin/logrotate /etc/logrotate.conf &gt;/dev/null 2&gt;&amp;1EXITVALUE=$?if [ $EXITVALUE != 0 ]; then /usr/bin/logger -t logrotate \"ALERT exited abnormally with [$EXITVALUE]\"fiexit 0 原因探究原因探究的过程非常简单——Google，所以略。 噗……友谊的小船说翻就翻！（╯－_－）╯╧╧ 回到正题。 引起该问题的原因与SELinux有关。使用getenforce查询SELinux状态：[root@localhost ~]# getenforceEnforcing 可以看到，我们当前的SELinux处于Enforcing模式下，此时，因为我们在之前MongoDB轮换配置文件中，使用了除了/var/log/之外的路径，那么： SELinux was restricting the access to logrotate on log files in directories which does not have the required SELinux file context type. “/var/log” directory has “var_log_t” file context, and logrotate was able to do the needful. 即，/var/log目录具有var_log_t文件上下文，如果要使用Logrotate，我们的日志目录也应该具备这个向下问。所以解决方案就是为配置文件中使用的日志目录设置文件上下文，可以通过下面两个命令做到：semanage fcontext -a -t var_log_t &lt;directory/logfile&gt;restorecon -v &lt;directory/logfile&gt; 第一个命令，用于设置上下文，第二个命令用于对于需要设置上下文的目录活文件，递归的设置。 解决过程检查安装情况执行man semanage或semanage -h检查是否安装semanage:[root@localhost ~]# man semanageNo manual entry for semanage[root@localhost ~]# semanage -h-bash: semanage: command not found 这里我们并没有找到这个命令，所以需要安装相关软件，如果已安装，则跳过这一步。 安装找到是什么软件提供了semanage命令：[root@localhost ~]# yum provides */semanageLoaded plugins: fastestmirror, refresh-packagekit, securityLoading mirror speeds from cached hostfile * base: mirrors.yun-idc.com * extras: mirrors.yun-idc.com * updates: mirrors.yun-idc.comlibsemanage-devel-2.0.43-5.1.el6.x86_64 : Header files and libraries used to build policy manipulation toolsRepo : baseMatched from:Filename : /usr/include/semanagelibsemanage-devel-2.0.43-5.1.el6.i686 : Header files and libraries used to build policy manipulation toolsRepo : baseMatched from:Filename : /usr/include/semanagepolicycoreutils-python-2.0.83-29.el6.x86_64 : SELinux policy core python utilitiesRepo : baseMatched from:Filename : /usr/sbin/semanagepolicycoreutils-python-2.0.83-29.el6.x86_64 : SELinux policy core python utilitiesRepo : installedMatched from:Filename : /usr/sbin/semanage 这里，我们手动安装一下policycoreutils-python即可：[root@localhost ~]# yum -y install policycoreutils-python 执行命令安装完毕，执行命令:[root@localhost ~]# semanage fcontext -a -t var_log_t '/mongoData/mongodb_log/mongodb.log'[root@localhost ~]# restorecon -Frvv /mongoData/mongodb_log/mongodb.log 设置完file context之后，记录会被持久化到/etc/selinux/targeted/contexts/files/file_contexts.local中，我们可以检查一下：[root@localhost ~]# cat /etc/selinux/targeted/contexts/files/file_contexts.local# This file is auto-generated by libsemanage# Do not edit directly./mongoData/mongodb_log/mongodb.log system_u:object_r:var_log_t:s0 此时，补锅工作结束。 更多详细内容，点击这里查看参考文章","tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"http://elbarco.cn/tags/MongoDB/"},{"name":"Logrotate","slug":"Logrotate","permalink":"http://elbarco.cn/tags/Logrotate/"}]},{"title":"使用Logrotate管理MongoDB日志","date":"2016-06-30T07:26:03.000Z","path":"2016/06/30/使用Logrotate管理MongoDB日志/","text":"痛点前段时间需要查询MongoDB日志，惊觉MongoDB的日志并没有配置自动切换轮转，这会导致在繁忙的业务下，日志增长量惊人。面对海量的MongoDB日志，开发和运维人员查看日志变的十分不方便，所以需要寻求使日志自动切换轮转的方式。 选型通过查看MongoDB官方文档，知悉MongoDB提供了几种轮转日志文件的策略，详见这里（据说新版本的MongoDB已经完成了自动的日志轮转功能？）。其中，可以使用MongoDB提供的logRotate命令或者通过向mongod进程发送SIGUSR1信号来实现。 然而看很多文章中均表示，MongoDB本身提供的logRotate机制存在很多问题，比如由于其不稳定性，会造成日志轮换中mongodb进程终止，不提供旧日志的压缩，即使轮转切换日志，还是占用了很多磁盘空间；日志文件重命名格式mongodb.log.2016-10-22T17-44-44不友好等等。所以我们在选择时就会变得很小心，尽量避免使用其内置logRotate。 被广泛认可的方案是通过Logrotate进行日志管理，其中可以执行脚本实现向mongod进程发送SIGUSR1信号。 Logrotate简介Logrotate可以帮助我们管理日志文件。比如周期性的读取日志、压缩日志、备份日志、创建新的日志文件等，基本上你希望做的，都能实现。通常来讲，常被用于来避免单个日志文件增长为难以处理的大小。也常被用于删除旧的日志文件来释放磁盘空间。 通常来讲，默认的Logrotate会作为/etc/cron.daily/中的一个计划任务每天执行一次。[root@localhost etc]# ls /etc/cron.daily/cups logrotate makewhatis.cron mlocate.cron 配置说明配置Logrotate通过编辑两处配置文件来完成： /etc/logrotate.conf /etc/logrotate.d/下面的不同服务特定的配置 logrotate.conf包含了通用的配置，下面是一个默认配置： # see &quot;man logrotate&quot; for details# rotate log files weeklyweekly# keep 4 weeks worth of backlogsrotate 4# create new (empty) log files after rotating old onescreate# use date as a suffix of the rotated filedateext# uncomment this if you want your log files compressed#compress# RPM packages drop log rotation information into this directoryinclude /etc/logrotate.d# no packages own wtmp and btmp -- we&apos;ll rotate them here/var/log/wtmp &#123; monthly create 0664 root utmp minsize 1M rotate 1&#125;/var/log/btmp &#123; missingok monthly create 0600 root utmp rotate 1&#125;# system-specific logs may be also be configured here. 上面的通用配置我们不用过多关心，因为我们具体服务的具体配置在目录/etc/logrotate.d/下。在这个目录里，许多应用在安装后已经设置了Logrotate，比如httpd，nginx等。下面，我们拿nginx的配置做一个简要的说明： [root@localhost ~]# cd /etc/logrotate.d/[root@localhost logrotate.d]# lltotal 44-rw-r--r--. 1 root root 185 Aug 2 2013 httpd-rw-r--r--. 1 root root 871 Jun 22 2015 mysqld-rw-r--r--. 1 root root 302 Apr 26 23:10 nginx-rw-r--r--. 1 root root 219 Nov 23 2013 sssd-rw-r--r--. 1 root root 210 Aug 15 2013 syslog-rw-r--r--. 1 root root 100 Feb 22 2013 yum[root@localhost logrotate.d]# cat nginx /var/log/nginx/*.log &#123; daily missingok rotate 52 compress delaycompress notifempty create 640 nginx adm sharedscripts postrotate [ -f /var/run/nginx.pid ] &amp;&amp; kill -USR1 `cat /var/run/nginx.pid` endscript&#125; 首先第一行，配置了要自动轮换的日志文件的路径/var/log/nginx/*.log，即针对在/var/log/nginx下的*.log文件进行轮换。 daily：每天轮换日志。可选选项有daily，weekly，monthly和yearly missingok：找不到*.log文件也是ok的，不要方…… rotate 52：保留52个日志文件，其他更老旧的日志文件删掉（在这里要配合daily使用，即保留52天的日志文件） compress：压缩日志文件（默认gzip格式） delaycompress：延迟压缩任务直到第二次轮换日志才进行。结果会导致你会有当前的日志文件，一个较旧的没有被压缩过的日志文件和一些压缩过的日志文件 compresscmd：设置使用什么命令来进行压缩，默认是gzip uncompresscmd：设置解压的命令，默认是gunzip。 notifempty：不轮转空文件 create 640 nginx adm：创建一个新的日志文件，并设置权限permissions/owner/group。本例中，使用用户ngxin和用户组adm创建了一个日志文件，文件权限为640.在很多系统中，owner和group一般都会是root。 sharedscripts：在所有的日志轮换完毕后执行postrotate脚本。如果该项没有设置，则会在每个匹配的文件轮换后执行postrotate脚本。 postrotate：轮换日志完成后运行的脚本。 更多的选项，参见这里。 使用Logrotate管理MongoDB日志经过上面对Logrotate的简单说明，这是我们就可以开始使用它来管理MongoDB日志了。 找到日志文件及PID记录文件首先，我们的MongoDB启动配置中，指定了logpath和pidfilepath： logpath=/mongoData/mongodb_log/mongodb.log pidfilepath=/mongoData/mongodb.pid mongod.pid和文件/mongoData/mongodb_data/mongod.lock中都存有mongod的PID，用这两个文件都可以获取PID，任选其一即可。 编写配置文件通过man logrotate查看详细参数，结合业务需求，编写的配置文件如下： /mongoData/mongodb_log/mongodb.log &#123; daily missingok rotate 30 copytruncate dateext compress notifempty create 644 root root sharedscripts postrotate /bin/kill -SIGUSR1 &apos;cat /mongoData/mongodb.pid 2&gt; /dev/null&apos; 2&gt; /dev/null || true endscript&#125; 这里做一下简单说明： copytruncate 这个命令很重要，意思是在创建副本后，将原文件清空，而不是将原文件重命名并创建新的日志文件。这样可以避免有些应用继续向原日志文件中输出，而不是新的日志文件。在没有配置这个命令之前，mongodb一直向轮换后的带时间戳的旧文件中输出日志。 dateext 用于切换日志文件时命名成为mongodb.log-YYYYMMDD格式。 create 644 root root 644权限，即-rw-r--r--与之前的日志文件保持一直的权限即可。 验证编写完配置文件之后，我们将文件拷贝到/etc/logrotate.d/下，执行命令logrotate -f -v /etc/logrotate.d/&lt;YOUR_CONFIG_FILE_NAME&gt;来验证日志是否被轮换了，示例执行结果如下： root@localhost mongodb_log]# logrotate -f -v /etc/logrotate.d/mongologrotatereading config file /etc/logrotate.d/mongologrotatereading config info for /mongoData/mongodb_log/mongodb.logHandling 1 logsrotating pattern: /mongoData/mongodb_log/mongodb.log forced from command line (30 rotations)empty log files are not rotated, old logs are removedconsidering log /mongoData/mongodb_log/mongodb.log log needs rotatingrotating log /mongoData/mongodb_log/mongodb.log, log-&gt;rotateCount is 30dateext suffix '-20160630'glob pattern '-[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]'glob finding old rotated logs failedcopying /mongoData/mongodb_log/mongodb.log to /mongoData/mongodb_log/mongodb.log-20160630set default create contexttruncating /mongoData/mongodb_log/mongodb.logrunning postrotate scriptcompressing log with: /bin/gzip[root@localhost mongodb_log]# lltotal 69604-rw-r--r--. 1 root root 37092 Jun 30 13:24 mongodb.log-rw-r--r--. 1 root root 1047190 Jun 30 13:24 mongodb.log-20160630.gz 特别指出由于我们的服务期开启了SELinux，并且是Enforcing模式下，会造成非/var/log/目录下的logrotate操作失败，所以需要执行下面的命令：[root@localhost ~]# semanage fcontext -a -t var_log_t '/mongoData/mongodb_log/mongodb.log'[root@localhost ~]# restorecon -Frvv /mongoData/mongodb_log/mongodb.log 上面的第一条命令用来定义mongodb.log这个文件的上下文，记录会被持久化到/etc/selinux/targeted/contexts/files/file_contexts.local里，我们可以验证一下。[root@localhost ~]# cat /etc/selinux/targeted/contexts/files/file_contexts.local# This file is auto-generated by libsemanage# Do not edit directly./mongoData/mongodb_log/mongodb.log system_u:object_r:var_log_t:s0 上面的第二条命令，可以递归的设置上下文，如果我们传入的是一个目录，则目录下的所有子目录及文件都会被递归的统一设置。 关于这个问题的说明，可以点击这里查看更多说明和解释。 结语至此，我们边完成了使用Logrotate来管理MongoDB日志了。可以看到，Logrotate十分强大，在使用时，可以通过man logrotate查看一下具体参数，知其然并知其所以然，让其更好地为我们所用。","tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"http://elbarco.cn/tags/MongoDB/"},{"name":"Logrotate","slug":"Logrotate","permalink":"http://elbarco.cn/tags/Logrotate/"}]},{"title":"MongoDB复制集Secondary节点持续Recovering状态解决办法","date":"2016-04-26T21:51:27.000Z","path":"2016/04/27/MongoDB复制集Secondary节点持续Recovering状态解决办法/","text":"前段时间发现MongoDB Replica Set中的某个Secondary节点一直持续Recovering状态，无法恢复，且上次操作时间（optimeDate）已经是N天前了，经过查看官方文档，得知出现这种情况的原因在于复制集中主节点（Primary）一直写入oplog，而从节点（Secondary）的复制过程远远落后，赶不上主节点的oplog写入，就像赌气的孩子跑步一样，赶不上前面的小伙伴，索性一赌气就不走了……当遇到这种情况的时候，是不可能指望从节点自己恢复的，需要我们手动重新同步（initial sync）。 官方给出了两种执行重新同步的方式—— 完全清空数据目录然后重启mongod服务 在其他成员的数据目录下拷贝最近的数据然后重启mongod服务 这里，偷懒不想打包scp数据，索性采用了第一种方式： 停止mongod服务：可在mongo shell中执行db.shutdownServer()来关闭mongod服务，也可以在shell中直接敲mongod --shutdown，或者简单粗暴直接kill -2 &lt;PID&gt;（这里不推荐-9，会造成下次启动不起来的情况，需要删除dbPath目录下的mongo.lock再尝试重新启动）。 对旧的dbPath的目录重命名，以做备份 启动mongod，指向新的空的dbPath目录 简单三步，MongoDB就会重新进行初始化同步，受限于数据量和网络环境等因素的影响，重新同步时间有长有短。重新同步完毕后，打开mongo shell查看复制集状态，一般情况下，这个从节点状态就会恢复正常了。然后要做的就是验证主从数据一致性，确保没问题之后，重命名过的dbPath目录可以删除了。 第二种方式，利用其它成员的最近数据进行启动的操作可见官方文档，这里就不赘述了。","tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"http://elbarco.cn/tags/MongoDB/"}]},{"title":"为CentOS6.5安装Kernel3.10","date":"2016-03-12T01:08:34.000Z","path":"2016/03/12/为CentOS6-5安装Kernel3-10/","text":"最近有想学习下Docker，在Linux下安装Docker对内核的要求至少是3.10以上，然而CentOS 6.5内核版本是2.6，所以首先要做的就是为CentOS 6.5安装3.10的Kernel。 我们并不需要自己编译安装，而是有小伙伴在在ELRepo上为我们准备好了一个package，我们只关心如何安装就好了。 启用ELReporpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org rpm -Uvh http://www.elrepo.org/elrepo-release-6-6.el6.elrepo.noarch.rpm 安装Kernelyum --enablerepo=elrepo-kernel install kernel-lt 配置grub需要编辑/etc/grub.conf来更改kernel顺序，将默认的1改为0.所以看起来应该是酱婶儿的： default=0timeout=5splashimage=(hd0,0)/boot/grub/splash.xpm.gzhiddenmenutitle CentOS (3.10.99-1.el6.elrepo.x86_64) root (hd0,0) kernel /boot/vmlinuz-3.10.99-1.el6.elrepo.x86_64 ro root=UUID=94e4e384-0ace-437f-bc96-057dd64f42ee rd_NO_LUKS rd_NO_LVM LANG=en_US.UTF-8 rd_NO_MD SYSFONT=latarcyrheb-sun16 crashkernel=auto KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM rhgb quiet initrd /boot/initramfs-3.10.99-1.el6.elrepo.x86_64.imgtitle CentOS (2.6.32-573.12.1.el6.x86_64) root (hd0,0) kernel /boot/vmlinuz-2.6.32-573.12.1.el6.x86_64 ro root=UUID=94e4e384-0ace-437f-bc96-057dd64f42ee rd_NO_LUKS rd_NO_LVM LANG=en_US.UTF-8 rd_NO_MD SYSFONT=latarcyrheb-sun16 crashkernel=auto KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM rhgb quiet initrd /boot/initramfs-2.6.32-573.12.1.el6.x86_64.img... 重启并查看reboot 重启后通过uname -a来查看内核版本[root@iZ2853cmjatZ ~]# uname -aLinux iZ2853cmjatZ 3.10.99-1.el6.elrepo.x86_64 #1 SMP Fri Mar 4 11:53:07 EST 2016 x86_64 x86_64 x86_64 GNU/Linux 大功告成！","tags":[{"name":"Linux","slug":"Linux","permalink":"http://elbarco.cn/tags/Linux/"},{"name":"CentOS6.5","slug":"CentOS6-5","permalink":"http://elbarco.cn/tags/CentOS6-5/"}]},{"title":"基于Redis的Tomcat集群Session共享","date":"2016-03-10T08:18:06.000Z","path":"2016/03/10/基于Redis的Tomcat集群Session共享/","text":"目前的web应用集群，使用Nginx做负载均衡，将upstream配置成ip_hash的方式，这种模式下，Nginx会根据来源IP和后端配置来做hash分配，确保固定IP只访问一个后端。 upstream YOUR_NAME &#123; ip_hash; server 192.168.8.15:8080; server 192.168.8.17:8080;&#125; 但是，由于固定某个IP只能访问单独的一个后端，如果宕机或者需要升级程序时做停机重启，正在操作的用户就会退出到登录页面，不仅用户体验很差，而且正在做的操作不能保证成功，容易产生脏数据等。 从Nginx upstream配置说起首先，来看一下Nginx upstream的几种负载均衡策略： 1）轮询upstream YOUR_NAME &#123; server 192.168.8.15:8080; server 192.168.8.17:8080;&#125; 2）权重：该策略可解决服务器性能不等的情况下轮询比率的调配upstream YOUR_NAME &#123; server 192.168.8.15:8080 weight=2; server 192.168.8.17:8080 weight=3;&#125; 3）ip_hashupstream YOUR_NAME &#123; ip_hash; server 192.168.8.15:8080; server 192.168.8.17:8080;&#125; 4）fair：需要安装Upstream Fair Balancer Module。该策略根据后端服务的响应时间来分配，响应时间短的后端优先分配upstream YOUR_NAME &#123; server 192.168.8.15:8080; server 192.168.8.17:8080; fair;&#125; 5）一致性Hash：需要安装Upstream Consistent Hash Module，该策略可以根据给定的字符串进行Hash分配，具体参见官方Wiki。 由此可见，我们迫切的需要使用轮训的方式来做负载均衡，那对于大规模集群部署的web应用来讲，轮训的方式就要Session必须进行共享。 Session共享机制在集群系统下实现Session共享机制一般有如下两种方案： 应用服务器间的Session复制共享（如Tomcat自带的Session共享） 基于缓存数据库的Session共享（如使用Memcached、Redis） 应用服务器间的Session复制共享Session复制共享，主要是指集群环境下，多台应用服务器之间同步Session，使其保持一致，对外透明。如果其中一台服务器发生故障，根据负载均衡的原理，Web服务器（Apache/Nginx）会遍历寻找可用节点，分发请求，由于Session已同步，故能保证用户的Session信息不会丢失。 此方案的不足之处： 技术复杂,必须在同一种中间件之间完成(如Tomcat-Tomcat之间). Session复制带来的性能损失会快速增加.特别是当Session中保存了较大的对象,而且对象变化较快时, 性能下降更加显著. 这种特性使得Web应用的水平扩展受到了限制。 Session内容序列化（Serialize），会消耗系统性能。 Session内容通过广播同步给成员，会造成网络流量瓶颈，即便是内网瓶颈。 基于缓存数据库的Session共享基于缓存数据库的Session共享是指使用如Memcached、Redis等Cache DB来存取Session信息：应用服务器接受新请求将Session信息保存到Cache DB中，当应用服务器发生故障，Web服务器（Apache/Nginx）会遍历寻找可用节点，分发请求，当应用服务器发现Session不在本机内存，则会去Cache DB中查找，如果找到，则复制到本机，这样就实现了Session的共享和高可用。 我选用的是Redis而不是Memcached，是因为Redis具有更丰富的数据结构，比如可以为Key指定过期时间，从而不需要我们定期的刷新缓存。而Memcached做不到，所有就有了这样一个合理的方案—— 在GitHub有这样一个开源工具tomcat-redis-session-manager，可以帮我们实现基于Redis的Session共享，然而直接拿来用的话，Session的key直接就是SessionID，没有一个统一的前缀，所以经过一些小改造，代码已托管到这里，可以通过Tomcat/conf/server.xml的最下面的中增加sessionCookieName配置你想要的Redis中key的前缀，如下所示： &lt;Context docBase=\"/root/YOUR_WEB_APP\" path=\"\" reloadable=\"true\" sessionCookieName=\"YOURJSessionID\" /&gt; 闲话少说，下面开始讲解如何使用：1）下载源码编译成Jar包，讲 tomcat-redis-session-manager-1.2.jar 、jedis-2.6.1.jar、commons-pool2-2.2.jar拷贝到Tomcat目录下的lib中（Jedis、commons-pool2版本任意）2）在Tomcat的conf目录下，编辑context.xml。如果你是用Redis单点，则可以仿照如下配置：&lt;Valve className=\"com.orangefunction.tomcat.redissessions.RedisSessionHandlerValve\" /&gt;&lt;Manager className=\"com.orangefunction.tomcat.redissessions.RedisSessionManager\" host=\"192.168.8.38\" port=\"6379\" database=\"1\" maxInactiveInterval=\"60\" /&gt; 如果是Redis集群环境，则可仿照如下配置：&lt;Valve className=\"com.orangefunction.tomcat.redissessions.RedisSessionHandlerValve\" /&gt;&lt;Manager className=\"com.orangefunction.tomcat.redissessions.RedisSessionManager\" database=\"1\" maxInactiveInterval=\"60\" sentinelMaster=\"mymaster\" sentinels=\"192.168.8.43:26379,192.168.8.45:26379,192.168.8.47:26379\"/&gt; 参数均可选，详见上面tomcat-redis-session-managerGithub中的说明。 关于maxInactiveInterval，即失效时间，这里做一些说明： 即使在这里配置的maxInactiveInterval是60s，如果web.xml配置了session的失效时间，则以web.xml为准。另，如果有一下三处配置了Session的失效时间，则下面的配置覆盖上面的配置: TOMCAT_HOME/conf/web.xml WebApplication/webapp/WEB-INF/web.xml 写在代码中的值 : HttpSession.setMaxInactiveInterval(int) 即实际生效顺序是:HttpSession.setMaxInactiveInterval(int) &gt; $WebApplication/webapp/WEB-INF/web.xml &gt; $TOMCAT_HOME/conf/web.xml 启动Tomcat，访问应用，即可在Redis中看到效果。 关于测试，可以将Nginx Upstream配置为轮询后，仅留一台应用服务器启动，登陆操作，然后启动另外一台，停止第一台服务，继续操作，发现并未受任何影响，即可。 参考nginx upstream的几种配置方式：http://alwaysyunwei.blog.51cto.com/3224143/1239182Load Balancing via Nginx Upstream :http://nginx.org/en/docs/http/load_balancing.htmlTomcat7基于Redis的Session共享：https://yq.aliyun.com/articles/1298Tomcat Session Timeout Web.xml: http://stackoverflow.com/questions/13463036/tomcat-session-timeout-web-xml","tags":[{"name":"Reids","slug":"Reids","permalink":"http://elbarco.cn/tags/Reids/"},{"name":"Tomcat","slug":"Tomcat","permalink":"http://elbarco.cn/tags/Tomcat/"}]},{"title":"如何在CentOS7上安装和配置VNCServer","date":"2016-03-09T09:22:56.000Z","path":"2016/03/09/如何在CentOS7上安装和配置VNCServer/","text":"原文地址：传送门转载自：Linux.cn 这是一个关于怎样在你的 CentOS 7 上安装配置 VNC 服务的教程。当然这个教程也适合 RHEL 7 。在这个教程里，我们将学习什么是 VNC 以及怎样在 CentOS 7 上安装配置 VNC 服务器。 我们都知道，作为一个系统管理员，大多数时间是通过网络管理服务器的。在管理服务器的过程中很少会用到图形界面，多数情况下我们只是用 SSH 来完成我们的管理任务。在这篇文章里，我们将配置 VNC 来提供一个连接我们 CentOS 7 服务器的方法。VNC 允许我们开启一个远程图形会话来连接我们的服务器，这样我们就可以通过网络远程访问服务器的图形界面了。 VNC 服务器是一个自由开源软件，它可以让用户可以远程访问服务器的桌面环境。另外连接 VNC 服务器需要使用 VNC viewer 这个客户端。 一些 VNC 服务器的优点： 远程的图形管理方式让工作变得简单方便。 剪贴板可以在 CentOS 服务器主机和 VNC 客户端机器之间共享。 CentOS 服务器上也可以安装图形工具，让管理能力变得更强大。 只要安装了 VNC 客户端，通过任何操作系统都可以管理 CentOS 服务器了。 比 ssh 图形转发和 RDP 连接更可靠。 那么，让我们开始安装 VNC 服务器之旅吧。我们需要按照下面的步骤一步一步来搭建一个可用的 VNC。 首先，我们需要一个可用的桌面环境（X-Window），如果没有的话要先安装一个。 注意：以下命令必须以 root 权限运行。要切换到 root ，请在终端下运行“sudo -s”，当然不包括双引号（“”） 这里我操作时，运维给准备了一台CentOS 7的环境已经安装了桌面。所以第一步我直接跳过，而是SSH到服务器，直接进行VNC的安装，不过还是保留原文的全部步骤吧。 安装 X-Window首先我们需要安装 X-Window，在终端中运行下面的命令，安装会花费一点时间。 # yum check-update# yum groupinstall \"X Window System\" # yum install gnome-classic-session gnome-terminal nautilus-open-terminal control-center liberation-mono-fonts ### 设置默认启动图形界面# unlink /etc/systemd/system/default.target# ln -sf /lib/systemd/system/graphical.target /etc/systemd/system/default.target # reboot 在服务器重启之后，我们就有了一个工作着的 CentOS 7 桌面环境了。 现在，我们要在服务器上安装 VNC 服务器了。 安装 VNC 服务器现在要在我们的 CentOS 7 上安装 VNC 服务器了。我们需要执行下面的命令。 # yum install tigervnc-server -y 配置 VNC然后，我们需要在 /etc/systemd/system/ 目录里创建一个配置文件。我们可以将 /lib/systemd/sytem/vncserver@.service 拷贝一份配置文件范例过来。 # cp /lib/systemd/system/vncserver@.service /etc/systemd/system/vncserver@:1.service 接着我们用自己最喜欢的编辑器打开 /etc/systemd/system/vncserver@:1.service ，找到下面这几行，用自己的用户名替换掉 。举例来说，我的用户名是 linoxide 所以我用 linoxide 来替换掉 ： ExecStart=/sbin/runuser -l &lt;USER&gt; -c \"/usr/bin/vncserver %i\"PIDFile=/home/&lt;USER&gt;/.vnc/%H%i.pid 替换成 ExecStart=/sbin/runuser -l linoxide -c \"/usr/bin/vncserver %i\"PIDFile=/home/linoxide/.vnc/%H%i.pid 如果是 root 用户则 ExecStart=/sbin/runuser -l root -c \"/usr/bin/vncserver %i\"PIDFile=/root/.vnc/%H%i.pid 好了，下面重启 systemd. # systemctl daemon-reload``` 最后还要设置一下用户的 VNC 密码。要设置某个用户的密码，必须要有能通过 sudo 切换到用户的权限，这里我用 `linoxide` 的权限，执行`su linoxide`就可以了。```shell# su linoxide$ sudo vncpasswd 确保你输入的密码多于6个字符。 开启服务用下面的命令（永久地）开启服务： $ sudo systemctl enable vncserver@:1.service 启动服务。 $ sudo systemctl start vncserver@:1.service 防火墙设置我们需要配置防火墙来让 VNC 服务正常工作。 $ sudo firewall-cmd --permanent --add-service vnc-server$ sudo systemctl restart firewalld.service 现在就可以用 IP 和端口号（LCTT 译注：例如 192.168.1.1:1 ，这里的端口不是服务器的端口，而是视 VNC 连接数的多少从1开始排序）来连接 VNC 服务器了。 用 VNC 客户端连接服务器好了，现在已经完成了 VNC 服务器的安装了。要使用 VNC 连接服务器，我们还需要一个在本地计算机上安装的仅供连接远程计算机使用的 VNC 客户端。 你可以用像 Tightvnc viewer 和 Realvnc viewer 的客户端来连接到服务器。 要用更多的用户连接，需要创建配置文件和端口，请回到第3步，添加一个新的用户和端口。你需要创建 vncserver@:2.service 并替换配置文件里的用户名和之后步骤里相应的文件名、端口号。请确保你登录 VNC 服务器用的是你之前配置 VNC 密码的时候使用的那个用户名。 VNC 服务本身使用的是5900端口。鉴于有不同的用户使用 VNC ，每个人的连接都会获得不同的端口。配置文件名里面的数字告诉 VNC 服务器把服务运行在5900的子端口上。在我们这个例子里，第一个 VNC 服务会运行在5901（5900 + 1）端口上，之后的依次增加，运行在5900 + x 号端口上。其中 x 是指之后用户的配置文件名 vncserver@:x.service 里面的 x 。 在建立连接之前，我们需要知道服务器的 IP 地址和端口。IP 地址是一台计算机在网络中的独特的识别号码。我的服务器的 IP 地址是96.126.120.92，VNC 用户端口是1。 执行下面的命令可以获得服务器的公网 IP 地址（译注：如果你的服务器放在内网或使用动态地址的话，可以这样获得其公网 IP 地址）。 # curl -s checkip.dyndns.org|sed -e 's/.*Current IP Address: //' -e 's/&lt;.*$//' 总结好了，现在我们已经在运行 CentOS 7 / RHEL 7 的服务器上安装配置好了 VNC 服务器。VNC 是自由开源软件中最简单的一种能实现远程控制服务器的工具，也是一款优秀的 Teamviewer Remote Access 替代品。VNC 允许一个安装了 VNC 客户端的用户远程控制一台安装了 VNC 服务的服务器。 关闭 VNC 服务。 # systemctl stop vncserver@:1.service 禁止 VNC 服务开机启动。 # systemctl disable vncserver@:1.service 关闭防火墙。 # systemctl stop firewalld.service","tags":[{"name":"Linux","slug":"Linux","permalink":"http://elbarco.cn/tags/Linux/"},{"name":"CentOS7","slug":"CentOS7","permalink":"http://elbarco.cn/tags/CentOS7/"}]},{"title":"一个成功的Git分支模型","date":"2016-03-08T02:29:22.000Z","path":"2016/03/08/一个成功的Git分支模型/","text":"前言从大公司跳到了小团队，版本控制软件从Git换到了SVN，然而前段时间，头儿让我研究下如何搭建私有Git服务器，如何“优雅”地使用Git。 针对如何搭建私有Git服务器，我选用的是GitLab，有一键安装包，也有很多Step by Step的教程，可自行Google。本文就上面提出的后两个问题，参考文章《A successful Git branching model》讲述如何合理的使用Git branch进行开发和版本管理。 先来张图： 详细展开主要分支在这个模型中，中央仓库持有两个生命周期无限长的主要分支： master develop 我们认为，origin/master这个主要分支上，源码的HEAD永远保持生产就绪的状态。origin/develop这个主要分支的源码HEAD则永远代表了最新提交的开发变更，所以也被称为是“集成分支”。该分支可以用于每晚的自动化构建所使用。 当develop分支的代码能够到达一个稳定点，并且已经准备好进行版本发布，所以的变更应当合并到master上，并且用版本号标注。具体操作后详细谈到。 因此，每当变更最终合并到master分支，这就是一个新的生产版本。对待这个分支，要极其严格，所以理论上来讲，可以使用一个Git hook脚本来进行自动化构建，每当有新内容提交到master，脚本自动将软件发布到成产环境。 支持性分支在这个模型中，有各类支持性分支来协助团队成员的并行开发，方便跟踪功能特性，准备生产版本和快速修复生产问题。与主要分支不同的是，这三个支持性分支是有有限生命周期的，最终会被移除。 这里使用的三类分支分别是： 功能特性分支（Feature branches） 发布用分支（Release branches） 补丁分支（Hotfix branches） 这三类分支目的明确，所以对于这些分支的源分支和合并的目标分支具有十分严格的规则。当然，这三类分支也仅仅是分支而已，并没有特别的地方。 功能特性分支 分支来源：develop合并目标：develop命名惯例：除master、develop、release-*或者hotfix-*之外的任何名字均可 功能特性分支（或者有时被称作专题分支）被用于开发接下来或者将来版本的新功能、新特性。当开始开发一项功能时，目标发布用分支并未明确，但只要功能在开发中，这个分支就存在，最终会合并回develop（意味着即将发布的版本中一定会包含该功能）或者被废弃（这当然是一种令人十分失望的情况）。 功能特性分支仅存在与开发的代码仓库，并不在origin。 创建一个功能特性分支 当着手开发新功能时，先在开发分支上检出新分支： $ git checkout -b myfeature developSwitched to a new branch \"myfeature\" 将完成的功能合并到开发分支上 完成的功能特性被合并到develop分支上，表示该功能要添加到即将发布的版本中： $ git checkout developSwitched to branch 'develop'$ git merge --no-ff myfeatureUpdating ea1b82a..05e9557(Summary of changes)$ git branch -d myfeatureDeleted branch myfeature (was 05e9557).$ git push origin develop --no-ff表示合并总是创建新的提交对象，这样可以避免在合并分支时丢失历史信息，对比图如下： 显而易见，这就是证据啊，证据！:joy: 发布用分支 分支来源：develop合并目标：develop和master命名惯例：release-* 发布用分支用于支持生产环境新版本，如修改少数的缺陷，准备版本发布的元数据（如版本号，构建日期等）。做完这些操作之后，develop分支便可以为了下个大版本接受这些新功能了。 将发布用分支从develop分支上检出的关键时刻是在开发几乎完全可以反映新功能理想状态的时候。此时，至少下个版本要发布的功能所在的功能分支要合并到develop上，而功能发布在将来的版本中则可以暂时不合并，等待下一次发布用分支的检出。 在发布用分支拉出时，就需要给其分配一个版本号。而此后的develop分支上的变更都将反映这个版本。 创建一个发布用分支 发布用分支在develop分支上检出。举例来讲，目前我们的生产环境版本是1.1.5，马上就要发布一个大版本。develop分支已经准备就绪，我们决定将下一个版本的版本号为1.2.所以我们拉出一个发布用分支，命名需要反映新的版本号： $ git checkout -b release-1.2 developSwitched to a new branch \"release-1.2\"$ ./bump-version.sh 1.2Files modified successfully, version bumped to 1.2.$ git commit -a -m \"Bumped version number to 1.2\"[release-1.2 74d9424] Bumped version number to 1.21 files changed, 1 insertions(+), 1 deletions(-) 创建完新分支之后，变更版本号（这里的bump-version.sh脚本用于修改文件版本号，当然，针对不同的场景，也可手动变更版本号）。 该分支会存在一段时间，这段时间内，该分支允许修改缺陷（而不是在develop上面）。在该分支上禁止添加新特性。最终，该分支必须合并到develop、 完成一个发布用分支 当发布用分支已经准备就绪可以发布一个现实的版本，我们仍然有很多工作要做。首先，将发布用分支合并到master（切记，所以提交到master内容一定是一新版本）。接着，提交到master上的变更必须添加标记（如使用版本号等进行标记），用于将来参考。最后，在这个发布用分支上进行的更改需要合并回develop，以保证将来的版本包含缺陷的修复。 在Git中的前两步： $ git checkout masterSwitched to branch 'master'$ git merge --no-ff release-1.2Merge made by recursive.(Summary of changes)$ git tag -a 1.2 此时，版本已发布，并且已标记。 Tips: 你可以使用-s或者-u &lt;key&gt;来加密标记。 为了保留发布用分支的变更，需要合并回develop分支： $ git checkout developSwitched to branch 'develop'$ git merge --no-ff release-1.2Merge made by recursive.(Summary of changes) 这一步可能也会产生冲突，所以，解决冲突并且提交。 此时，我们可以移除该发布用分支： $ git branch -d release-1.2Deleted branch release-1.2 (was ff452fe) 补丁分支 分支来源：master合并目标：develop和master命名惯例：hotfix-* 这类分支与发布用分支很类似，不过补丁分支的产生是为了快速响应生产环境中的紧急问题。当线上遭遇紧急缺陷需要立刻解决，则需要在对应标记的master分支上拉出一个补丁分支。 在某一位或者几位开发者修复线上问题的同时，develop分支可以继续进行。 创建一个补丁分支 补丁分支在master上拉出，举例来说，1.2版本是目前的线上版本，由于一个严重bug造成宕机的情况出现，但是目前develop分支上的变更还不够稳定，此时，我们可以使用补丁分支，先来解决紧急问题： $ git checkout -b hotfix-1.2.1 masterSwitched to a new branch \"hotfix-1.2.1\"$ ./bump-version.sh 1.2.1Files modified successfully, version bumped to 1.2.1.$ git commit -a -m \"Bumped version number to 1.2.1\"[hotfix-1.2.1 41e61bb] Bumped version number to 1.2.11 files changed, 1 insertions(+), 1 deletions(-) 不要忘记增加版本号。 然后，修复bug并提交变更。 $ git commit -m \"Fixed severe production problem\"[hotfix-1.2.1 abbe5d6] Fixed severe production problem5 files changed, 32 insertions(+), 17 deletions(-) 结束使用一个补丁分支 修复bug之后，补丁分支必须合并到master，同时，也需要合并到develop，确保在下一个版本中包含bug的修复。此时的操作与发布用分支完全一致。 首先，更新master并且标注版本： $ git checkout masterSwitched to branch 'master'$ git merge --no-ff hotfix-1.2.1Merge made by recursive.(Summary of changes)$ git tag -a 1.2.1 接着，合并到develop： $ git checkout developSwitched to branch 'develop'$ git merge --no-ff hotfix-1.2.1Merge made by recursive.(Summary of changes) 这个规则存在一个例外情况：如果发布用分支当前存在，则需要将补丁分支合并到发布用分支，而不是develop，因为该发布用分支最终会合并到develop（如果develop分支立刻需要这个bug得到修复，而等不到发布用分支结束，则你需要小心谨慎的将修正合并到未准备就绪的develop分支上）。 最后，移除这个临时分支： $ git branch -d hotfix-1.2.1Deleted branch hotfix-1.2.1 (was abbe5d6). 结语这个模型看起来并没有什么特别令人震惊的地方，但是却十分合理。在StackOverflow上问题What does Bump Version stand for?中，有解答者提到了该文，并十分推荐。 原文作者Twitter：@nvie","tags":[{"name":"Git","slug":"Git","permalink":"http://elbarco.cn/tags/Git/"}]},{"title":"英雄联盟中的随机行为优化","date":"2016-03-07T05:48:24.000Z","path":"2016/03/07/英雄联盟中的随机行为优化/","text":"原文地址：传送门原创翻译，转载请注明出处 对于像英雄联盟这样不断演进的产品的开发者而言，需要不断的致力于与系统的熵作斗争，因为他们将越来越多的内容添加到资源有限的服务器中。新的内容带了新的隐性成本——不仅是更多的实施成本，同时也包括由于创造了更多的纹理、仿真和处理造成的内存和性能成本。如果我们忽略（或者错误估算）了这些成本，则整体游戏性能不佳，可玩性减少。故障使人厌恶，延迟使人愤怒，帧率下降让人沮丧。 我是致力于提高英雄联盟性能团队中的一员。我们为客户端和服务器做快照，发现问题 (性能相关和其他)，然后修复问题。同时，我们将在这个过程中学到的东西反馈其他团队，并且给他们提供工具，使他们在影响用户之前来检测并定位他们自己的性能问题。我们不断的提高英雄联盟的性能为艺术家和设计师添加新的东西提供了空间：当他们使游戏更庞大更优秀的同时，我们使之更快。 这是关于我们团队如何优化英雄联盟性能系列的第一篇文章，后续我们将不断持续更新。这是一项回报丰厚的挑战，这篇文章将深入介绍我们在粒子系统中遇到的一些有趣的挑战——正如在下图中，你可以看到粒子系统在游戏中扮演了十分重要的角色。 上图是在英雄联盟游戏中高粒子密度的一个例子。 优化，并不是在程序集中重写大量的代码——尽管有些时候是这样的。我们仅变更那些不仅能够提高性能，而且维护正确性的代码，如果有可能的话，还会提高代码质量。最后一项略显挑剔：任何不易读、不易维护的代码都会产生技术债务，这个我们稍后再谈。 优化已有的代码库，我们采用了三个基本步骤：鉴别、理解和迭代。 步骤一：鉴别 在开始之前，我们首先需要确认哪些代码需要进行优化。即使有些代码看起来明显性能较差，但是由于其对整体性能影响极小，优化这类代码收益极少（尤其当花费在上面的时间和精力在其他方面可以做到更好的收益）。我们使用代码检测工具和采样分析器来帮助识别非性能部分的基本代码。 步骤二：理解 一旦我们得知代码库的哪部分代码性能较差，我们便会详细的查看这部分代码以求完全理解代码。理解代码意味着理解这些代码的含义及原本的目的。接着，我们就能知悉为何这些代码产生瓶颈了。 步骤三：迭代 当我们理解了为何特定部分代码执行较慢及代码本意要执行的内容，我们就有了足够的信息来设计和开发一套可行的解决方案。使用鉴别步骤中的工具和得到的快照数据，我们将新代码和旧代码的性能做了比较。如果解决方案效果出众，我们会彻底的进行测试以确保不引入来新的bug，那么接下来就可以击掌庆贺了，因为我们已经为其他内部测试做好了充分的准备。在大多数情况下，新的代码不见的足够快，所以我们不断迭代解决方案，知道新的代码能达到优化的目的。 现在，让我们看下在英雄联盟代码库中这几个步骤的实施细节，并以最近优化的粒子系统逐步介绍。 步骤1:鉴别拳头的工程师使用大量的分析工具来检查游戏客户端和服务器的性能。我们先查看来客户端的帧率和通过Waffles得到的高级分析信息（通过工具的特定函数获得的输出信息），这个内部工具可以让我们在内部构建的客户端与服务器保持联通。此外，Waffles还具备其他功能，如在测试过程中触发调试、检查游戏内部数据如导航分格和暂停或者减缓游戏过程。 Waffles截图 Waffles提供了一个实时展示界面，并提供详细的性能信息。上图是Waffles如何展现客户端性能表现的经典例子，上边图形（绿色柱状图）以毫秒为单位表示了帧率——越高的柱状图表示越低的帧率。非常慢的帧率在游戏中是可以感受得到的。柱状图下面是重要功能的分层视图，通过点击任何绿色柱状图，工程师都会看到影响该帧率的详细信息。通过这里，我们可以看出些端倪，即哪部分代码运行时导致性能较差的关键。 我们使用一个简单的宏在代码库内手工检测一些重要函数来提供这份性能相关的信息。在对外发布的游戏版本中，这个宏并没有被打包编译，但在测试版本打包中，这个宏作为一个很小的class存在，它创建了一个事件，存放于配置文件缓冲区。该事件包含一个字符串识别码、一个线程ID、一个时间戳和其他必要的信息（比如它还可以存储在其生命周期内所有发生的内存配置数）。当对象超出范围后，析构器会在配置缓冲区中更新该事件自构造以来的运行时间。在随后的时间，可以输出和解析此配置文件缓冲区——理想的情况是在另一个进程进行以尽量减少对游戏本身的影响。 Chrome Tracing 在这个例子中，我们将分析缓冲区输出到文件，并且读入到构建在Chrome浏览中可视化工具中（关于跟踪工具的更多信息，可以点击这里，你可以在自己的Chrome浏览器中通过在地址栏敲入”chrome://tracing/“进行尝试。这个扩展程序被设计用来进行网页性能分析，输入格式时JSON，所以你可以轻松的根据你自己的数据构造输入）。通过图形化后的结果，我们可以看到哪些是执行较慢的函数，或者在那里不断有大量的小函数被调用：这些都是次优代码的迹象。 让我来展示详细操作：上面的视图是Chrome Tracing的视图，图中展示了客户端上两个运行的线程。上部分的是主线程，执行大多数的处理工作，底部的是粒子线程，用来执行粒子处理。每一个着色的横条均对应一个函数，横条的长度指示了其执行时间。被调用的函数由竖直栈结构展示，父函数在子函数之上。这个工具提供给我们一种非常神奇方式来可视化执行复杂度以及帧的签名时间。当我们发现一个次优代码区域，我们可以放大粒子区域以求查看更多细节。 Chrome Tracing放大效果图 让我们放大图形中间部分。从上面的线程中我可以看到一个非常场的等待，只有当下面的粒子模拟函数执行完毕才结束。模拟功能包含大量不同函数（着色的横条）的调用。每一类都是粒子系统的更新功能，用于将位置、 方向和每个粒子在该系统中其他可见性状态进行更新。一个明显的优化方式是将模拟函数改造成多线程方式，即可运行在主线程中，也可以在粒子线程中执行，对于本例，我们仅关注与优化模拟代码本身。 既然现在我们知道去何处查看性能问题，我们可以切换到样本分析。这类分析周期性的读取和存储程序计数器和运行中的进程的栈信息（可选）。一段时间后，这个信息可以给出一个随机概述，概述中描述了代码库内的耗时。较慢的函数会得到更多的样本，更有用的是，用时最长的单个函数会累积更多的样本。在这里，我们不仅可以看到哪些函数执行最慢，同时可以看到哪几行代码执行最慢。如今有很多不错的样本分析工具可供选择，从免费的Very Sleepy到更多特性支持的商业软件，如Intel的VTune。 通过在游戏客户端上运行VTune来检查粒子线程，我们可以看到如下列表中运行最慢的函数。 VTune中的Hot Functions 上面的表格展现了一些粒子相关的函数。作为参考，最上面两个较大的函数用于为每个粒子更新矩阵和位置、方向相关的状态。举例来说，我们来看在第三和第九项AnimatedVariableWithRandomFactor&lt;&gt;中的Evaluate函数，函数很小（并且容易理解），但是相对而言比较耗时。 步骤2:理解现在，我们选择了一个需要优化的函数，则需要理解这个函数要做的事情和为什么这么做的原因。在本例中，AnimatedVariables被英雄联盟美术师用来定义粒子特征是如何随着时间变化。一旦一个美术师为一个特定的粒子可见性指定关键帧值后，代码中便会插入这些数据来产生一条曲线。插值方法是线性插值或一阶或二次集成。动画曲线被大量的使用——尽在召唤师峡谷（译者注：英雄联盟的地图之一，也是最热门的地图）中就有接近40000的动画曲线——涵盖了从粒子颜色扩展到旋转速度方方面面。Evaluate函数在每场游戏中会被调用数以亿计次。此外，LOL中的粒子系统是游戏体验中很重要的一部分，所以它们的行为不能做出任何改变。 这个类其实已经做过了优化，通过查表的方式，对每个timestep所需要的值都预先计算过并存储在一个数组中，所以在读取这些数值时不必再次计算，所以减少了计算的耗时。这是一个明智的选择，因为曲线的一阶和二次集成是一个昂贵的进程。为每个系统中的每个粒子上的动画变量进行这个操作会使得处理过程大大减少。 动画变量曲线的查询表 在查询性能问题时，通过找到最坏的场景来放大问题往往是一个十分有用的技巧。为了模拟粒子处理减缓，我开始了一场单个玩家的游戏，游戏中有9个中期级别的电脑，并且在下路挑起了一场混乱的团战。接着，我在团战期间在客户端上运行了VTune，记录了大量的数据用于分析。这些数据给出了在Evaluate代码中的归因样本（如下图所示）。 下图中我截取了第91-95行代码，为了更好的说明第90行调用Evaluate的情形。 VTune中的分析样本 对于不熟悉VTune的人来说，其实这个试图展示的就是解析期间所收集的代码。右侧的红色横条指示了命中次数，横条越长就意味着命中次数越多，而命中次数越多表示这一行执行越慢。挨着横条的时间是处理这行代码所用的预估时间。你也可以就某个特定函数的到一个准确视图来查看是什么因素“贡献”了执行缓慢。 如果就红色的横条来看，第95行代码就是问题所在。但是这段代码所做的仅仅是在Vector3f中复制出拼写错误的查询表，为什么这个函数成为最慢的部分呢？为什么12字节的复制这么慢？ 答案在于现代CPU访问内存的方式中。CPU非常忠实的遵循了摩尔定律，每年都会提速60%，而内存速度每年的增速只有可怜的10%。 图出自《计算机体系结构：量化研究方法》By John L. Hennessy, David A. Patterson, Andrea C. Arpaci-Dusseau 缓存可以减小性能差距，运行英雄联盟的大多数CPU都有3级缓存，一级缓存最快但容量最小，三级缓存最慢但容量相对最大。从一级缓存读取数据只需要4个周期，而读取主内存却需要大约300个周期甚至更多。你可以在300个周期内做大量处理工作。 最初查询表的解决方案的问题在于，虽然从查询表中的顺序读取值的操作是非常快的(由于硬件预取)，但是我们正在处理的颗粒并不是按照时间顺序存储，所以实际查找顺序是随机的。这通常会导致CPU等待从主存储设备读取数据时产生延迟。虽然300个周期比一级或者二级集成代价更低，但我们还是需要知道这个函数在游戏中的使用频率如何，因为毕竟这个函数在游戏中被大量的使用。 为了探求真相，我们在代码中添加一些额外的内容来收集AnimatedVariables的数量和类型。结果表示，在38000个AnimatedVariables中： 37500个是线性插值；100个是一级，400个是二级 31500个仅有一个关键值；2500个有3个关键值；1500有2个或者4个关键值 所以最常见的途径是针对单键值。因为代码总是生成查询表，这就产生了一个不需要传播的单数值表。也就意味着每次查询（总是返回相同值）一般会产生缓存丢失，进而导致大量的内存和CPU周期浪费。 通常来讲，代码成为瓶颈一半有四个原因： 调用频率过高 算法选择不佳：如O(n^2)vsO(n) 做了不必要的工作或者太频繁的执行必要的操作 数据较差：或者是数据量太大，或者是数据分布和访问模式较差 这里产生的问题原因不是由于代码设计不好或者开发质量导致。解决方案是好的，但是在被美术师大量使用之后，普通路径是针对单值的，而这些简单的问题在使用过程中是很不明显的。 顺便说一句，我学会了作为一名程序员最重要的事情之一便是尊重你正在处理的代码。代码有可能看起很疯狂，但是这样写的目的可能是基于一个好的出发点。在没有完全理解代码如何使用和其为何设计之前不要错误的认为这些代码是丑陋愚蠢的。 来自：http://codesoftly.com/2010/03/ha-code-entropy-explained.html 步骤3:迭代现在我们了解了哪部分代码执行较慢、这部分代码本意是什么和为何执行较慢，是时候开始构想解决方案了。每个常见的执行路径都是为单独变量设计，我们还知道数量少的键的线性插值非常快（在少量高速缓存中作简单的计算），所以我们需要在考虑这种情况的基础上进行重新设计。最后，我们可以回到前面罕见集成曲线的预计算查询表上。 在某些情况下，当我们不使用查询表时，首先构造这些表是没有意义的，所以会释放大量意义非凡的内存（大多数表具有256个条目或者更多，每个条目可达12字节的大小，这相当于大约每张表3kb）。所以现在，我们可以使用额外的一些内存来添加缓存的条目和存储的单值的数量。 之前的代码看起来是这个样子的： template &lt;typename T&gt;class AnimatedVariable&#123; // &lt;snip&gt;private: std::vector&lt;float&gt; mTimes; std::vector&lt;T&gt; mValues;&#125;;template &lt;typename T&gt;class AnimatedVariablePrecomputed&#123; // &lt;snip&gt;private: std::vector&lt;T&gt; mPrecomutedValues;&#125;; AnimatedVariablePrecomputed对象在AnimatedVariable中进行构造，从它的指定大小插值和构建一个表。Evaluate()仅在预计算对象中被调用。 我们修改了一下AnimatedVariable类，现在看起来是这个样子的： template &lt;typename T&gt;class AnimatedVariable&#123; // &lt;snip&gt;private: int mNumValues; T mSingleValue; struct Key &#123; float mTime; T mvalue; &#125;; std::vector&lt;Key&gt; mKeys; AnimatedVariablePrecomputed&lt;T&gt; *mPrecomputed;&#125;; 我们添加了一个缓存值mSingleValue，和一个整数mNumValues，用于告诉我们何时才使用mSingleValue。如果mNumValues是1（即对应单值的情况），Evaluate()会直接返回mSingleValue的值——不需要其他多余的处理。你还可以注意到插入时间和值构造的Key能够减少缓存未命中的情况。 指向此类的数据向量大小现在范围从24到36个字节不等，具体取决于模板类型（同时也依赖与平台，std::vector&lt;&gt;的大小也会不同）。 Evaluate()之前的代码看起来是这样子的： template &lt;typename T&gt;T AnimatedVariablePrecomputed&lt;T&gt;::Evaluate(float time) const&#123; in numValues = mPrecomputedValues.size(); RIOT_ASSERT(numValues &gt; 1); int index = static_cast&lt;int&gt;(time * numValues); // clamp to valid table entry to handle the 1.0 boundary or out of bounds input index = Clamp(index, 0, numValues - 1); return mPrecomputedValues[index];&#125; 修改后的Evaluate()方法代码如下，这是在VTune中展示的。你可以看到三个可能的执行case：单值（红色部分），线性插值（蓝色部分）和预计算查询（绿色部分）。 在VTune中展示的优化过的代码片段 修改后的代码执行速度大约快了3倍：在最慢的函数列表中该函数从第三位降到了第22位！不仅执行更快，同时还降低了内存的使用，大约减少了750kb。这还不算完，不仅函数执行更快，占内存更少，同时提高了线性插值的准确度。可谓一石三鸟。 这里并没有提到的内容（尽管文章已经足够长了）是我如何通过不断迭代找到了这个解决方案。我最初的尝试减少在粒子生命周期内样本表的大小。这个方案几乎有效——但有些移动较快的粒子由于样本表的减少，变的参差不齐。幸运的是，这个现象很快就被发现了，使得我依然能够将方案更换为本文中提到的方法。当然还有一些其他的代码修改，但是对于性能提高并没有直接效果，也有些代码的修改甚至造成了代码执行更慢。 总结本文中介绍的是英雄联盟游戏代码库中代码优化的一个典型案例。虽然变动更小，但是这个改动使得内存节约了750kb，粒子线程比较之前运行快了1到2毫秒，这使得主线程执行的更快。 当程序员寻求优化的时候，虽然看似显而易见，但这里提到的三个阶段都常常会被忽视。这里只是为了强调一下： 鉴别：分析应用并找出性能最差的部分 理解：理解代码的本意和执行缓慢的原因 迭代：基于上面两个阶段的到的成果进行代码的修改、迭代，并重新分析。重复这三个步骤直到足够快。 上面提到的解决方案不见得是最快的解决方案，但至少方向是正确的——性能提升的安全路径是通过迭代改进。 本文作者：Tony Albrecht","tags":[{"name":"翻译","slug":"翻译","permalink":"http://elbarco.cn/tags/翻译/"}]},{"title":"开山第一篇","date":"2016-03-03T01:30:24.000Z","path":"2016/03/03/开山第一篇/","text":"关于博客用过好多博客，如Cnblog、CSDN、ITeye等等，后来觉得用Github更Geek一点，于是学习gizak搞了一个介个。后来，在郭师哥的怂恿下，搞了台阿里云主机把玩，因为有之前的经验，所以比较愉快选择了Hexo来搭建自己的博客。拖延了好久，上周末终于下定决心好好弄一弄。 Hexo的主题十分丰富，官方的主题向这里(自备梯子)看齐。至于我，选用的是indigo，因为Material Design的风格很舒服，而且移动设备适配也很好，功能基本满足，个性化定制也方便（主要是修改起来方便……(:3 」∠)）。刚搭建完成的时候，我将整个博客的源码都放在了GitHub上面，看这个项目，dev分支是备份，master分支是第一次生成的博客内容。Feel free to build your own blog based on that. 关于我90后，男，单身狗，程序猿。目前帝都某创业型互联网公司就职，云计算相关的Java攻城狮，所以到底是🐶还是🦁️，傻傻分不清楚。喜欢做技术，热爱互联网，拥抱开源。一个人惯了，也爱宅。爱好十分广泛，美剧、电影、音乐、旅行、折腾。其他关键字，强迫症（尽管我是射手座不是处女座）、轻微人格分裂、偶尔犯二……各位看官，随便感受下就好。这里的我，无关紧要。 关于域名目前域名为0x4b5.top，数字表示了我的生日，无他。正在备案的域名elbarco.cn，通过后将正式启用。El barco（音译：埃尔巴科），西班牙语船的意思，朋友们喊我小船，估计是因为我是张帆。家人取的这名重名率极高，选个域名无从下手，所以才想到了用El barco，BTW，本域名与什么elbarco.com毫无关系，特此声明。也希望在这里，我能做那沉舟侧畔千帆中的一员，有所分享，有所进步。 最后，感谢各位看官老爷。","tags":[{"name":"杂","slug":"杂","permalink":"http://elbarco.cn/tags/杂/"}]}]